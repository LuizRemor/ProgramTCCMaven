<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Join the Red Hat team at NodeConf EU 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/23/join-red-hat-team-nodeconf-eu-2022" /><author><name>Lucas Holmquist</name></author><id>af2d04c4-c425-4b66-b2db-7b7fc5ba23c0</id><updated>2022-09-23T07:00:00Z</updated><published>2022-09-23T07:00:00Z</published><summary type="html">&lt;p&gt;It's that time of the year again, and NodeConf EU is almost upon us. This annual event is one of the leading &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="43652567-d1ab-4765-a588-4e905032ad7f" href="https://developers.redhat.com/topics/nodejs" title="Node.js: Develop server-side JavaScript applications"&gt;Node.js&lt;/a&gt; events in Europe. It brings together contributors and innovators from the Node.js community to deliver a wide range of talks and workshops.&lt;/p&gt; &lt;p&gt;The conference will be back in person this year after being virtual for the past two years on October 3rd–5th in Kilkenny, Ireland.&lt;/p&gt; &lt;p&gt;The Node.js team here at Red Hat will be talking about lesser-known Node.js Core modules as well as guiding attendees through a workshop that will get you familiar with cloud-native development with Node.js. &lt;/p&gt; &lt;h2&gt;Talk: Journey into mystery: Lesser-known Node Core modules and APIs&lt;/h2&gt; &lt;p&gt;Wednesday, October 4th, 2022, 9:30 UTC&lt;/p&gt; &lt;p&gt;Presenter: Luke Holmquist (&lt;a href="https://twitter.com/sienaluke"&gt;@sienaluke&lt;/a&gt;), Senior Software Engineer, Red Hat&lt;/p&gt; &lt;p&gt;One of the key concepts of Node.js is its modular architecture, and Node makes it very easy to use a wide variety of modules and &lt;a href="https://developers.redhat.com/topics/api-management"&gt;APIs&lt;/a&gt; from the community. Some of the modules and APIs that are part of Node.js Core are very familiar, like HTTP and Events. But what about those lesser-known core modules just waiting to be used? This talk will journey into mystery as we explore some of the lesser-known Core modules and APIs that Node.js offers.&lt;/p&gt; &lt;h2&gt;Workshop: Elevating Node.js applications to the cloud&lt;/h2&gt; &lt;p&gt;Wednesday, October 4th, 2022, 3:00 UTC&lt;/p&gt; &lt;p&gt;Presenters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bethany Griggs, Senior Software Engineer, Red Hat&lt;/li&gt; &lt;li&gt;Michael Dawson (&lt;a href="https://twitter.com/mhdawson1"&gt;@mhdawson1&lt;/a&gt;), Node.js Lead, Red Hat&lt;/li&gt; &lt;li&gt;Luke Holmquist (&lt;a href="https://twitter.com/sienaluke"&gt;@sienaluke&lt;/a&gt;), Senior Software Engineer, Red Hat&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This workshop provides an introduction to cloud-native development with Node.js. We will walk you through building cloud-native Node.js applications, incorporating typical components, including observability components for &lt;a href="https://developers.redhat.com/articles/2021/05/10/introduction-nodejs-reference-architecture-part-2-logging-nodejs"&gt;logging&lt;/a&gt;, metrics, and more. Next, we'll show you how to deploy your application to cloud environments. The workshop will cover cloud-native concepts and technologies, including health checks, metrics, building &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and deployments to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For a full list of the various talks and workshops, check out the &lt;a href="https://www.nodeconf.eu/agenda"&gt;NodeConf EU 2022 agenda&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Collaborator Summit&lt;/h2&gt; &lt;p&gt;There will also be a OpenJS Collaborator Summit in Dublin, Ireland on October 1-2, 2022, two days before NodeConf EU. We hope to see you there to discuss all things &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; and Node.js. Our team members will be leading or active participants in many sessions.&lt;/p&gt; &lt;p&gt;The Collab Summit is for maintainers or core contributors of an OpenJS project, plus any open source enthusiast interested in participating. This is the time for deep dives on important topics and to meet with people working across your favorite JavaScript projects. Get more details on the &lt;a href="https://openjsf.org/blog/2022/09/01/openjs-collaborator-summit-join-us-in-dublin-virtual-october-1-2%EF%BF%BC/"&gt;OpenJS website&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;More Node.js resources&lt;/h2&gt; &lt;p&gt;Don't miss the latest installments of our series on the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to learn more about Red Hat and IBM’s involvement in the Node.js community and what we are working on, check out our topic pages at &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Red Hat Developer&lt;/a&gt; and &lt;a href="https://developer.ibm.com/languages/node-js/"&gt;IBM Developer&lt;/a&gt;. &lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/23/join-red-hat-team-nodeconf-eu-2022" title="Join the Red Hat team at NodeConf EU 2022"&gt;Join the Red Hat team at NodeConf EU 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2022-09-23T07:00:00Z</dc:date></entry><entry><title type="html">Creating your first cloud-agnostic serverless application with Java</title><link rel="alternate" href="https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html" /><author><name>Helber Belmiro</name></author><id>https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html</id><updated>2022-09-22T10:35:00Z</updated><content type="html">If you are new to Serverless Workflow or serverless in general, creating a simple application for a serverless infrastructure is a good place to start. In this article, you will run through the steps to create your first serverless Java application that runs on any cloud. WHAT IS SERVERLESS? Contrary to what the name says, there are still servers in serverless, but you don’t need to worry about managing them. You just need to deploy your containers and the serverless infrastructure is responsible for providing resources to your application scale up or down. The best part is that it automatically scales up when there is a high demand or scales to zero when there is no demand. This will reduce the amount of money you spend with the cloud. WHAT WILL YOU CREATE? You will use Quarkus to create a simple Java application that returns a greeting message to an HTTP request and deploy it to Knative. WHY KNATIVE? In the beginning, serverless applications used to consist of small pieces of code that were run by a cloud vendor, like AWS Lambda. In this first phase, the applications had some limitations and were closely coupled to the vendor libraries. Knative enables developers to run serverless applications on a Kubernetes cluster. This gives you the flexibility to run your applications on any cloud, on-premises, or even mix all of them. WHY QUARKUS? Because serverless applications need to start fast. Since the biggest advantage of serverless is scale up and down (even zero) according to demand, serverless applications need to start fast when scaling up, otherwise, requests would be denied. One of the greatest characteristics of Quarkus applications is their super fast start-up. Also, Quarkus is , which means that it’s easy to deploy Quarkus applications to Kubernetes without having to understand the intricacies of the underlying Kubernetes framework. REQUIREMENTS * A local Knative installation. See . * This article uses minikube as the local Kubernetes cluster. * kn CLI installed. See . * JDK 11+ installed with JAVA_HOME configured appropriately. * Apache Maven 3.8.1+. * GraalVM (optional to deploy a native image). CREATE A QUARKUS APPLICATION &gt; NOTE: If you don’t want to create the application, you can just clone it &gt; from  and skip to  mvn io.quarkus.platform:quarkus-maven-plugin:2.11.2.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=knative-serving-quarkus-demo cd knative-serving-quarkus-demo RUN YOUR APPLICATION LOCALLY To verify that you created the project correctly, run the project locally by running the following command: mvn quarkus:dev After downloading the dependencies and building the project, you should see an output similar to: __ ____ __ _____ ___ __ ____ ______ --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,&lt; / /_/ /\ \ --\___\_\____/_/ |_/_/|_/_/|_|\____/___/ 2022-08-15 16:50:25,135 INFO [io.quarkus] (Quarkus Main Thread) knative-serving-quarkus-demo 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.11.2.Final) started in 1.339s. Listening on: http://localhost:8080 2022-08-15 16:50:25,150 INFO [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated. 2022-08-15 16:50:25,150 INFO [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, resteasy-reactive, smallrye-context-propagation, vertx] On a different terminal window or in the browser, you can access the application by sending a request to the  endpoint: curl -X 'GET' 'http://localhost:8080/hello' -H 'accept: text/plain' If you see the following output, then you have successfully created your application: Hello from RESTEasy Reactive Hit Ctrl + C to stop the application. PREPARE YOUR APPLICATION FOR DEPLOYMENT TO KNATIVE ADD THE REQUIRED DEPENDENCIES Add the following dependencies to the pom.xml file: &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-kubernetes&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-container-image-jib&lt;/artifactId&gt; &lt;/dependency&gt; CONFIGURE THE APPLICATION FOR DEPLOYMENT TO KNATIVE Add the following configuration to the src/main/resources/application.properties file: quarkus.kubernetes.deployment-target=knative quarkus.container-image.group=dev.local/hbelmiro &gt; NOTE: In the quarkus.container-image.group property, replace hbelmiro with &gt; your container registry username. DEPLOY YOUR APPLICATION TO KNATIVE START THE MINIKUBE TUNNEL &gt; NOTE: This step is only necessary if you are using minikube as the local &gt; Kubernetes cluster. On a different terminal window, run the following command to start the minikube tunnel: minikube tunnel --profile knative You should see an output similar to the following: Status: machine: knative pid: 223762 route: 10.96.0.0/12 -&gt; 192.168.49.2 minikube: Running services: [kourier] errors: minikube: no errors router: no errors loadbalancer emulator: no errors Leave the terminal window open and running the above command. CONFIGURE THE CONTAINER CLI TO USE THE CONTAINER ENGINE INSIDE MINIKUBE eval $(minikube -p knative docker-env) DEPLOY THE APPLICATION Run the following command to deploy the application to Knative: mvn clean package -Dquarkus.kubernetes.deploy=true You should see an output similar to the following: [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Deploying to knative server: https://192.168.49.2:8443/ in namespace: default. [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: Service knative-serving-quarkus-demo. [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 8952ms [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ CHECK THE KNATIVE DEPLOYED SERVICES Run the following command to check the Knative deployed services: kn service list You should see your application listed on the deployed services like the following: NAME URL LATEST AGE CONDITIONS READY REASON knative-serving-quarkus-demo http://knative-serving-quarkus-demo.default.10.106.207.219.sslip.io knative-serving-quarkus-demo-00001 23s 3 OK / 3 True &gt; IMPORTANT: In the above output, check the READY status of the application. If &gt; the status is not True, then you need to wait for the application to be ready, &gt; or there is a problem with the deployment. SEND A REQUEST TO THE DEPLOYED APPLICATION Use the URL returned by the above command to send a request to the deployed application. curl -X 'GET' 'http://knative-serving-quarkus-demo.default.10.106.207.219.sslip.io/hello' -H 'accept: text/plain' You should see the following output: Hello from RESTEasy Reactive GOING NATIVE You can create a native image of your application to make it start even faster. To do that, deploy your application by using the following command: mvn clean package -Pnative -Dquarkus.native.native-image-xmx=4096m -Dquarkus.native.remote-container-build=true -Dquarkus.kubernetes.deploy=true &gt; IMPORTANT: -Dquarkus.native.native-image-xmx=4096m is the amount of memory &gt; Quarkus can use to generate the native image. You should adjust it or &gt; completely remove it depending on your local machine’s specifications. NOW YOU ARE READY TO RUN SERVERLESS APPLICATIONS USING JAVA Easy, isn’t it? Quarkus and Knative give you the freedom to run serverless applications using Java on-premises or in the cloud, no matter the vendor. You can even mix more than one cloud vendor with your on-premises infrastructure. This flexibility brings you agility and reduces your costs with infrastructure. NEXT STEP If you want to go further on serverless with more exciting stuff, check out  The post appeared first on .</content><dc:creator>Helber Belmiro</dc:creator></entry><entry><title>Learn about the new BGP capabilities in Red Hat OpenStack 17</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/22/learn-about-new-bgp-capabilities-red-hat-openstack-17" /><author><name>Daniel Alvarez Sanchez</name></author><id>d3766211-f376-45f2-b86d-2b3cbe44900a</id><updated>2022-09-22T07:00:00Z</updated><published>2022-09-22T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://www.redhat.com/en/technologies/linux-platforms/openstack-platform"&gt;Red Hat OpenStack Platform&lt;/a&gt; is an Infrastructure-as-a-Service (IaaS) offering from Red Hat. Version 17.0 of the platform includes dynamic routing for both the control and data planes. This lets you deploy a cluster in a pure layer-3 (L3) data center, overcoming the scaling issues of traditional layer-2 (L2) infrastructures such as large failure domains, large broadcast traffic, or long convergence times in the event of failures.&lt;/p&gt; &lt;p&gt;This article will illustrate this new feature by outlining a simple three-rack spine and leaf topology, where the layer-2 boundaries are within each rack on the Red Hat OpenStack Platform. The control plane spans the three racks, and each rack also hosts a compute node. Figure 1 illustrates our topology.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_12.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_12.png?itok=5G4F2AYN" width="600" height="285" alt="Diagram showing two leaf nodes connecting each control node to the spines." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Two leaf nodes connect each control node to the spines. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;The main characteristics of this deployment are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Border Gateway Protocol (BGP) is running on every element in the network: controllers, computes, leaves, and spine. The Red Hat OpenStack Platform uses &lt;a href="https://frrouting.org/"&gt;FRRouting&lt;/a&gt; (FRR) to enable BGP in the overcloud nodes, and it operates here as follows: &lt;ul&gt; &lt;li&gt;Leaves are configured as route reflectors, re-advertising learned routes to the spine.&lt;/li&gt; &lt;li&gt;The IPv6 link-local address of each interface uses &lt;em&gt;BGP Unnumbered&lt;/em&gt; to establish BGP sessions. There is no need to assign and configure unique IP addresses on these interfaces, simplifying the deployment.&lt;/li&gt; &lt;li&gt;FRR advertises all local IP addresses (that is, /32 on IPv4 or /128 on IPv6) as directly connected host routes.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Each device has outgoing default &lt;a href="https://study-ccna.com/ecmp-equal-cost-multi-path/"&gt;equal-cost multi-path routing&lt;/a&gt; (ECMP) routes for load balancing and high availability (no L2 bonds).&lt;/li&gt; &lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/rfc5880/"&gt;Bidirectional Forwarding Detection&lt;/a&gt; (BFD), which is &lt;a href="https://opendev.org/openstack/tripleo-ansible/src/commit/7da489819193352f009949f10fe988809a607ab7/tripleo_ansible/roles/tripleo_frr/defaults/main.yml#L23-L32"&gt;configurable&lt;/a&gt;, is used for network failure detection for fast convergence times.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.hpc.cam.ac.uk/cloud/userguide/02-neutron.html"&gt;OpenStack Neutron&lt;/a&gt; and &lt;a href="https://www.ovn.org/en/"&gt;Open Virtual Network&lt;/a&gt; (OVN) are agnostic and require no changes or configuration. &lt;ul&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Constraints and limitations&lt;/h3&gt; &lt;p&gt;Before we move on, it's worth noting the constraints and limitations of the implementation shown in this article:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This feature will only work with the Neutron &lt;a href="https://docs.openstack.org/neutron/latest/admin/config-ml2.html"&gt;ML2/OVN&lt;/a&gt; mechanism driver.&lt;/li&gt; &lt;li&gt;Workloads in provider networks and floating IP addresses are advertised. Routes to these workloads go directly to the compute node hosting the virtual machine (VM).&lt;/li&gt; &lt;li&gt;Tenant networks can &lt;a href="https://opendev.org/openstack/tripleo-ansible/src/commit/2381a7c3b246713744ab259ea8ac22be826344cb/tripleo_ansible/roles/tripleo_frr/defaults/main.yml#L69"&gt;optionally be advertised&lt;/a&gt;, but: &lt;ul&gt; &lt;li&gt;Overlapping CIDRs are not supported. Tenants need to ensure uniqueness (e.g., through the use of &lt;a href="https://docs.openstack.org/neutron/wallaby/admin/config-address-scopes.html"&gt;address scopes&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Traffic to workloads in tenant networks traverses the gateway node.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;An &lt;a href="https://opendev.org/x/ovn-bgp-agent"&gt;agent&lt;/a&gt; is required to run on each overcloud node. This agent is responsible for steering the traffic to or from the OVN overlay, as well as triggering FRR to advertise the IPv4 or IPv6 addresses of the workloads.&lt;/li&gt; &lt;li&gt;The provider bridge (typically &lt;code&gt;br-ex&lt;/code&gt; or &lt;code&gt;br-provider&lt;/code&gt;) is not connected to a physical NIC or bond. Instead, egress traffic from the local VMs is processed by an extra routing layer in the Linux kernel. Similarly, ingress traffic is processed by this extra routing layer and forwarded to OVN through the provider bridge.&lt;/li&gt; &lt;li&gt;There is no support for datapath acceleration, because the agent relies on kernel networking to steer the traffic between the NICs and OVN. Acceleration mechanisms such as &lt;a href="https://docs.openvswitch.org/en/latest/intro/install/dpdk/"&gt;Open vSwitch with DPDK&lt;/a&gt; or &lt;a href="https://docs.openstack.org/neutron/rocky/admin/config-ovs-offload.html"&gt;OVS hardware offloading&lt;/a&gt; are not supported. Similarly, &lt;a href="https://www.networkworld.com/article/3535850/what-is-sr-iov-and-why-is-it-the-gold-standard-for-gpu-sharing.html"&gt;SR-IOV&lt;/a&gt; is not compatible with this configuration because it skips the hypervisor.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Control plane&lt;/h2&gt; &lt;p&gt;With this configuration, the control plane no longer has to be in the same L3 network as the endpoints, because endpoints are advertised via BGP and traffic is &lt;em&gt;routed&lt;/em&gt; to the nodes hosting the services.&lt;/p&gt; &lt;p&gt;&lt;em&gt;High availability&lt;/em&gt; (HA) is provided fairly simply. Instead of announcing the VIP location upon failover by sending broadcast GARPs to the upstream switch, &lt;a href="https://clusterlabs.org/pacemaker/doc/2.1/Pacemaker_Explained/singlehtml/"&gt;Pacemaker&lt;/a&gt; just configures the VIP addresses in the loopback interface, which triggers FRR to advertise a directly connected host route to it.&lt;/p&gt; &lt;h3&gt;Sample traffic route&lt;/h3&gt; &lt;p&gt;Let's take the example of the control plane's &lt;a href="http://www.haproxy.org"&gt;HAproxy &lt;/a&gt;endpoint and check its Pacemaker configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@ctrl-1-0 ~]# pcs constraint colocation config Colocation Constraints: ip-172.31.0.1 with haproxy-bundle (score:INFINITY) [root@ctrl-1-0 ~]# pcs resource config ip-172.31.0.1 Resource: ip-172.31.0.1 (class=ocf provider=heartbeat type=IPaddr2) Attributes: cidr_netmask=32 ip=172.31.0.1 nic=lo Meta Attrs: resource-stickiness=INFINITY Operations: monitor interval=10s timeout=20s (ip-172.31.0.1-monitor-interval-10s) start interval=0s timeout=20s (ip-172.31.0.1-start-interval-0s) stop interval=0s timeout=20s (ip-172.31.0.1-stop-interval-0s) [root@ctrl-1-0 ~]# ip addr show lo 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet 172.31.0.1/32 scope global lo valid_lft forever preferred_lft forever ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After Pacemaker configures the VIP in one of the nodes, it configures this IP address in the &lt;code&gt;lo&lt;/code&gt; interface, triggering FRR to advertise a directly connected route on that node:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@ctrl-1-0 ~]# podman exec -it frr vtysh -c "show ip bgp" | grep 172.31.0.1 *&gt; 172.31.0.1/32 0.0.0.0 0 0 32768 ?&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we can explore the route to this IP address, which is hosted by &lt;code&gt;ctrl-1-0&lt;/code&gt;, from the &lt;code&gt;leaf-2-1&lt;/code&gt; leaf node in &lt;code&gt;rack-2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# for i in leaf-2-1 spine-2 spine-1 leaf-1-1 leaf-1-2; do ssh $i ip route show 172.31.0.1; done Warning: Permanently added 'leaf-2-1' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 330 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:fefe:158a dev eth2 weight 1 nexthop via inet6 fe80::5054:ff:fe55:bdf dev eth1 weight 1 Warning: Permanently added 'spine-2' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 161 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:feb4:d2d0 dev eth3 weight 1 nexthop via inet6 fe80::5054:ff:fec5:7bad dev eth2 weight 1 Warning: Permanently added 'spine-1' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 439 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:fe6f:466b dev eth3 weight 1 nexthop via inet6 fe80::5054:ff:fe8d:c63b dev eth2 weight 1 Warning: Permanently added 'leaf-1-1' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 142 via 100.65.1.2 dev eth3 proto bgp metric 20 Warning: Permanently added 'leaf-1-2' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 123 via 100.64.0.2 dev eth3 proto bgp metric 20&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Traffic directed to the OpenStack control plane VIP (172.31.0.1) from &lt;code&gt;leaf-2-1&lt;/code&gt; goes through either the &lt;code&gt;eth1&lt;/code&gt; (on &lt;code&gt;spine-1&lt;/code&gt;) or &lt;code&gt;eth2&lt;/code&gt; (on &lt;code&gt;spine-2&lt;/code&gt;) ECMP routes. The traffic continues from &lt;code&gt;spine-1&lt;/code&gt; on ECMP routes again to &lt;code&gt;leaf-1-1&lt;/code&gt; , or from &lt;code&gt;spine-2&lt;/code&gt; to &lt;code&gt;leaf1-2&lt;/code&gt;. Finally, the traffic goes through &lt;code&gt;eth3&lt;/code&gt; to the controller hosting the service, &lt;code&gt;ctrl-1-0&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;High availability through BFD&lt;/h3&gt; &lt;p&gt;As mentioned earlier, BFD is running in the network to detect network failures. In order to illustrate its operation, following the example in the previous section, let's take down the NIC in &lt;code&gt;leaf-1-1&lt;/code&gt; that connects to the controller node, and see how the routes adjust on the &lt;code&gt;spine-1&lt;/code&gt; node to go through the other leaf in the same rack.&lt;/p&gt; &lt;p&gt;Initially, there is an ECMP route in the &lt;code&gt;spine-1&lt;/code&gt; node to the VIP that sends the traffic to both leaves in rack 1:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@spine-1 ~]# ip route show 172.31.0.1 172.31.0.1 nhid 179 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:fe6f:466b dev eth3 weight 1 nexthop via inet6 fe80::5054:ff:fe8d:c63b dev eth2 weight 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's bring down the interface that connects &lt;code&gt;leaf-1-1&lt;/code&gt; to &lt;code&gt;ctrl-1-0&lt;/code&gt;, which is hosting the VIP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@leaf-1-1 ~]# ip link set eth3 down&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The BFD state changes to &lt;code&gt;down&lt;/code&gt; for this interface, and the route has been withdrawn in the spine, which now goes only through &lt;code&gt;leaf-1-2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; [root@leaf-1-1 ~]# tail -f /var/log/frr/frr.log | grep state-change 2022/09/08 12:14:47 BFD: [SEY1D-NT8EQ] state-change: [mhop:no peer:100.65.1.2 local:100.65.1.1 vrf:default ifname:eth3] up -&gt; down reason:control-expired [root@spine-1 ~]# ip route show 172.31.0.1 172.31.0.1 nhid 67 via inet6 fe80::5054:ff:fe6f:466b dev eth3 proto bgp metric 20&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Similarly, if we bring up the interface again, BFD will detect this condition and the ECMP route will be re-installed.&lt;/p&gt; &lt;p&gt;The newly introduced &lt;code&gt;frr&lt;/code&gt; container runs in all controller, network, and compute nodes. Its configuration can be queried through the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo podman exec -it frr vtysh -c 'show run'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Data plane&lt;/h2&gt; &lt;p&gt;The data plane refers here to the workloads running in the OpenStack cluster. This section describes the main pieces introduced in this configuration to allow VMs to communicate in a Layer-3 only datacenter.&lt;/p&gt; &lt;h3&gt;OVN BGP Agent&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://opendev.org/x/ovn-bgp-agent"&gt;OVN BGP Agent&lt;/a&gt; is a &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;-based daemon that runs on every compute and network node. This agent connects to the OVN southbound database and keeps track of when a workload is spawned or shut down on a particular hypervisor. The agent then triggers FRR to advertise or withdraw its IP addresses, respectively. The agent is also responsible for configuring the extra routing layer between the provider bridge (&lt;code&gt;br-ex&lt;/code&gt; or &lt;code&gt;br-provider&lt;/code&gt;) and the physical NICs.&lt;/p&gt; &lt;h3&gt;BGP advertisement&lt;/h3&gt; &lt;p&gt;The same principle shown earlier for the control plane applies to the data plane. The difference is that for the control plane, Pacemaker configures the IP addresses to the loopback interface, whereas for the data plane, the OVN BGP Agent adds the addresses to a local &lt;a href="https://access.redhat.com/solutions/5855721"&gt;VRF&lt;/a&gt;. The VRF is used for isolation, because we don't want these IP addresses to interfere with the host routing table. We just want to trigger FRR to advertise and withdraw the addresses as appropriate (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_10.png?itok=fFZZXz67" width="600" height="479" alt="Diagram showing that the OVN BGP Agent controls FRR in order to advertise/withdraw routes." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The OVN BGP Agent controls FRR in order to advertise/withdraw routes. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;div&gt; &lt;/div&gt; &lt;h3&gt;Traffic routing&lt;/h3&gt; &lt;p&gt;As mentioned earlier, OVN has not been modified in any way to support this configuration. Thus, OVN believes that the L2 broadcast domain of the provider networks spans multiple hypervisors, but this is not true anymore. Both ingress and egress traffic require an extra layer of routing. The OVN BGP Agent is responsible for configuring this layer through the following actions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Enable an ARP/NDP proxy in the provider bridge. Requests don't hit the destination because there's no L2 connectivity, so they're answered locally by the kernel:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sysctl net.ipv4.conf.br-ex.proxy_arp net.ipv4.conf.br-ex.proxy_arp = 1 $ sysctl net.ipv6.conf.br-ex.proxy_ndp net.ipv6.conf.br-ex.proxy_ndp = 1&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;For ingress traffic, add host routes in the node to forward the traffic to the provider bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo ip rule show | grep br-ex 32000: from all to 172.24.100.217 lookup br-ex $ sudo ip route show table br-ex default dev br-ex scope link 172.24.100.217 dev br-ex scope link&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;For egress traffic, add flows that change the destination MAC address to that of the provider bridge, so that the kernel will forward the traffic using the default outgoing ECMP routes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ip link show br-ex 7: br-ex: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 3e:cc:28:d7:10:4e brd ff:ff:ff:ff:ff:ff $ sudo ovs-ofctl dump-flows br-ex cookie=0x3e7, duration=48.114s, table=0, n_packets=0, n_bytes=0, priority=900,ip,in_port="patch-provnet-b" actions=mod_dl_dst:3e:cc:28:d7:10:4e,NORMAL cookie=0x3e7, duration=48.091s, table=0, n_packets=0, n_bytes=0, priority=900,ipv6,in_port="patch-provnet-b" actions=mod_dl_dst:3e:cc:28:d7:10:4e,NORMAL cookie=0x0, duration=255892.138s, table=0, n_packets=6997, n_bytes=1368211, priority=0 actions=NORMAL $ ip route show default default nhid 34 proto bgp src 172.30.2.2 metric 20 nexthop via 100.64.0.5 dev eth1 weight 1 nexthop via 100.65.2.5 dev eth2 weight 1 &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This example is for a VM on a provider network and applies as well to Floating IP addresses. However, for workloads in tenant networks, host routes are advertised from network and compute nodes using the Neutron gateway IP address as the next hop. From the gateway node, the traffic reaches the destination compute node through the Geneve tunnel (L3) as usual.&lt;/p&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;p&gt;More information can be found at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://opendev.org/x/ovn-bgp-agent/src/commit/1fa471083c4fdbdac8d2781822c55eb7b8069fa2/doc/source/contributor/bgp_mode_design.rst"&gt;OVN BGP Agent upstream documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://ltomasbo.wordpress.com/2021/02/04/ovn-bgp-agent-in-depth-traffic-flow-inspection/"&gt;OVN BGP Agent: In-depth traffic flow inspection blogpost&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=eKH14UN856o"&gt;OpenInfra Summit Berlin '22 - Using BGP to interconnect workloads across clouds&lt;/a&gt; (video)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=91daVTMt9AA"&gt;Devconf 2021 - Layer 3 Networking with BGP in hyperscale DCx&lt;/a&gt; (video)&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/22/learn-about-new-bgp-capabilities-red-hat-openstack-17" title="Learn about the new BGP capabilities in Red Hat OpenStack 17"&gt;Learn about the new BGP capabilities in Red Hat OpenStack 17&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Alvarez Sanchez</dc:creator><dc:date>2022-09-22T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 22 September 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-09-22.html" /><category term="quarkus" /><category term="resteasy" /><category term="kie" /><category term="keycloak" /><category term="wildfly" /><author><name>Romain Pelisse</name><uri>https://www.jboss.org/people/romain-pelisse</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-09-22.html</id><updated>2022-09-22T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus,resteasy,kie,keycloak,wildfly"&gt; &lt;h1&gt;This Week in JBoss - 22 September 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Hi everyone and welcome to the latest installment of JBoss editorial! Today’s stars of the show: Quarkus and KIE (Kogito/Drools)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus"&gt;Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Quarkus is quite busy this month! Just yesterday, the project released &lt;a href="https://quarkus.io/blog/quarkus-2-12-3-final-released/"&gt;Quarkus 2.12.3.Final&lt;/a&gt;, the third round of bugfixes and performance enhance of for the 2.12, which we mentioned in our previous editorial. But that’s not all, Quarkus tooling also got some love with the release of &lt;a href="https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/"&gt;Quarkus Tools for IntelliJ 1.13.0 released!&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Beyond the publication of new software and bugfixes, James Cobb also took the time to publish the 24th installment of the &lt;a href="https://quarkus.io/newsletter/24/"&gt;Quarkus Newsletter&lt;/a&gt;, a must-read for anyone who wants to follow or play with Quarkus! And to this point, an interesting new player has joined the project’s community: &lt;a href="https://quarkus.io/blog/aphp-user-story/"&gt;Quarkus adoption by APHP (Assistance Publique des Hôpitaux de Paris)&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Of course, if you are already familiar with Quarkus, you may want something more technical to quench your thirst and Clément Escoffier has just the article for you: &lt;a href="https://quarkus.io/blog/redis-job-queue/"&gt;How to implement a job queue with Redis&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_kie"&gt;KIE&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;KIE community has been quite active too in the last days and produced quite an amount of interesting articles about their technology. First, we’ll suggest you’ll dive into this one about &lt;a href="https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html"&gt;Creating your first cloud-agnostic serverless application with Java&lt;/a&gt;. It’s a good place to start!&lt;/p&gt; &lt;p&gt;Another one, called &lt;a href="https://blog.kie.org/2022/09/new-visualizer-for-the-serverless-workflow-editor.html"&gt;New visualizer for the Serverless Workflow Editor&lt;/a&gt; provides a nice overview of this new tool and we’ll certainly learn more about it and use it. If you are more interested into technical details and implementation, you are in luck, there is a rather detailed overview of the &lt;a href="https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html"&gt;Efesto refactoring&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Wait, that’s not all! Check out this article, and the video it links to: &lt;a href="https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html"&gt;Transparent ML, integrating Drools with AIX360&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_techbytes"&gt;Techbytes&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;If KIE and Quarkus have been the most prolific of the last two weeks, there is still a few more articles, coming from other projects, that you may want to check out: * &lt;a href="http://www.mastertheboss.com/java/how-to-spot-java-bugs-with-spotbugs/"&gt;How to spot Java bugs with SpotBugs&lt;/a&gt; * &lt;a href="http://www.mastertheboss.com/jboss-frameworks/resteasy/getting-started-with-jakarta-restful-services/"&gt;Getting started with Jakarta RESTful Services&lt;/a&gt; * &lt;a href="https://infinispan.org/blog/2022/09/12/infinispan-14-console-wizard"&gt;Creating cache with wizard - Infinispan 14&lt;/a&gt; * &lt;a href="https://www.wildfly.org//news/2022/09/14/Remote-dev-watch/"&gt;Remote dev-watch development with WildFly Jar Maven Plugin&lt;/a&gt; * &lt;a href="https://blog.kie.org/2022/09/multiple-repositories-pull-request-chaos-crawl-them-all-in-one-single-place.html"&gt;Multiple repositories Pull Request chaos, crawl them all in one single place&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases…​&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;As always, the JBoss community has been quite active and a few projects published new version in the last two weeks:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-5-3-final-released/"&gt;Quarkus 2.12.2.Final released&lt;/a&gt; followed by &lt;a href="https://quarkus.io/blog/quarkus-2-12-3-final-released/"&gt;Quarkus 2.12.3.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/"&gt;Quarkus Tools for IntelliJ 1.13.0 released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.dev/2022/09/08/resteasy-6.2.0.Beta1-release/"&gt;RESTEasy 6.2.0.Beta1 Release&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/09/keycloak-1902-released"&gt;Keycloak 19.0.2 released&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_decaf"&gt;Decaf'&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Feeling too jittery? Enough Java for now? Get refreshed with these two next articles about &lt;strong&gt;regular expressions&lt;/strong&gt;:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/09/14/beginners-guide-regular-expressions-grep"&gt;A beginner’s guide to regular expressions with grep&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/09/16/regex-how-quantifiers-pattern-collections-and-word-boundaries"&gt;Regex how-to: Quantifiers, pattern collections, and word boundaries&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again next time for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/romain-pelisse.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Romain Pelisse&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Romain Pelisse</dc:creator></entry><entry><title type="html">Efesto refactoring &amp;#8211; technical details</title><link rel="alternate" href="https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html</id><updated>2022-09-21T11:09:56Z</updated><content type="html">This post is meant as a description of the APIs and other technical details of the Efesto framework. It continues the introduction made in the BASE CONCEPTS. There are some concepts around which the APIs are implemented: * Generated resource * Unique identifier * Context of execution The framework provides and manage default implementations of the classes representing those concepts. Those classes could be extended by different engines for their specific needs (e.g. the Kie-drl compilation plugin define a context that contains a KnowledgeBuilderConfiguration) but this specific addition should never leak out of the engine itself, and the functionality of the framework itself should never rely on such "custom" details. GENERATED RESOURCE A represent the result of a compilation. By itself is just a marker interface because there are different kind of generated resources: * executable resources () * redirect resources () * “container” resources (like ). Executable resources represents the "entry point" for execution at runtime, and it contains information required to "instantiate" the executable unit. For some code-generation models (e.g. rules, predictions) this means store the class to instantiate at runtime, that will be used to start the evaluation. For models that does not rely on code-generation for execution (e.g. decisions), this resource contains the name of the class to be instantiated and/or the methods/parameters to be invoked. Redirect resources contains information needed to forward the execution request to a different engine, and it contains the informatio about the ewngine to be invoked. Container resources are meant to store other informations needed at runtime (e.g. the classes generated during compilation). UNIQUE IDENTIFIER The unique identifier () contains the information required to uniquely identify an executable or redirect generated resource. ModelLocalUriId contains information about: * the model/engine to invoke * the full path to the given resource The unique identifier is represented a "path" whose root is the model/engine to invoke, and the path describe all the elements required to get to the specific resource. Stateless engines (e.g. DMN, PMML) describe that as "/namespace/model_name" or "/filename/model_name". Statefull engines would require further path compoenents to identify the specific "state" to be invoked (e.g. "/drl/rule_base/session_name/session_identifier"). ModelLocalUriId is a property of both GeneratedExecutableResource and GeneratedRedirectResource, since both of them have to be retrieved during runtime execution. ModelLocalUriId implements and is a feature that was initially implemented in the Kogito Incubation API, for which an explanation is available . For each module, client code should be able to invoke a method like that to retrieve the unique identifier: ModelLocalUriId modelLocalUriId = appRoot("") .get(PmmlIdFactory.class) .get(fileNameNoSuffix, modelName); This is a fluent API, and each get invocation corresponds to an element in the generated path. The appRoot parameter is only used to differentiate multiple applications (e.g. in distributed context). The first get is needed to start the path building. Each module should implement its own factory extending , that, in turn, will be used to generate the full path. Each of the following get should return an object that extends ModelLocalUriId, since each it represent the path until that specific segment. Each module may provide its own strategy to define such paths, so each module may implement its own subclasses, depending on the needs. Since the The ModelLocalUriId constructor requires a instance, any of its subclasses should implement a way to call that constructor with such instance. In the following example: public class PmmlIdFactory implements ComponentRoot { public LocalComponentIdPmml get(String fileName, String name) { return new LocalComponentIdPmml(fileName, name); } } the PmmlIdFactory expose a get method ( the fluent API) that requires fileName and name parameters. This, in turns, are used to invoke the LocalComponentIdPmml constructor. public class LocalComponentIdPmml extends ModelLocalUriId { public static final String PREFIX = "pmml"; public LocalComponentIdPmml(String fileName, String name) { super(LocalUri.Root.append(PREFIX).append(fileName).append(name)); } } This snippet: LocalUri.Root.append(PREFIX).append(fileName).append(name) will lead to the creation of the following path: /{PREFIX}/{fileName}/{name} CONTEXT OF EXECUTION. The contains basic information about the current execution. It contains informations about the generated classes and the unique identifiers generated during compilation. is the specialization used at runtime to retrieve the generated classes. is the default implementation. Engines may extends the above as per their needs. For example, (the EfestoCompilationContext used inside the rule engine) defines KnowledgeBuilderConfiguration for its needs. COMPILATION CONTEXT is the specialization used at compile time, and it is used to store the classes generated during compilation. is the default implementation. provide a static method to retrieve the default implementation () with all the classes eventually compiled from a previous compilation. That static method, behind the scenes, invokes the constructor that scan the classloader to look for efesto-related classes. RUNTIME CONTEXT is the specialization used at runtime to retrieve the generated classes. is the default implementation. provide a static method to retrieve the default implementation () with all the efesto-related compiled classes. That static method, behind the scenes, invokes the constructor that scan the classloader to look for efesto-related classes. PUBLIC APIS The framework consists basically of two set of APIs, the "compilation" and the "runtime" ones. Those APIs are defined inside and . Those are the APIs that "client code" is expected to invoke. Said differently, "client code" is expected to interact with engines only through those APIs. COMPILATION API void processResource(EfestoCompilationContext context, EfestoResource... toProcess); This is the method that "External applications" (e.g. kie-maven-plugin) should invoke to create executables units out of given models. is the DTO wrapping a single model to be processed. Its only method T getContent(); is invoked by the compilation manager to get the object to be processed. The more common usage is to provide an actual File to the compilation manager, in which case there already is an implementation, . is a specific abstract implementations that wraps a Set of models. As for the previous, there already exist an implementation to manage FIles, . RUNTIME API Collection&lt;EfestoOutput&gt; evaluateInput(EfestoRuntimeContext context, EfestoInput... toEvaluate); This is the method that "External applications" (e.g. kogito execution) should invoke to retrieve a result out of executable units generated at compile-time. is the DTO wrapping a the data to be evaluated and the unique identifier of the executable units. It has two methods: ModelLocalUriId getModelLocalUriId(); T getInputData(); the former returns the unique identifier of the executable units; the latter returns the data to use for evaluation. Currently there are no "default" implementations of it, since the input structure is generally model-specific; so, every plugin should provide its own implementation. INTERNAL APIS Behind the scenes, when CompilationManager and RuntimeManager receives a request, they scan the classloader for engine plugins. Such plugins should implement, respectively, the and the . COMPILERSERVICE API declares three methods: boolean canManageResource(EfestoResource toProcess); List&lt;E&gt; processResource(EfestoResource toProcess, U context); String getModelType(); The first one is invoked by the CompilationManager to verify if the specific implementation is able to manage the given resource. The evaluation could be based on the actual type of the resource, on some details of the content, or on a mix of them. It is responsibility of the implementation to find the appropriate logic. The only requirement to keep in mind is that, during execution, there should be at most one implementation that return true for a given EfestoResource, otherwise an exception is thrown. The following snippet is an example where a given EfestoResource is considered valid if it is an DecisionTableFileSetResource: @Override public boolean canManageResource(EfestoResource toProcess) { return toProcess instanceof DecisionTableFileSetResource; } The above implementation works because DecisionTableFileSetResource is a class specifically defined by the plugin itself, so there are no possible "overlaps" with other implementations. On the other side, the following snippet is an example where a given EfestoResource is considered valid if it is an EfestoFileResource and if the contained model is a PMML: @Override public boolean canManageResource(EfestoResource toProcess) { return toProcess instanceof EfestoFileResource &amp;amp;&amp; ((EfestoFileResource) toProcess).getModelType().equalsIgnoreCase(PMML_STRING); } In this case, the actual class of EfestoResource is not enough, since EfestoFileResource is one of the default implementations provided by the framework. So, a further check is needed, that is about the model that is wrapped in the resource. A single plugin may manage multiple representations of the same model. For example, a plugin may manage both an EfestoFileResource and an EfestoInputStreamResource. There are different possible strategies to do that. For example, the plugin may provide one single "compilation-module" with two classes implementing the KieCompilerService; or it may define two "compilation-modules", each of which with one implementation, or one single class may manage both kind of inputs. Again, this is responsibility of the plugin itself. This also push toward code reusage. For a given model, there could be a common path that provide the final compilation output, and different entry point depending on the model representation. It is so possible that multiple compilation models creates a compilation output that, in turns, it is also an EfestoResource. Then, there could be another implementation that accept as input the above intermediate resuorce, and transform it to the final compilation outpout. This chaining is managed by the efesto framework out of the box. An example of that is featured by drools-related pmml models. During compilation, the PMML compiler generates an that is both an EfestoResource and an EfestoCompilationOutput. When the CompilationManager retrieves that compilation output, being it an EfestoResource, scans the plugins to find someone that is able to compile it. The fullfill this requirement, and proceed with drl-specific compilation. One thing to notice here is that different modules should limit as much as possible direct dependency between them. The second method is invoked by the compilation manager if the previous one returned true. That method receives also an EfestoCompilationContext as parameter. Code-generating implementations should rely on that context for compilation and classloading. The third method is used by the framework to discover, at execution time, which models can actually be managed. Thanks to that method, there is a complete de-coupling between the framework and the implementation themselves, since the framework can discover dynamically the available models, and every plugin may freely define its own model. Last critical bit is that every compilation module should contain an org.kie.efesto.compilationmanager.api.service.KieCompilerService file inside src/main/resources/META-INF directory, and that file should contain all the KieCompilationService implementations provided by that module. RUNTIMESERVICE API declares three methods: boolean canManageInput(EfestoInput toEvaluate, K context); Optional&lt;E&gt; evaluateInput(T toEvaluate, K context); String getModelType(); The first one is invoked by the RuntimeManager to verify if the specific implementation is able to manage the given input. The evaluation could be based on the actual type of the resource, on some details of the content, or on a mix of them. It is responsibility of the implementation to find the appropriate logic. The only requirement to keep in mind is that, during execution, there should be at most one implementation that return true for a given EfestoInput, otherwise an exception is thrown. The following snippet is an example where a given EfestoInput is considered valid if it is an EfestoInputPMML and the given identifier has already been compiled: public static boolean canManage(EfestoInput toEvaluate, EfestoRuntimeContext runtimeContext) { return (toEvaluate instanceof EfestoInputPMML) &amp;amp;&amp; isPresentExecutableOrRedirect(toEvaluate.getModelLocalUriId(), runtimeContext); } The above implementation works because EfestoInputPMML is a class specifically defined by the plugin itself, so there are no possible "overlaps" with other implementations. The difference with the compilation side is that the KieRuntimeService implementation should also check that the model related to the given unique identifier has already been compiled. A single plugin may manage different types of input for the same model. For example, the rule plugin may manage both an EfestoInputDrlKieSessionLocal and an AbstractEfestoInput that contains an EfestoMapInputDTO. There are different possible strategies to do that. For example, the plugin may provide one single "runtime-module" with two classes implementing the KieRuntimeService; or it may define two "runtime-modules", each of which with one implementation, or one single class may manage both kind of inputs. Again, this is responsibility of the plugin itself. This also push toward code reusage. For a given model, there could be a common code-path that provides the final runtime result, and different entry point depending on the input format. It is so possible that a runtime implementation would need a result from another implementation. In that case, the calling runtime will create a specifically-crafted EfestoInput and will ask the RuntimeManage the result for it. This chaining is managed by the efesto framework out of the box. An example of that is featured by drools-related pmml models. During execution, the PMML runtime generates an EfestoInput&lt;EfestoMapInputDTO&gt; and send it to the RuntimeManager. The RuntimeManager scans the plugins to find someone that is able to execute it. The fullfill this requirement, and proceed with drl-specific execution. One thing to note here is thet modules should limit as much as possible direct dependency between them! The second method is invoked by the runtime manager if the previous one returned true. That method receives also an EfestoRuntimeContext as parameter. Code-generating implementations should rely on that context to retrieve/load classes generated during compilation. The third method is used by the framework to discover, at execution time, which models can actually be managed. Thanks to that method, there is a complete de-coupling between the framework and the implementation themselves, since the framework can discover dynamically the available models, and every plugin may freely define its own model. Last critical bit is that every compilation module should contain an org.kie.efesto.runtimemanager.api.service.KieRuntimeService file inside src/main/resources/META-INF directory, and that file should contain all the KieRuntimeService implementations provided by that module. CONCLUSION This post was meant to provide more technical details on what have been introduced in the . Following ones will provide concrete step-by-step tutorial and real uses-cases so… stay tuned!!! The post appeared first on .</content><dc:creator>Gabriele Cardosi</dc:creator></entry><entry><title>Bind services created with AWS Controllers for Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/21/bind-services-created-aws-controllers-kubernetes" /><author><name>Baiju Muthukadan</name></author><id>ea05a1b5-94fc-4f41-9f45-43a331d6a4c7</id><updated>2022-09-21T07:00:00Z</updated><published>2022-09-21T07:00:00Z</published><summary type="html">&lt;p&gt;Application developers can define Amazon Web Services (AWS) resources directly from Kubernetes using &lt;a href="https://aws-controllers-k8s.github.io/community/docs/community/overview/"&gt;AWS Controllers for Kubernetes&lt;/a&gt; (ACK). You can use the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/intro.html"&gt;Service Binding Operator&lt;/a&gt; to easily connect applications to any AWS service provisioned through ACK.&lt;/p&gt; &lt;p&gt;This article explores the connection with an RDS database and demonstrates configuring ACK to create a service instance for the AWS Relational Database Service (RDS). You can also learn how to use Service Binding Operator annotations to bind a PostgreSQL service created using RDS and a REST API.&lt;/p&gt; &lt;h2&gt;Benefits of the Service Binding Operator and AWS Controllers for Kubernetes &lt;/h2&gt; &lt;p&gt;One benefit of the Service Binding Operator and ACK is that they streamline the formation of a connection. The Service Binding Operator implements the &lt;a href="https://servicebinding.io"&gt;Service Binding specification for Kubernetes&lt;/a&gt;. This is a Kubernetes-wide specification for automating the process of service secrets communicating to workloads.&lt;/p&gt; &lt;p&gt;Another benefit of using the Service Binding Operator is that the only focus of applications with many microservices (maybe hundreds of them) is setting the correct label to receive binding data from the services specified by Service Binding Operator resources using the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/binding-workloads-using-sbo/binding-options.html#binding-workloads-using-a-label-selector"&gt;label selector&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Service Binding Operator supports the following methods to obtain connection details from a service:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/servicebinding/spec#provisioned-service"&gt;Provisioned Service&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/servicebinding/spec#direct-secret-reference"&gt;Direct Secret Reference&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/exposing-binding-data/adding-annotation.html"&gt;Annotations&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently, ACK does not support the Provisioned Service method. And no single secret contains all the connection details. In such a scenario, you can use the annotation support provided by the Service Binding Operator and add this annotation to a Custom Resource (CR) or Custom Resource Definition (CRD).&lt;/p&gt; &lt;p&gt;The following articles offer more information about ACK, including where the ACK project came from, why the Operator pattern is used, and how to configure and use ACK:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;Create AWS resources with Kubernetes and Operators&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Step 1:  Prerequisites setup&lt;/h2&gt; &lt;p&gt;The prerequisites for this demonstration are pretty simple. You must have an AWS account and a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster with the Service Binding Operator installed.&lt;/p&gt; &lt;h3&gt;AWS account permissions&lt;/h3&gt; &lt;p&gt;Your AWS account must have the &lt;a href="https://aws-controllers-k8s.github.io/community/docs/user-docs/authorization/#aws-iam-permissions-for-ack-controller"&gt;IAM role permissions&lt;/a&gt; for the Amazon Relational Database Service (RDS) ACK controller. The policy required for RDS is:&lt;/p&gt; &lt;p&gt;&lt;code&gt;arn:aws:iam::aws:policy/AmazonRDSFullAccess&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;OpenShift cluster with the Service Binding Operator&lt;/h3&gt; &lt;p&gt;You need administrator access to an OpenShift cluster. To install the Sevice Binding Operator, create a subscription similar to this example:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: my-service-binding-operator namespace: openshift-operators spec: channel: stable name: rh-service-binding-operator source: redhat-operators sourceNamespace: openshift-marketplace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, place this configuration in a file named &lt;code&gt;subscription.yaml&lt;/code&gt;. Then use the following &lt;code&gt;oc&lt;/code&gt; command to create the resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f subscription.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can install the Service Binding Operator from &lt;a href="https://operatorhub.io"&gt;OperatorHub&lt;/a&gt; using the OpenShift administrator console.&lt;/p&gt; &lt;h2&gt;Step 2:  Install the RDS Operator in an OpenShift cluster&lt;/h2&gt; &lt;p&gt;These four steps use the ACK Operator to install the RDS database. The official documentation shows detailed information about configuring ACK in an OpenShift cluster.&lt;/p&gt; &lt;h3&gt;1. Create a namespace&lt;/h3&gt; &lt;p&gt;The following example uses a namespace called &lt;code&gt;ack-system&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project ack-system&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the output you should see:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Now using project "ack-system" on server "https://example.org:6443". ...&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;2. Create a config map&lt;/h3&gt; &lt;p&gt;Create a config map with the following content in a &lt;code&gt;config.txt&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=us-west-2 AWS_ENDPOINT_URL= ACK_RESOURCE_TAGS=hellofromocp&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use this config map in your OpenShift cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create configmap --namespace ack-system \ --from-env-file=config.txt ack-rds-user-config &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;3. Create a secret&lt;/h3&gt; &lt;p&gt;Save the following authentication values in a file, such as &lt;code&gt;secrets.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;AWS_ACCESS_KEY_ID=&lt;access key id&gt; AWS_SECRET_ACCESS_KEY=&lt;secret access key&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use this &lt;code&gt;secrets.txt&lt;/code&gt; file to create a secret in your OpenShift cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic \ --namespace ack-system \ --from-env-file=secrets.txt ack-rds-user-secrets&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Be sure to secure access to this resource and the namespace because you will keep sensitive information in this secret—your AWS Access Key ID and AWS Secret Access Key.&lt;/p&gt; &lt;p&gt;Alternatively, you can set up secure access using &lt;a href="https://aws-controllers-k8s.github.io/community/docs/user-docs/irsa/#create-an-iam-role-for-your-ack-service-controller"&gt;IAM Roles for Service Accounts&lt;/a&gt; (IRSA).&lt;/p&gt; &lt;h3&gt;4. Install the relational database service&lt;/h3&gt; &lt;p&gt;Refer to the article &lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;How to get Operators to use AWS Controllers for Kubernetes&lt;/a&gt; for ACK RDS controller installation instructions. After successful installation, this page (Figure 1) appears in the administrator console.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog-ack.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/blog-ack.png?itok=nbjyUns8" width="1440" height="710" alt="This page appears in the OpenShift administrator console after installation." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: After the ACK RDS controller is installed, this page appears in the OpenShift administrator console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Step 3:  The consumption of annotations and label selectors&lt;/h2&gt; &lt;p&gt;To enable binding, the Service Binding Operator uses the following annotations that are part of the &lt;code&gt;DBInstance&lt;/code&gt; resource in a &lt;a href="https://helm.sh"&gt;Helm chart&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: rds.services.k8s.aws/v1alpha1 kind: DBInstance metadata: annotations: "service.binding/type": "path={.spec.engine}" "service.binding/provider": "aws" "service.binding/host": "path={.status.endpoint.address}" "service.binding/port": "path={.status.endpoint.port}" "service.binding/username": "path={.spec.masterUsername}" "service.binding/password": 'path={.spec.masterUserPassword.name},objectType=Secret,sourceKey=password' "service.binding/database": "path={.spec.engine}" ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;DBInstance&lt;/code&gt; definition represents an AWS RDS resource.&lt;/p&gt; &lt;p&gt;To define the workload, the Service Binding Operator uses the following label selector (part of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Helm chart):&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: binding.operators.coreos.com/v1alpha1 kind: ServiceBinding metadata: name: servicebinding-rds-endpoint-demo spec: bindAsFiles: true services: - group: rds.services.k8s.aws version: v1alpha1 kind: DBInstance name: {{ .Values.dbinstance.name }} application: labelSelector: matchLabels: psql.provider: aws (*) version: v1 group: apps resource: deployments&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;(*) This line specifies the label that the Service Binding Operator uses to identify the workload.&lt;/p&gt; &lt;p&gt;The Helm charts are available in the &lt;a href="https://github.com/redhat-developer/openshift-app-services-demos"&gt;app-services-samples repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We have not deployed the application yet. Typically, the ServiceBinding controller waits for a workload resource with a matching &lt;code&gt;psql.provider: aws&lt;/code&gt; label. As soon as a workload resource is available with the matching label, the Operator uses the ServiceBinding controller to project the binding values to the workload.&lt;/p&gt; &lt;p&gt;The binding values projects into the &lt;code&gt;/bindings&lt;/code&gt; directory inside the container of the workload resource. The following directory structure stores the values:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;/bindings └── servicebinding-rds-endpoint-demo ├── type ├── database ├── host ├── username └── password&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The REST API application uses a suitable and compliant &lt;a href="https://servicebinding.io/application-developer/#language-specific-libraries"&gt;library&lt;/a&gt; to consume the projected binding values.&lt;/p&gt; &lt;h2&gt;Step 4:  Create a database instance&lt;/h2&gt; &lt;p&gt;After you clone the &lt;a href="https://github.com/redhat-developer/openshift-app-services-demos"&gt;app-services-samples repository&lt;/a&gt; described in the previous section, change to the &lt;code&gt;openshift-app-services-demos/samples/sbo/ack-rds-blog&lt;/code&gt; directory to perform these two steps:&lt;/p&gt; &lt;p&gt;1. Run Helm on the &lt;code&gt;rds-postgre-chart-demo&lt;/code&gt; chart:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm install rds-postgre-chart-demo -n ack-system rds-postgre-chart-demo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the output you should see:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME: rds-postgre-chart-demo LAST DEPLOYED: Thu Aug 4 09:29:26 2022 NAMESPACE: ack-system STATUS: deployed REVISION: 1 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2. Run the following command to validate the database instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get dbinstance rds-test-demo -n ack-system -o=jsonpath='{.status.dbInstanceStatus}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;available&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the database is ready to use.&lt;/p&gt; &lt;h2&gt;Step 5:  Deploy the REST API application&lt;/h2&gt; &lt;p&gt;In this demo, we use the Software Security Module (SSM), a Go-based REST API application. For convenience, deploy the application using the Helm chart in the &lt;a href="https://github.com/redhat-developer/openshift-app-services-demos"&gt;app-services-samples repository&lt;/a&gt;. After you clone the repository, perform the following steps from the &lt;code&gt;openshift-app-services-demos/samples/sbo/ack-rds-blog&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;1. Run Helm on the &lt;code&gt;ssm-chart&lt;/code&gt; chart:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm install ssm-chart -n ack-system ssm-chart&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME: ssm-chart LAST DEPLOYED: Thu Aug 4 04:22:24 2022 NAMESPACE: ack-system STATUS: deployed REVISION: 1 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2. Verify that the deployment of the REST API application is successful by running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get deployment -n ack-system&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME READY UP-TO-DATE AVAILABLE AGE ack-rds-controller 1/1 1 1 28m&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The deployment is defined as follows in the Helm chart:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.k8Name }} annotations: app.kubernetes.io/part-of: ssm labels: psql.provider: aws (*) ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(*) This line specifies the required matching label that the ServiceBinding controller uses to identify the workload and project the bindings.&lt;/p&gt; &lt;p&gt;The ServiceBinding controller watches for a deployment matching the label. After the deployment is ready, the Operator uses the ServiceBinding controller to project the binding values to the workload.&lt;/p&gt; &lt;h2&gt;Step 6:  Access and validate the REST API application&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;ssm-chart&lt;/code&gt; Helm chart also creates an &lt;code&gt;ssm&lt;/code&gt; service resource for convenient access to the application. The &lt;code&gt;ssm&lt;/code&gt; service resource points to the REST API application. Before connecting to this application, make sure you have the &lt;code&gt;DBInstance&lt;/code&gt; resource created and ready with an RDS instance provisioned in the AWS.&lt;/p&gt; &lt;p&gt;Switch to another terminal to run the commands in the following steps.&lt;/p&gt; &lt;h3&gt;1. Access the REST API application by forwarding the port of the service&lt;/h3&gt; &lt;p&gt;An &lt;code&gt;oc&lt;/code&gt; command on OpenShift is useful for port forwarding:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc port-forward --address 0.0.0.0 svc/ssm 8080:8080 -n ack-system&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;2. Validate the application&lt;/h3&gt; &lt;p&gt;Validate that the application works as follows:&lt;/p&gt; &lt;h4&gt;Generate a based64-encoded string&lt;/h4&gt; &lt;p&gt;Start by creating a string from random input:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ openssl rand 32 | base64&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This output contains the string you will use as input in the next step.:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;rgeR0ENzlxG+Erss6tw0gBkBWdLOPrQhEFQpH8O5t/Y=&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;h4&gt;Call the wrap API&lt;/h4&gt; &lt;p&gt;Call the application's &lt;code&gt;wrap&lt;/code&gt; API to create a cipher from the string by using the based64-encoded string from the previous step as input when calling the &lt;code&gt;wrap&lt;/code&gt; API:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl http://localhost:8080/wrap -d '{"key": "rgeR0ENzlxG+Erss6tw0gBkBWdLOPrQhEFQpH8O5t/Y="}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This output contains the cipher string you will use as input in the next step:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{"cipher":"D/S6wDJPH ... "}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;h4&gt;Call the unwrap API&lt;/h4&gt; &lt;p&gt;Now call the application's &lt;code&gt;unwrap&lt;/code&gt; API to restore the original based64 -encoded string by submitting the JSON from the output in the previous section to the &lt;code&gt;unwrap&lt;/code&gt; API:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl http://localhost:8080/unwrap -d '{"cipher":"D/S6wDJPH ... "}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output returns the original based64-encoded string:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{"key":"rgeR0ENzlxG+Erss6tw0gBkBWdLOPrQhEFQpH8O5t/Y="} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;h2&gt;The Service Binding Operator simplifies installation and deployment&lt;/h2&gt; &lt;p&gt;With the annotation support of the Service Binding Operator, you can easily bind ACK services without making any changes to the code. You can use the same label to bind any number of workloads. The REST API application consumes the projected binding values by using one of the &lt;a href="https://servicebinding.io/application-developer/#language-specific-libraries"&gt;libraries&lt;/a&gt; compliant with the Service Binding specification for Kubernetes. You can use the REST API application to connect to the AWS RDS service without any specific change.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/21/bind-services-created-aws-controllers-kubernetes" title="Bind services created with AWS Controllers for Kubernetes"&gt;Bind services created with AWS Controllers for Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Baiju Muthukadan</dc:creator><dc:date>2022-09-21T07:00:00Z</dc:date></entry><entry><title type="html">New visualizer for the Serverless Workflow Editor</title><link rel="alternate" href="https://blog.kie.org/2022/09/new-visualizer-for-the-serverless-workflow-editor.html" /><author><name>Roger Palleja</name></author><id>https://blog.kie.org/2022/09/new-visualizer-for-the-serverless-workflow-editor.html</id><updated>2022-09-21T00:08:57Z</updated><content type="html">We’re happy to announce that a new diagram visualizer for the domain has been released, as part of the kogito tooling 0.23.0, and It becomes as default for the . Kogito – Serverless Workflow Editor – VSCode extension If you are not familiar with the kogito tooling and its extensions, please refer to the guide first. A part from the previous capabilities of the editor, this new diagram visualizer provides a bunch of additional features to help users during the authoring of their workflows, such as: * Automatic workflow reloading It dynamically reloads the workflow’s visualization, once any change is being done in the JSON declaration text panel. * Error Handling In case the workflow’s JSON declaration is not valid (thus the workflow cannot be automatically reloaded), the editor presents the latest valid visualization for the workflow, and also an icon appears on the top right corner: On mouse over the error icon, it  displays the cause of the error as well, by showing a user friendly message. Once the diagram is valid again, the error icon will disappear, and the visualization will be properly updated. * State navigation Once an state is being selected in the diagram visualizer (by clicking on it), the editor automatically navigates to the line, in the JSON declaration, where the state is being defined. * Mediators Users are able to play with mediators either by using the mouse, or by using the available buttons in the mediators bar: * Auto-fit to diagram size: It fits the diagram to the actual viewport size * Zoom: Scales the viewport accordingly (also available by using mouse mediators, please see the keybindings page) * Panning: Translates the viewport accordingly (only available by using mouse mediators, please see the keybindings page) * Export  workflow to SVG From the technical perspective, just mention it is based on , and it relies on Canvas as the main rendering technology. Please keep posted on further updates, new features and improvements coming soon! The post appeared first on .</content><dc:creator>Roger Palleja</dc:creator></entry><entry><title>Quarkus 2.12.3.Final released</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-12-3-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-12-3-final-released/</id><updated>2022-09-21T00:00:00Z</updated><published>2022-09-21T00:00:00Z</published><summary type="html">Today, we released Quarkus 2.12.3.Final, with a new round of bugfixes and documentation improvements. It is a safe upgrade for anyone using 2.12. Migration Guide If you are not already using 2.12, please refer to our migration guide. Full changelog You can get the full changelog of 2.12.3.Final on GitHub....</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-09-21T00:00:00Z</dc:date></entry><entry><title type="html">Transparent ML, integrating Drools with AIX360</title><link rel="alternate" href="https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html</id><updated>2022-09-20T12:10:33Z</updated><content type="html">Following up from about integrating Drools with the Open Prediction Service, in this new post we want to share the current results from another exploration work: this time integrating Drools with research on Transparent Machine Learning by IBM. INTRODUCTION Transparency is a key requirement in many business sectors, from FSI (Financial Services Industry), to Healthcare, to Government institutions, and many others. In more recent years, a generalized need for increased transparency in the decision making processes has gained a great deal of attention from several different stakeholders, especially when it comes to automated decisioning and AI-based decision services. Specifically in the Eurozone, this ties with the and the requirement for explainability in the way businesses automate processes and decision making. Additionally, an “” is proposed and currently under discussion at the European Commission: under the current status of the proposal several risk levels are identified. The integration of AI in the business process and decision model will likely require explainability, transparency and a conformity assessment, depending on the applicable risk level: In other parts of the world, similar legislations are coming into effect or are currently being proposed. You can read more details in . With these considerations in mind, we will explore how to leverage rule induction strategies and specific types of machine learning models, with the intent of producing predictive models which can integrate with effective results into this general context. TRANSPARENT ML WITH DROOLS AND AIX360 One way to address some of the problems and requirements highlighted in the previous section is to use Machine Learning to generate specific types of models that are inherently readable and transparent. As we will see in this blog post, a transparent predictive model can be handed over easily to the next phase as a decision model, in order to be evaluated as-is, but most importantly for the ability to be inspected and authored directly! Comparing a Transparent ML approach with the broader general Machine Learning, we can highlight some of its characteristics: General Machine Learning evaluation:Transparent ML approach:All supported model types, but black box evaluationModel can be inspected, authored, evaluatedAccuracy focusedTransparency focusedeXplainable AI complements, such as Intrinsically eXplainableMLOps —governed by data scienceBusiness centric governanceMultiple runtimesPotentially single runtime Naturally the transparent ML approach has its limitations; we will discuss alternative approaches in the conclusions of this blog post. An example pipeline can be summarized as follows: For the examples in this blog post, we will use the dataset  (predicting if income exceeds $50K/yr from census data). Let’s get started! RULE SET INDUCTION In this section we will make use of the , an open-source library that supports interpretability and explainability of datasets and machine learning models. Our goal in this phase is to generate a predictive model from the UCI Adult dataset, using Machine Learning techniques: To generate a transparent predictive model, we can drive the generation of a RuleSet , as explained in the following Jupyter notebook : As a result of this, we have now generated a set of rules, in the form of a PMML RuleSet, which represents the transparent predictive model for the Adult dataset: If you are interested to delve into more details about using AIX360 and related algorithms, you can check out . DROOLS In this section, we will transform the result from the previous steps into an executable decision model, which can also be directly authored. Please note: in a different context, where the only requirement is the execution of predictive models in general, you can simply make reference to the PMML support for Drools from the , or to integration blueprints such as the integration of Drools with IBM Open Prediction Service from a . In this article instead, as premised, we’re interested in the result of a transparent prediction model, which can be fully inspected, authored and (naturally!) evaluated. Specifically, we will transform the transparent predictive model serialized as a RuleSet, into a DMN model with DMN Decision Tables. To perform this transformation, we will make use of the kie-dmn-ruleset2dmn utility; this is available as a developer API, and as a command line utility too. You can download a published version of the command line utility (executable .jar) from ; otherwise, you can lookup a more recent version directly from . To transform the RuleSet file into a DMN model, you can issue the following command: $ java -jar kie-dmn-ruleset2dmn-cli-8.27.0.Beta.jar adult.pmml --output=adult.dmn This will result in a .dmn file generated, which you can author with the Kogito Tooling and evaluate as usual with the ! We can upload the generated .dmn file onto the sandbox: We can make use of the Kie Sandbox extended services, to evaluate locally the DMN model, as-is or authored as needed! It’s interesting to note the static analysis of the DMN decision table identifies potential gaps in the table, and subsumptions in the rules inducted during the Machine Learning phase; this is expected, and can be authored directly depending on the overall business requirements. From the model evaluation perspective, overlapping rules are not a problem, as they would evaluate to the same prediction; this is a quite common scenario when the ML might have identified overlapping “clusters” or grouping over a number of features, leading to the same output. From a decision table perspective however, overlapping rules can be simplified, as a more compact representation of the same table semantic is often preferable in decision management. Here it is up to the business to decide if to keep the table as translated from the original predictive model, or to leverage the possibilities offered by the transparent ML approach, and simplify/compact the table for easier read and maintenance by the business analyst. DEPLOY We can deploy directly from the KIE Sandbox: Our Transparent prediction and decision model is available as a deployment on OpenShift ! As you can see, with just the click of a button in the KIE Sandbox, our transparent ML model has been easily deployed on OpenShift. If you want to leverage the serverless capabilities of Knative for auto-scaling (including auto scale to zero!) for the same predictive model, you can consider packaging it as a Kogito application. You can find more information in this . CONCLUSION We have seen how a Transparent ML approach can provide solutions to some of the business requirements and conformance needs to regulations such as GDPR or AI Act; we have seen how to drive rule induction by generating predictive models which are inherently transparent, can be authored directly as any other decision model, and can be deployed on a cloud-native OpenShift environment. In this post, we have focused ourselves on using directly upstream AIX360 and Drools. You can refer to the above diagram for commercial solutions by IBM and Red Hat that include these projects too, such as , , . If you are interested in additional capabilities for eXplainable AI solutions, check-out the ! The Transparent ML predictive model, now available as a decision service, can be integrated in other DMN models and other applications, as needed. For example, the transparent prediction on the Adult dataset (predicting if income exceeds $50K/yr) could become invocable as part of another decision service that decides on the applicability for the requests of issuing a certain type of credit card. Another possible integration could be to employ a transparent ML predictive model in the form of scorecards, inside a broader DMN model for segmentation; that is, first identify the applicable category/segment based on the input data, and then apply one of several score cards for the specific segment. Don’t miss on checking out the on related Transparent ML topics! Hope you have enjoyed this blog post, showcasing integration of several technologies to achieve a transparent ML solution! Questions? Feedback? Let us know with the comment section below! Special thanks for Greger Ottosson and Tibor Zimanyi for their help while crafting this content. The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>How hashing and cryptography made the internet possible</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/20/how-hashing-and-cryptography-made-internet-possible" /><author><name>Andy Oram</name></author><id>60312bd5-c40d-4f54-9a4e-fbce728d8518</id><updated>2022-09-20T07:00:00Z</updated><published>2022-09-20T07:00:00Z</published><summary type="html">&lt;p&gt;A lot of technologies, business choices, and public policies gave us the internet we have today—a tremendous boost to the spread of education, culture, and commerce, despite its well-documented flaws. But few people credit two deeply buried technologies for making the internet possible: hashing and cryptography.&lt;/p&gt; &lt;p&gt;If more people understood the role these technologies play, more money and expertise would go toward uncovering and repairing security flaws. For instance, we probably would have fixed the &lt;a href="http://heartbleed.com/"&gt;Heartbleed&lt;/a&gt; programming error much earlier and avoided widespread vulnerabilities in encrypted traffic.&lt;/p&gt; &lt;p&gt;This article briefly explains where hashing and cryptography come from, how they accomplish what they do, and their indelible effect on the modern internet.&lt;/p&gt; &lt;h2&gt;Hashing&lt;/h2&gt; &lt;p&gt;Hashing was &lt;a href="https://www.geeksforgeeks.org/importance-of-hashing/"&gt;invented in the 1950s&lt;/a&gt; at the world's pre-eminent computer firm of that era, IBM, by Hans Peter Luhn. What concerned him at the time was not security—how many computer scientists thought about that?—but saving disk space and memory, the most costly parts of computing back then.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;hash&lt;/em&gt; is a way of reducing each item of data to a small, nearly unique, semi-random string of bits. For instance, if you are storing people's names, you could turn each name into the numerical value of the characters and run a set of adds, multiplies, and shift instructions to produce a 16-bit value. If the hash is good, there will be very few names that produce the same 16-bit value—very few &lt;em&gt;collisions&lt;/em&gt;, as that situation is called.&lt;/p&gt; &lt;p&gt;Now suppose you want to index a database for faster searching. Instead of indexing the names directly, it's much simpler and more efficient to make the index out of 16-bit values. That was one of the original uses for hashes. But they turned out to have two properties that make them valuable for security: No one can produce the original value from the hash, and no one can substitute a different value that produces the same hash. (It is theoretically possible to do either of those things, but doing so would be computationally infeasible, so they're impossible in practice.)&lt;/p&gt; &lt;p&gt;Early Unix systems made use of this property to preserve password security. You created a password along with your user account and gave it to the computer, but the operating system never stored the password itself—it stored only a hash. Every time you entered your password after that, the operating system ran the hash function and let you log in if the resulting hash matched the one in the system. If the password file were snatched up by a malicious intruder, all they would get is a collection of useless hashes. (This clever use of hashes eventually turned out not to be secure enough, so it was replaced with &lt;em&gt;encryption,&lt;/em&gt; which we'll discuss in more detail in the next section of this article.)&lt;/p&gt; &lt;p&gt;Hashes are also good for ensuring that no one has tampered with a document or software program. Injecting malware into free software on popular repositories is not just a theoretical possibility—&lt;a href="https://github.blog/2022-05-26-npm-security-update-oauth-tokens/"&gt;it can actually happen&lt;/a&gt;. Therefore, every time a free software project releases code, the team runs it through a hash function. Every user who downloads the software can run it through the same function to make sure nobody has intercepted the code and inserted malware. If someone changed even one bit and ran the hash function, the resulting hash would be totally different.&lt;/p&gt; &lt;p&gt;Git is another of the myriad tools that use hashes to ensure the integrity of the repository, as well as to enable quick checks on changes to the repository. You can see a hash (a string of random characters) each time you issue a push or log command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;commit 2de089ad3f397e735a45dda3d52d51ca56d8f19a Author: Andy Oram &lt;andyo@example.com&gt; Date: Sat Sep 3 16:28:41 2022 -0400 New material related to commercialization of cryptography. commit f39e7c87873a22e3bb81884c8b0eeeea07fdab48 Author: Andy Oram &lt;andyo@example.com&gt; Date: Fri Sep 2 07:47:42 2022 -0400 Fixed typos. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hash functions can be broken, so &lt;a href="https://valerieaurora.org/hash.html"&gt;new ones are constantly being invented&lt;/a&gt; to replace the functions that are no longer safe.&lt;/p&gt; &lt;h2&gt;Cryptography&lt;/h2&gt; &lt;p&gt;Mathematically speaking, the goal of cryptography has always been to produce output where each bit or character has an equal chance of being another character. If someone intercepted a message and saw the string "xkowpvi," the "x" would have an equal chance of representing an A, a B, a C, and so on.&lt;/p&gt; &lt;p&gt;In digital terms, every bit in an encrypted message has a 50% chance of representing a 0 and a 50% chance of representing a 1.&lt;/p&gt; &lt;p&gt;This goal is related to hashing, and there is a lot of overlap between the fields. Security experts came up with several good ways to create encrypted messages that couldn't be broken—that is, where the decryption process would be computationally infeasible without knowing the secret key used to encrypt the message. But for a long time these methods suffered from an "initial exchange" problem: The person receiving the message needed to somehow also learn what that secret encryption key was, and learn it in a way that didn't reveal the key to anybody else. Whether you're a spy in World War II Berlin trying to communicate with your U.S. buddies, or a modern retail site trying to confirm a customer's credit card online, getting the shared secret securely is a headache.&lt;/p&gt; &lt;p&gt;The solution by now is fairly familiar. The solution creates a pair of keys, one of which you keep private and the other of which you can share freely. Like a hash, the public key is opaque, and no one can determine your private key from it. (The number of bits in the key has to be doubled every decade or so as computers get more powerful.) This solution is generally &lt;a href="https://cryptography.fandom.com/wiki/Diffie%E2%80%93Hellman_key_exchange"&gt;attributed to Whitfield Diffie, Martin Hellman, and Ralph Merkle&lt;/a&gt;, although a British intelligence agent thought of the solution earlier and kept it secret.&lt;/p&gt; &lt;p&gt;Diffie in particular was acutely conscious of social and political reasons for developing public key encryption. In the 1970s, I think that few people thought of doing online retail sales or services using encryption. It was considered a tool of spies and criminals—but also of political dissidents and muckraking journalists. These associations explain why the U.S. government tried to suppress it, or at least keep it from being exported, for decades.&lt;/p&gt; &lt;p&gt;Diffie is still quite active in the field. The most recent article I've seen with him listed as an author was published on July 18, 2022.&lt;/p&gt; &lt;p&gt;The linchpin of internet cryptography came shortly afterward with &lt;a href="https://www.telsy.com/rsa-encryption-cryptography-history-and-uses/"&gt;RSA encryption&lt;/a&gt;, invented by Ron Rivest, Adi Shamir, and Len Adleman. RSA encryption lets two parties communicate without previously exchanging keys, even public keys. (They were prevented from reaping much profit from this historic discovery because the U.S. government prevented the export of RSA technology during most of the life of their patent.)&lt;/p&gt; &lt;p&gt;A big problem in key exchange remains: If someone contacts you and says they are Andy Oram, proffering what they claim to be Andy Oram's public key, how do you know they're really me? The two main solutions (web of trust and certificate authorities) are beyond the scope of this article, and each has vulnerabilities and a lot of overhead. Nevertheless, the internet seems to work well enough with certificate authorities.&lt;/p&gt; &lt;h2&gt;The internet runs on hashes and cryptography&lt;/h2&gt; &lt;p&gt;The internet essentially consists of huge computer farms in data centers, to which administrators and other users have to log in. For many years, the universal way to log into another system was Telnet, now abandoned almost completely because it's insecure. If you use Telnet, someone down the hall can watch your password cross the local network and steal the password. Anyone else who can monitor the network could do the same.&lt;/p&gt; &lt;p&gt;Nowadays, all communication between users and remote computers goes over the secure shell protocol (SSH), which was invented &lt;a href="https://www.oreilly.com/library/view/ssh-the-secure/0596008953/ch01s05.html"&gt;as recently as 1995&lt;/a&gt;. All the cloud computing and other data center administration done nowadays depend on it.&lt;/p&gt; &lt;p&gt;Interestingly, 1995 also saw the advent of the &lt;a href="https://www.techtarget.com/searchsecurity/definition/Secure-Sockets-Layer-SSL"&gt;secure sockets layer&lt;/a&gt; (SSL) protocol, which marks the beginning of web security. Now upgraded to Transport Layer Security (TLS), this protocol is used whenever you enter a URL beginning with HTTPS instead of HTTP. The protocol is so important that &lt;a href="https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html"&gt;Google penalizes web sites that use unencrypted HTTP&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because most APIs now use web protocols, TLS also protects distributed applications. In addition to SSH and TLS, encryption can be found everywhere modern computer systems or devices communicate. That's because the modern internet is beset with attackers, and we use hashes and encryption to minimize their harm.&lt;/p&gt; &lt;p&gt;Some observers think that quantum computing will soon have the power to break encryption as we know it. That could leave us in a scary world: Everything we send over the wire would be available to governments or large companies possessing quantum computers, which are hulking beasts that need to be refrigerated to within a few degrees of absolute zero. We may soon need a &lt;a href="https://nakedsecurity.sophos.com/2022/08/03/post-quantum-cryptography-new-algorithm-gone-in-60-minutes/"&gt;new army of Luhns, Diffies, and other security experts&lt;/a&gt; to find a way to save the internet as we know it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/20/how-hashing-and-cryptography-made-internet-possible" title="How hashing and cryptography made the internet possible"&gt;How hashing and cryptography made the internet possible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andy Oram</dc:creator><dc:date>2022-09-20T07:00:00Z</dc:date></entry></feed>
