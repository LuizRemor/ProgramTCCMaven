<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">RESTEasy 6.2.0.Beta1 Release</title><link rel="alternate" href="https://resteasy.github.io/2022/09/08/resteasy-6.2.0.Beta1-release/" /><author><name /></author><id>https://resteasy.github.io/2022/09/08/resteasy-6.2.0.Beta1-release/</id><updated>2022-09-08T18:11:11Z</updated><dc:creator /></entry><entry><title type="html">This Week in JBoss - 08 September 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-09-08.html" /><category term="quarkus" /><category term="wildfly" /><category term="java" /><category term="wildfly" /><category term="kogito" /><category term="hibernate" /><author><name>Jason Porter</name><uri>https://www.jboss.org/people/jason-porter</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-09-08.html</id><updated>2022-09-08T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, java, wildfly, kogito, hibernate"&gt; &lt;h1&gt;This Week in JBoss - 08 September 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welcome back, glad to have you back with us this week! Progress continues to happen at JBoss, and Red Hat. We have some releases, blogs, and a couple of videos as well.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-12-0-final-released/"&gt;Quarkus 2.12.0&lt;/a&gt; - GraalVM/Mandrel 22.2, Kotlin 1.7, Smallrye Config SecretKeys, and SQL Server JDBC Driver update&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-12-1-final-released/"&gt;Quarkus 2.12.1&lt;/a&gt; - A performance regression fix&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2022/09/08/hibernate-orm-613-final/"&gt;Hibernate ORM 6.1.3&lt;/a&gt; - Maintenance release&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2022/08/30/hibernate-orm-5611-final/"&gt;Hibernate ORM 5.6.11&lt;/a&gt; - Maintenance release (includes performance fixes)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/09/kogito-1-27-0-released.html"&gt;Kogito 1.27.0&lt;/a&gt; - New feature release&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2022/08/31/WildFly2612-Released/"&gt;Wildfly 26.1.2&lt;/a&gt; - Maintenance release&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_blogs"&gt;Blogs&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/09/monitoring-quarkus-applications-with-dashbuilder.html"&gt;Monitoring Quarkus Applications With Dashbuilder&lt;/a&gt; - Building dashboards using DashBuilder&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/redis-api-intro/"&gt;Introducing the new Redis API - How to cache with Redis?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/09/operator-crs"&gt;The future of Keycloak Operator CRs&lt;/a&gt; - Looking at the new way of managing Keycloak resources&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/bearer-only-support-openid-connect/"&gt;Bearer Token Support for the Elytron OIDC Client Subsystem&lt;/a&gt; - Learn how to update your application to support Bearer Tokens using OIDC (OpenID Connect)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/top-five-reasons-to-join-elytron-open-source-day/"&gt;Top 5 Reasons To Join Us At Open Source Day&lt;/a&gt; - September 16 is Open Source Day! Wildfly Electron was selected as a project for this day long hackathon&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/cool-stuff/mongodb/building-java-enteprise-applications-using-mongodb/"&gt;Building Java Enterprise applications using MongoDB&lt;/a&gt; - Dig into MongoDB in a Jakarta EE Application&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/08/25/optimize-loops-long-variables-java"&gt;Optimize loops with long variables in Java&lt;/a&gt; - A look into JVM loop optimizations&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_videos"&gt;Videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=NJglcdL9m7A"&gt;Quarkus Insights #100: EDDI chatbot goes cloud-native with Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_until_next_time"&gt;Until next time!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Everyone stay safe out there, and we look forward to seeing you in two weeks for our next edition!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/jason-porter.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Jason Porter&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Jason Porter</dc:creator></entry><entry><title type="html">Kogito 1.27.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/09/kogito-1-27-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/09/kogito-1-27-0-released.html</id><updated>2022-09-07T12:42:31Z</updated><content type="html">We are glad to announce that the Kogito 1.27.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Adding patch http method for process instance. This allows partial updates of the process model.  * Variable change events are more specific, only the changed value of the Workflow model is published rather than the whole object * Kogito now supports injection of multiple WorkItemHandlerConfig instances * Fix ArrayNode merging to replace the whole array. This prevents duplication of values.  * Clone procedure performance has been improved.    * Exceptions thrown by an action can be stored into a process variable for BPMN.  KNOWN ISSUE(S) * Quarkus SVG addon fails to compile in native mode, see * WorkItemNotFoundException when running OpenAPI generated client in dev mode, see * ClassNotFoundException in enum persistence For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.22.0 artifacts are available at the . A detailed changelog for 1.27.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>Red Hat OpenShift Connectors: Configuring change data capture</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/07/configuring-change-data-capture" /><author><name>Bernard Tison</name></author><id>41f98b40-5aa6-451e-9c6d-2655264e4f6a</id><updated>2022-09-07T07:00:00Z</updated><published>2022-09-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors&lt;/a&gt; is a new cloud service offering from Red Hat. The service provides prebuilt connectors to enable quick and reliable connectivity across data, services, and systems. Each connector is a fully managed service, tightly integrated with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;, Red Hat's managed cloud service for Apache Kafka.&lt;/p&gt; &lt;p&gt;Change data capture (CDC) is a software process that detects changes (inserts, updates, deletes) to data in a database and transforms these changes into event streams that can be consumed by other systems or applications to react to these changes.&lt;/p&gt; &lt;p&gt;Typical use cases for change data capture include: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data replication&lt;/li&gt; &lt;li&gt;Streaming analytics&lt;/li&gt; &lt;li&gt;Event-driven distributed applications&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Red Hat OpenShift Connectors offers several source connectors for change data capture, based on the popular &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt; open source project. OpenShift Connectors support the following databases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PostgreSQL&lt;/li&gt; &lt;li&gt;MySQL&lt;/li&gt; &lt;li&gt;SQL Server&lt;/li&gt; &lt;li&gt;MongoDB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article demonstrates how to configure a source connector to capture data change events from &lt;a href="https://www.mongodb.com/cloud"&gt;MongoDB Atlas&lt;/a&gt;, a fully managed cloud database provided by MongoDB.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;This article assumes that you have created an instance of OpenShift Streams for Apache Kafka and that the instance is in the &lt;code&gt;Ready&lt;/code&gt; state. Please refer to &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; for step-by-step instructions on creating your Kafka instance.&lt;/p&gt; &lt;p&gt;Create a service account and configure the access rules for it. The service account requires privileges to read and write topics and to create new topics. The &lt;a href="https://developers.redhat.com/articles/2022/06/09/get-started-red-hat-openshift-connectors"&gt;Get started with Red Hat OpenShift Connectors&lt;/a&gt; article has detailed instructions on creating and configuring the service account and the access rules.&lt;/p&gt; &lt;h2&gt;Set up a MongoDB Atlas instance&lt;/h2&gt; &lt;p&gt;The following instructions will guide you through the process of setting up a MongoDB Atlas instance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.mongodb.com/cloud/atlas/register"&gt;Sign up&lt;/a&gt; to provision a MongoDB Atlas instance. You can create an Atlas account or sign up with your Google account.&lt;/li&gt; &lt;li&gt;After registration, you are taken to a page where you can choose the type of cloud database you want to provision. MongoDB Atlas offers different tiers, including a free tier for an easy getting-started experience. The free tier is sufficient to follow this demonstration.&lt;/li&gt; &lt;li&gt;When creating your MongoDB Atlas instance, you have to specify the security settings. Choose a username&lt;strong&gt; &lt;/strong&gt;and password and create a user to connect to your MongoDB instance.&lt;/li&gt; &lt;li&gt;Add an IP address to the &lt;strong&gt;IP Access List&lt;/strong&gt;. Enter &lt;code&gt;0.0.0.0/0 &lt;/code&gt; because you don't know the IP address where the managed OpenShift Connector is running. This effectively allows connections to your MongoDB instance from anywhere. You can make the IP Access List more restrictive later on.&lt;/li&gt; &lt;li&gt;Add databases and collections once the MongoDB Atlas instance is up and running.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MongoDB Atlas provides a sample dataset described &lt;a href="https://www.mongodb.com/docs/atlas/sample-data/"&gt;in the documentation&lt;/a&gt;. We will use the &lt;code&gt;sample_mflix&lt;/code&gt; database from the sample dataset. This dataset contains data about movies.&lt;/p&gt; &lt;h2&gt;10 steps to create an instance for change data capture&lt;/h2&gt; &lt;p&gt;Now that you have provisioned and configured a MongoDB Atlas instance and loaded the sample dataset, you can create an OpenShift Connectors source connector to capture change events from the &lt;code&gt;sample_mflix&lt;/code&gt; database. From your &lt;a href="https://console.redhat.com"&gt;Red Hat console&lt;/a&gt;, complete the following 10 steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Application and Data Services&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the &lt;strong&gt;Application and Data Services&lt;/strong&gt; page, select &lt;strong&gt;Connectors&lt;/strong&gt; and click &lt;strong&gt;Create Connectors instance&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;To find the &lt;strong&gt;Debezium MongoDB&lt;/strong&gt; connector, enter &lt;code&gt;mongo&lt;/code&gt; in the search field. Click the &lt;strong&gt;Debezium MongoDB Connector&lt;/strong&gt; card, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the &lt;strong&gt;Streams for Apache Kafka&lt;/strong&gt; instance for the connector. (This is the instance you created in the prerequisites step.) Then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the &lt;strong&gt;Namespace&lt;/strong&gt; page, click &lt;strong&gt;Create preview namespace&lt;/strong&gt; to provision a namespace for hosting the connector instances that you create (or select your existing namespace if you created one earlier). This evaluation namespace will remain available for 48 hours. You can create up to four connector instances per namespace. Once the namespace is available, select it and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide the core configuration for your connector by entering the following values:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;A unique name for the connector.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; of the service account that you created for your connectors.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide the connection configuration for your connector. For the &lt;strong&gt;Debezium MongoDB&lt;/strong&gt; connector, enter the following information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hosts&lt;/strong&gt;: The set of the replicaset public addresses for your MongoDB instance. You can find these on the web console for your MongoDB instance. Click the &lt;strong&gt;Overview&lt;/strong&gt; tab shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mongo-replicaset.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mongo-replicaset.png?itok=CpbJzBQ2" width="600" height="262" alt="A screenshot of the public addresses for the MongoDB replicaset. " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Overview tab on the web console of the MongoDB Atlas instance shows the public addresses of the MongoDB replicaset. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click the first link to get to the status page of the replica. The address of the replica is shown at the top of the page in &lt;code&gt;host:port&lt;/code&gt; format. Copy the address to a text editor.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Repeat the procedure for the other replicas.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Go back to the OpenShift Connectors configuration page and enter the addresses of the three replicas separated by commas in the &lt;strong&gt;Hosts&lt;/strong&gt; field. The entry should look like the following list (your values will be different):&lt;/p&gt; &lt;p&gt;&lt;code class="java"&gt;ac-whrxxxx-shard-00-00.ooulprt.mongodb.net:27017, ac-whrxxxx-shard-00-01.ooulprt.mongodb.net:27017, ac-whrxxxx-shard-00-02.ooulprt.mongodb.net:27017&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Namespace&lt;/strong&gt;: A unique name that identifies this MongoDB instance. For example, enter &lt;code&gt;mongo-mflix&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Password&lt;/strong&gt;: The password of the database user you created previously. Note that this is not the same user with which you logged into MongoDB Atlas.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;: The user name of the database user you created.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Make sure the &lt;strong&gt;Enable SSL connection to MongoDB&lt;/strong&gt; is checked.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 2 shows what a filled-out form looks like.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_12.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image1_12.png?itok=Ebn64xZ_" width="600" height="348" alt="A screenshot of configuration properties for MongoDB Debezium connector." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Enter configuration properties for the MongoDB Debezium connector. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the next page of the wizard, set the &lt;strong&gt;Database filter&lt;/strong&gt; to &lt;code&gt;sample_mflix&lt;/code&gt; and the &lt;strong&gt;Collection filter&lt;/strong&gt; to &lt;code&gt;sample_mflix.movies. &lt;/code&gt; This will ensure you capture change events only from the &lt;code&gt;movies&lt;/code&gt; collection.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Make sure that the &lt;strong&gt;Include&lt;/strong&gt; box is selected for both entries. Click &lt;strong&gt;Apply&lt;/strong&gt; to apply the filter. [Do not change the values on the &lt;strong&gt;Data &amp; runtime&lt;/strong&gt; page of the wizard. These are advanced values that you rarely need to change.]&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Review the summary of the configuration properties. Pay particular attention to the MongoDB &lt;strong&gt;Hosts&lt;/strong&gt; field. Click &lt;strong&gt;Create Connector&lt;/strong&gt; to create the connector.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your connector instance will be added to the table of connectors. After a couple of seconds, the status of your connector instance should change to &lt;code&gt;Ready&lt;/code&gt;. If your connector ends up in an &lt;code&gt;Error&lt;/code&gt; state, you can click the options icon (the three vertical dots) next to the connector. Then edit the configuration and restart the connector.&lt;/p&gt; &lt;h2&gt;Capture data change events from MongoDB&lt;/h2&gt; &lt;p&gt;Once the Debezium MongoDB connector is ready, it connects to the MongoDB database, creates a snapshot of the collections it monitors, and creates data change events for every record present in the collection.&lt;/p&gt; &lt;p&gt;To verify, use the message viewer in the OpenShift Streams for Apache Kafka web console.&lt;/p&gt; &lt;p&gt;Head over to the &lt;strong&gt;Application and Data Services&lt;/strong&gt; page of the Red Hat console and select &lt;strong&gt;Streams for Apache Kafka→Kafka Instances&lt;/strong&gt;. Click the name of the Streams for the Apache Kafka instance that you created for connectors. Select the &lt;strong&gt;Topics&lt;/strong&gt; tab.&lt;/p&gt; &lt;p&gt;You should see four new topics. Debezium connectors run on top of Kafka Connect (in contrast to the other OpenShift Connectors instances, which are based on Camel-K). Kafka Connect creates three topics to maintain its internal state. These are the topics ending with &lt;code&gt;-config&lt;/code&gt;, &lt;code&gt;-offset&lt;/code&gt;, and &lt;code&gt;-status&lt;/code&gt; shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/connectors-dbz-topics.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/connectors-dbz-topics.png?itok=Fq3nwp4K" width="1440" height="534" alt="The Kafka Connect connector creates four topics." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The Kafka Connect connector creates four topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The other topic, named &lt;code&gt;mongo-mflix.sample_mflix.movies&lt;/code&gt;, holds the data change events from the movies collection. Click the topic name and select the &lt;strong&gt;Messages&lt;/strong&gt; tab. You should see the most recent ten messages in the topic, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/data-change-event-messages.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/data-change-event-messages.png?itok=EXj1cQO8" width="1440" height="793" alt="A topic's Messages tab displays recent messages in the topic." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: A topic's Messages tab displays recent messages in the topic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The offset of the last message in Figure 4 is 23529, which indicates that there are 23530 events in the topic. This corresponds to the number of records in the &lt;code&gt;movies&lt;/code&gt; collection.&lt;/p&gt; &lt;p&gt;Each data change message has a JSON payload with the following structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;after&lt;/strong&gt;: Contains the latest state of the document.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;source&lt;/strong&gt;: Contains metadata about the connector and the MongoDB instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;op&lt;/strong&gt;: Specifies the operation that created this change. In this case, the operation is &lt;code&gt;r&lt;/code&gt;, which stands for &lt;code&gt;read&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please refer to the &lt;a href="https://debezium.io/documentation/reference/1.9/"&gt;Debezium documentation&lt;/a&gt; for more information on Debezium and the structure of the data change events it produces.&lt;/p&gt; &lt;p&gt;At this point, you can add new records to the MongoDB collection or modify existing records. The easiest way to do so is through the MongoDB Atlas web console:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On the &lt;strong&gt;Database&lt;/strong&gt; page, select the &lt;strong&gt;Collections&lt;/strong&gt; tab&lt;/li&gt; &lt;li&gt;Then select the &lt;code&gt;sample_mflix.movies&lt;/code&gt; collection.&lt;/li&gt; &lt;li&gt;From here you can add new records to the collection or modify existing ones. Every time you make a change, a data change event is produced with the latest state of the document and an operation equal to &lt;code&gt;c&lt;/code&gt; for creates and &lt;code&gt;u&lt;/code&gt; for updates.&lt;/li&gt; &lt;li&gt;You can verify that change events are generated by checking the message viewer of the data change event topic in the Red Hat console.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;An end-to-end data pipeline example&lt;/h2&gt; &lt;p&gt;Capturing data change events from a database with an OpenShift Connector instance is just the first step in using the data. Typically, the data change events are consumed by other services or applications to, for instance, replicate the data or build a local view of the data.&lt;/p&gt; &lt;p&gt;The following video contains a demo of what an end-to-end data pipeline could look like. The demo uses OpenShift Connectors to stream the data change events to &lt;a href="https://aws.amazon.com/kinesis/"&gt;AWS Kinesis&lt;/a&gt;. The events trigger an AWS Lambda function that extracts the state of the document from the event and updates an AWS OpenSearch index. Figure 5 shows the pipeline.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/connectors-data-pipeline_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/connectors-data-pipeline_0.png?itok=CD0w0bc5" width="958" height="544" alt="A flow chart illustrating OpenShift Connectors feeds change events." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Red Hat OpenShift Connectors feeds change events from the database to consumers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h3&gt;Watch this quick CDC pipeline demo:&lt;/h3&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;OpenShift Connectors speed up data collection applications&lt;/h2&gt; &lt;p&gt;We hope you found this demonstration informative and easy to follow. Try &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors&lt;/a&gt; for yourself and see how they speed up data collection applications. Feel free to reach out to us. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/07/configuring-change-data-capture" title="Red Hat OpenShift Connectors: Configuring change data capture"&gt;Red Hat OpenShift Connectors: Configuring change data capture&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bernard Tison</dc:creator><dc:date>2022-09-07T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.12.1.Final released - Fixes a performance regression</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-12-1-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-12-1-final-released/</id><updated>2022-09-07T00:00:00Z</updated><published>2022-09-07T00:00:00Z</published><summary type="html">2.12.1.Final is the first maintenance release of the 2.12 release train. If you have already upgraded to 2.12, we highly recommend this upgrade as it fixes, amongst other things, a performance regression introduced in Quarkus 2.12.0.Final. It is a safe upgrade for anyone using 2.12. Migration Guide If you are...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-09-07T00:00:00Z</dc:date></entry><entry><title type="html">Monitoring Quarkus applications with Dashbuilder</title><link rel="alternate" href="https://blog.kie.org/2022/09/monitoring-quarkus-applications-with-dashbuilder.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/09/monitoring-quarkus-applications-with-dashbuilder.html</id><updated>2022-09-06T18:18:42Z</updated><content type="html">Dalle-2 generated Chart image Most of extensions allow users to expose information in JSON format or metrics to be used by Prometheus. What I miss is a tool that allows us to visualize data from Quarkus or even embed dashboards in my Quarkus application. In this article we will share a tool called Dashbuilder to help us to build dashboards that run on the client-side, without the requirement of installing anything and be used to monitor Quarkus. DASHBOARDS WITH DASHBUILDER Dashbuilder is a Web Application that runs Dashboards in format. It can consume data from JSON arrays, CSV and metrics (the same metrics scrapped by Prometheus). You can transform JSON to arrays using JSONAta expressions. The data can be displayed in pages composed of Displayers. Displayers can be anything that shows the data, such as a Bar Chart or a Heatmap. To create dashboards you can use the great . Once the YAML is done then you can deploy the dashboard using the Dashbuilder web app from NPM package with the instructions from . Now you may be asking yourself how Dashbuilder can help us with Quarkus applications. Any application that has metrics exposed can be monitored by Dashbuilder without the requirement of using Prometheus or Grafana. DASHBOARDS FOR QUARKUS MONITORING To monitor quarkus with Dashbuilder only two configurations are required on the Quarkus side: * Add the extension quarkus-micrometer-registry-prometheus * If the dashboard is not embedded in Quarkus, then you need to enable cors using  quarkus.http.cors=true in application.properties Now you can use the Dashbuilder to build dashboards: * Start your quarkus app in dev mode:  * mvn clean compile quarkus:dev * Check if the metrics are available at http://localhost:8080/q/metrics * Go to the Dashbuilder , create a new Dashboard and use the following YAML content: datasets: - uuid: metrics url: http://localhost:8080/q/metrics pages: - components: - settings: lookup: uuid: metrics Now you should be able to see a table with all metrics: From here you can explore the to build more complex dashboards. In our online editor you can start This dashboard reads the metrics for real time information. Users can change a property on top of the YAML file to make it auto-update (data polling). Notice that this dashboard does not require the use of Prometheus, but we don’t keep the track of the metrics, however, it is possible to use Dashbuilder with Prometheus as described in . EMBEDDING DASHBOARDS IN A QUARKUS APPLICATION Dashboards can be embedded in a Quarkus application by simply unzipping Dashbuilder content into the static content folder (resources/META-INF/resources). To make it easy to use Dashbuilder we created a for the NPM package.  You can check a sample application in or follow the steps below to embed dashbuilder in your application: * Create the files setup.js and your YAML dashboard in the directory that will contain dashbuilder content. In our case the directory is dashbuilder: src/main/resources/META-INF/resources/dashbuilder/setup.js dashbuilder = { dashboards: ["hello.dash.yaml"] } src/main/resources/META-INF/resources/dashbuilder/hello.dash.yaml datasets: - uuid: metrics url: /q/metrics pages: - components: - settings: lookup: uuid: metrics * Add the Dashbuilder web jar version to the properties section of your pom.xml &lt;dashbuilder.version&gt;0.22.0&lt;/dashbuilder.version&gt; * Add the Web Jar as a dependency in the dependencies section: &lt;dependency&gt; &lt;groupId&gt;org.webjars.npm&lt;/groupId&gt; &lt;artifactId&gt;kie-tools__dashbuilder-client&lt;/artifactId&gt; &lt;version&gt;${dashbuilder.version}&lt;/version&gt; &lt;/dependency&gt; * Add a plugin to unpack Dashbuilder in the plugins section:       &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;unpack-dashbuilder&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;unpack&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactItems&gt; &lt;artifactItem&gt; &lt;groupId&gt;org.webjars.npm&lt;/groupId&gt; &lt;artifactId&gt;kie-tools__dashbuilder-client&lt;/artifactId&gt; &lt;version&gt;${dashbuilder.version}&lt;/version&gt; &lt;overWrite&gt;true&lt;/overWrite&gt; &lt;outputDirectory&gt;${project.build.directory}/dashbuilder&lt;/outputDirectory&gt; &lt;/artifactItem&gt; &lt;/artifactItems&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;overWriteSnapshots&gt;true&lt;/overWriteSnapshots&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; * Finally copy the contents from the dashbuilder package to the destination folder &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dashbuilder-resources&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;${project.build.outputDirectory}/META-INF/resources/dashbuilder&lt;/outputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;${project.build.directory}/dashbuilder/META-INF/resources/webjars/kie-tools__dashbuilder-client/${dashbuilder.version}/dist/&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; That’s all! When you start your app then the dashboard will be available in localhost:8080/dashbuilder CONCLUSION In this post we shared Dashbuilder, an alternative to monitor Quarkus applications! Check the sample project at . The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Build trust in continuous integration for your Rust library</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/06/build-trust-continuous-integration-your-rust-library" /><author><name>Gris Ge</name></author><id>af4e4d46-5834-4409-9225-eb8bfeaf27b1</id><updated>2022-09-06T07:00:00Z</updated><published>2022-09-06T07:00:00Z</published><summary type="html">&lt;p&gt;This article concludes the 4-part series about how to take advantage of the recent Rust support added to Linux. I hope you have read the previous articles in the series:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2022/07/15/3-essentials-writing-linux-system-library-rust"&gt;3 essentials for writing a Linux system library in Rust&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2022/07/29/how-create-c-binding-rust-library"&gt;How to create C binding for a Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2022/08/11/how-create-python-binding-rust-library"&gt;How to create Python binding for a Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article will demonstrate how to build trust in a Continuous Integration (CI) system for your Rust library.&lt;/p&gt; &lt;p&gt;You can download the demo code from its &lt;a href="https://github.com/cathay4t/librabc"&gt;GitHub repository&lt;/a&gt;. The package contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An echo server listening on the Unix socket &lt;code&gt;/tmp/librabc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;A Rust crate that connects to the socket and sends a &lt;code&gt;ping&lt;/code&gt; packet every 2 seconds&lt;/li&gt; &lt;li&gt;A C/Python binding&lt;/li&gt; &lt;li&gt;A command-line interface (CLI) for the client&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The CI system of this &lt;a href="https://github.com/cathay4t/librabc"&gt;GitHub repository&lt;/a&gt; is built on Github Action with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rust lint check using &lt;code&gt;cargo fmt&lt;/code&gt; and &lt;code&gt;cargo clippy&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Python lint check using &lt;code&gt;pylint&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Rust unit test.&lt;/li&gt; &lt;li&gt;C memory leak test.&lt;/li&gt; &lt;li&gt;Integration test on CentOS stream 8 and CentOS stream 9.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We find that &lt;code&gt;pytest&lt;/code&gt; framework provides more control over how the test case runs than Rust native &lt;code&gt;cargo test&lt;/code&gt;. Hence, we will use &lt;code&gt;pytest&lt;/code&gt; as an integration test framework.&lt;/p&gt; &lt;h2&gt;How to build trust in continuous integration in 4 steps:&lt;/h2&gt; &lt;p&gt;Open source projects receive contributions around the world from contributors with different code skill sets and habits. Therefore, we strongly recommend maintaining trust in the CI system for every critical systems project. The goal of a CI system is to build trust by ensuring that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The pending patch will not introduce any regression.&lt;/li&gt; &lt;li&gt;A test case attached to the pending patch proves what it fixed.&lt;/li&gt; &lt;li&gt;New features are tested with commonly used test cases.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Step 1. Isolation test environment and test setup&lt;/h3&gt; &lt;p&gt;Effective isolation is the first thing to consider when you design the CI system. When the CI system utilizes a large bash script to set up a complex environment for your test case to run, porting this CI to a new CI platform or debugging a certain test case would be difficult. This arduous task would result in the following issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mixing CI platform-related code with test setup code complicates the efforts of developers debugging specific test cases in their local environment.&lt;/li&gt; &lt;li&gt;New contributors would have to complete a lengthy document for their first contribution with a test case attached, making the project less friendly to the open source community.&lt;/li&gt; &lt;li&gt;A large bash script could become bloated with enormous race problems.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our &lt;a href="https://github.com/cathay4t/librabc"&gt;demo project&lt;/a&gt; comprises three isolated layers for CI setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Layer 1: The &lt;a href="https://github.com/cathay4t/librabc/blob/main/.github/workflows/main.yaml"&gt;&lt;code&gt;.github/workflows/main.yaml&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/cathay4t/librabc/blob/main/.github/runtest.sh"&gt;&lt;code&gt;.github/runtest.sh&lt;/code&gt;&lt;/a&gt; contain the CI platform (Github Action) specific setup codes that: test artifacts, run test case run on the matrix of toolsets combinations, invoke the test in different containers, and install the package of the current project.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The first layer is CI platform specific. Thus, you should refer to their documentation for detail.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Layer 2: The &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/runtest.sh"&gt;&lt;code&gt;tests/runtest.sh&lt;/code&gt;&lt;/a&gt; contains the test environment setup codes that include a helper for running the test in developer mode, and a specific argument passing to &lt;code&gt;pytest&lt;/code&gt;. It has &lt;strong&gt;zero&lt;/strong&gt; lines of code for the environment setup of a specific test case.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The second layer is test framework specific, the developer should find out the suitable test framework.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Layer 3: The &lt;code&gt;rabc_daemon()&lt;/code&gt; pytest fixture in &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/integration/rabc_test.py"&gt;&lt;code&gt;tests/integration/rabc_test.py&lt;/code&gt;&lt;/a&gt; contains the environment setup for a certain test case.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's elaborate on the third layer's pytest fixture which is designed to setup the environment and clean up after the test case finishes (fail or pass).&lt;/p&gt; &lt;p&gt;To use pytest fixture to setup the test environment, we have the following lines in &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/integration/rabc_test.py"&gt;&lt;code&gt;tests/integration/rabc_test.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="python"&gt;@pytest.fixture(scope="session", autouse=True) def rabc_daemon(): daemon = subprocess.Popen( "rabcd", stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid, ) yield os.killpg(daemon.pid, signal.SIGTERM) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the &lt;code&gt;@pytest.fixture(scope="session", autouse=True)&lt;/code&gt; decorator, the whole test session will have &lt;code&gt;rabcd&lt;/code&gt; daemon started beforehand and stopped afterward. The&lt;code&gt;pytest&lt;/code&gt; will handle the failure of daemon start/stop properly.&lt;/p&gt; &lt;p&gt;The pytest also has scopes for &lt;code&gt;module&lt;/code&gt;, &lt;code&gt;class&lt;/code&gt;, and &lt;code&gt;function&lt;/code&gt; for test environment setup/cleanup.&lt;/p&gt; &lt;p&gt;By chaining the pytest fixtures, we could split test fixtures into small parts and reuse them between test cases. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class="python"&gt;@pytest.fixture def setup_a(): start_a() yield stop_a() @pytest.fixture def setup_b(): start_b() yield stop_b() @pytest.fixture def setup_ab(setup_a, setup_b): yield @pytest.fixture def setup_ba(setup_b, setup_a): yield &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the above example code, the fixtures &lt;code&gt;setup_ab&lt;/code&gt; and &lt;code&gt;setup_ba&lt;/code&gt; are holding different orders of setup and cleanup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;setup_ab&lt;/code&gt; will run &lt;code&gt;setup_a()&lt;/code&gt; and then &lt;code&gt;setup_b()&lt;/code&gt; before test starts.&lt;/li&gt; &lt;li&gt;&lt;code&gt;setup_ab&lt;/code&gt; will run &lt;code&gt;stop_b()&lt;/code&gt; and then &lt;code&gt;stop_a()&lt;/code&gt; after test ends.&lt;/li&gt; &lt;li&gt;&lt;code&gt;setup_ba&lt;/code&gt; will run &lt;code&gt;setup_b()&lt;/code&gt; and then &lt;code&gt;setup_a()&lt;/code&gt; before test starts.&lt;/li&gt; &lt;li&gt;&lt;code&gt;setup_ba&lt;/code&gt; will run &lt;code&gt;stop_a()&lt;/code&gt; and then &lt;code&gt;stop_b()&lt;/code&gt; after test ends.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Step 2. Minimize the use of the internet during the test&lt;/h3&gt; &lt;p&gt;A single glitch of internet access in the CI platform might fail our test and will waste our efforts on debugging that failure. So we take the following actions to minimize the use of the internet during tests:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use a prebuild container with all required packages.&lt;/li&gt; &lt;li&gt;Use CI platform-specific way for test host setup.&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;rpm&lt;/code&gt;/&lt;code&gt;dpdk&lt;/code&gt; instead of &lt;code&gt;dnf&lt;/code&gt;/&lt;code&gt;apt-get&lt;/code&gt; to avoid repository problems.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the demo project, we use &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/container/Dockerfile.c9s-rabc-ci"&gt;&lt;code&gt;tests/container/Dockerfile.c9s-rabc-ci&lt;/code&gt;&lt;/a&gt; to define all the required packages necessary for building the project and running the test. The &lt;a href="https://quay.io/repository/librabc/c9s-rabc-ci"&gt;quay.io&lt;/a&gt; will automatically rebuild the container on every merged commit. Some CI platforms can even cache the container to speed up the test run.&lt;/p&gt; &lt;p&gt;Using containers could also eliminate the failure caused by the upgrade of the test framework (e.g. pytest, tox, etc).&lt;/p&gt; &lt;p&gt;Instead of using your own script to test the project on multiple branches of Rust or Python versions, you can trust the CI platform-specific way which is fast, well tested, and well maintained. For example, we could install Rust in Github Action within one second via:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;- name: Install Rust stable uses: actions-rs/toolchain@v1 with: toolchain: stable override: true components: rustfmt, clippy &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3. Group test cases into tiers&lt;/h3&gt; &lt;p&gt;Normally, I group test cases into these tiers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;tier1&lt;/code&gt;: Test cases for real use cases learned from project consumers. This tier is used for gating on building a downstream package or running downstream tests.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;tier2&lt;/code&gt;: Not real use case but for code coverage.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;slow&lt;/code&gt;: Slow test cases require massive CPU and memory resources. This tier is used for running special test cases on dedicated test hosts.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You could put &lt;code&gt;@pytest.mark.tier1&lt;/code&gt; decorator on your &lt;code&gt;pytest&lt;/code&gt; test cases, and invoke them via &lt;code&gt;pytest -m tier1&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Step 4. Enforcing the merging rules&lt;/h3&gt; &lt;p&gt;Once the CI system is up and running, developers with commit rights should enforce the following rules to maintain trust in the CI system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A patch can only be merged with CI passing or explained CI failure.&lt;/li&gt; &lt;li&gt;Each bug fix should contain a test case proving what it fixed. The patch reviewer should run the test case without the fix to reproduce the original problem. A unit test case is required when possible.&lt;/li&gt; &lt;li&gt;Each new feature should contain an integration test case explaining the common use case of this feature.&lt;/li&gt; &lt;li&gt;Fix the random failures as soon as possible.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Without enforcement, a CI system can violate this trust due to random failures and code coverage deficit.&lt;/p&gt; &lt;p&gt;In the &lt;a href="https://github.com/cathay4t/librabc"&gt;demo git project&lt;/a&gt;, we have a CI system built up with these guidelines.&lt;/p&gt; &lt;p&gt;You may check the test results in the &lt;code&gt;Checks&lt;/code&gt; tab of &lt;a href="https://github.com/cathay4t/librabc/pull/2"&gt;this pull request&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Tips for the CI system of the Rust project&lt;/h2&gt; &lt;p&gt;Here are some tips for testing the system library in Rust:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run a Rust unit test for non-public API.&lt;/li&gt; &lt;li&gt;Do a memory leak check for C binding written in Rust.&lt;/li&gt; &lt;li&gt;Pytest log collection&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Rust Unit Test for non-public API&lt;/h3&gt; &lt;p&gt;The unit test cases are supposed to test isolated function input and output without running the whole project in a real environment.&lt;/p&gt; &lt;p&gt;The Rust official document demonstrates how &lt;a href="https://doc.rust-lang.org/book/ch11-00-testing.html"&gt;&lt;code&gt;cargo test&lt;/code&gt;&lt;/a&gt; uses the automation test. But that is for the integration test, hence your test code can only access &lt;code&gt;pub&lt;/code&gt; functions and structures.&lt;/p&gt; &lt;p&gt;Since we are building up unit test cases, testing on &lt;code&gt;pub(crate)&lt;/code&gt; functions and structures is also required. We can place our test code as &lt;code&gt;mod&lt;/code&gt; of the current crate and mark test functions with &lt;code&gt;#[test]&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You may refer to the entire test code at &lt;code&gt;src/lib/unit_tests&lt;/code&gt; of &lt;a href="https://github.com/cathay4t/librabc/tree/main/src/lib/unit_tests"&gt;github repo&lt;/a&gt; or follow these steps:&lt;/p&gt; &lt;p&gt;In &lt;code&gt;lib.rs&lt;/code&gt;, we have the unit test case folder included as a normal crate internal module:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;mod unit_tests; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In &lt;code&gt;unit_tests/mod.rs&lt;/code&gt;, we mark &lt;code&gt;unit_tests/timer.rs&lt;/code&gt; as the test module:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[cfg(test)] mod timer; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, in &lt;code&gt;unit_tests/timer.rs&lt;/code&gt;, you can access &lt;code&gt;pub(crate)&lt;/code&gt; structure using the code &lt;code&gt;use crate::timer::RabcTimer&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Memory leak check for C binding written in Rust&lt;/h3&gt; &lt;p&gt;I recommend a memory leak check since we used &lt;code&gt;unsafe&lt;/code&gt; keywords for the Rust raw pointer in the C binding.&lt;/p&gt; &lt;p&gt;You may refer to the complete test code at &lt;code&gt;src/clib&lt;/code&gt; of the &lt;a href="https://github.com/cathay4t/librabc/tree/main/src/lib/unit_tests"&gt;github repo&lt;/a&gt; or follow this example.&lt;/p&gt; &lt;p&gt;At the top of &lt;code&gt;Makefile&lt;/code&gt;, we defined &lt;code&gt;make clib_check&lt;/code&gt; as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;.PHONY: clib_check clib_check: $(CLIB_SO_DEV_DEBUG) $(CLIB_HEADER) $(DAEMON_DEBUG) $(eval TMPDIR := $(shell mktemp -d)) cp $(CLIB_SO_DEV_DEBUG) $(TMPDIR)/$(CLIB_SO_FULL) ln -sfv $(CLIB_SO_FULL) $(TMPDIR)/$(CLIB_SO_MAN) ln -sfv $(CLIB_SO_FULL) $(TMPDIR)/$(CLIB_SO_DEV) cp $(CLIB_HEADER) $(TMPDIR)/$(shell basename $(CLIB_HEADER)) cc -g -Wall -Wextra -L$(TMPDIR) -I$(TMPDIR) \ -o $(TMPDIR)/rabc_test src/clib/tests/rabc_test.c -lrabc $(DAEMON_DEBUG) &amp; LD_LIBRARY_PATH=$(TMPDIR) \ valgrind --trace-children=yes --leak-check=full \ --error-exitcode=1 \ $(TMPDIR)/rabc_test 1&gt;/dev/null rm -rf $(TMPDIR) pkill $(DAEMON_EXEC) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Generally, it links the &lt;code&gt;src/clib/tests/rabc_test.c&lt;/code&gt; to the Rust C binding stored in a temporary folder and runs &lt;code&gt;valgrind&lt;/code&gt; for the memory check.&lt;/p&gt; &lt;h3&gt;Pytest log collection&lt;/h3&gt; &lt;p&gt;Many CI platforms support uploading test artifacts. Instead of outputting everything to the console, please store debug logs to files and only print necessary lines to the console. With that, we can identify what went wrong at the first glance of the test console output and still able to investigate it with the debug log in test artifacts.&lt;/p&gt; &lt;p&gt;In pytest, we use these options:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pytest -vvv --log-file-level=DEBUG \ --log-file-date-format='%Y-%m-%d %H:%M:%S' \ --log-file-format='%(asctime)s %(filename)s:%(lineno)d %(levelname)s %(message)s' \ --log-file=${TEST_ARTIFACTS_FOLDER}/rabc_test.log \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will store the DEBUG+ logs to &lt;code&gt;${TEST_ARTIFACTS_FOLDER}/rabc_test.log&lt;/code&gt; file instead of dumping into the console. It will be uploaded by &lt;a href="https://github.com/cathay4t/librabc/blob/main/.github/runtest.sh"&gt;&lt;code&gt;.github/runtest.sh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The Rust language is an elegant language for a Linux system library. During my work on &lt;a href="https://github.com/nmstate/nmstate"&gt;nmstate&lt;/a&gt; and &lt;a href="https://github.com/nispor/nispor/"&gt;nispor&lt;/a&gt; projects, it saved us from the worry of thread and memory safety. A trustworthy CI system enables us to embrace open source contributions around the world with confidence.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/06/build-trust-continuous-integration-your-rust-library" title="Build trust in continuous integration for your Rust library"&gt;Build trust in continuous integration for your Rust library&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Gris Ge</dc:creator><dc:date>2022-09-06T07:00:00Z</dc:date></entry><entry><title>How to create C binding for a Rust library</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/05/how-create-c-binding-rust-library" /><author><name>Gris Ge</name></author><id>e48f9d2b-2a03-42cf-b213-c51d8d6649bc</id><updated>2022-09-05T07:00:00Z</updated><published>2022-09-05T07:00:00Z</published><summary type="html">&lt;p&gt;This article is the second installment of a series about how to take advantage of the recent Rust support added to Linux. The first article in the series, &lt;a href="https://developers.redhat.com/articles/2022/07/15/3-essentials-writing-linux-system-library-rust"&gt;3 essentials for writing a Linux system library in Rust&lt;/a&gt;, describes special considerations that system libraries require when you are writing in Rust. This article demonstrates how to create a C binding so that programmers in C or C++ can call your Rust library. Rust has not conquered the Linux world yet, so our system library needs to provide bindings to other languages.&lt;/p&gt; &lt;p&gt;Check out the other three articles in this series:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2022/07/15/3-essentials-writing-linux-system-library-rust"&gt;3 essentials for writing a Linux system library in Rust&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2022/08/11/how-create-python-binding-rust-library"&gt;How to create Python binding for a Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2022/08/21/build-trust-continuous-integration-your-rust-library"&gt;Build trust in continuous integration for your Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Rust team has created a great document, &lt;a href="https://rust-lang.github.io/api-guidelines/"&gt;Rust API Guidelines&lt;/a&gt;, about how to create a robust Rust library and crate. This article focuses on Linux-specific topics.&lt;/p&gt; &lt;p&gt;You can download the demo code from its &lt;a href="https://github.com/cathay4t/librabc"&gt;GitHub repository&lt;/a&gt;. The package contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An echo server listening on the Unix socket &lt;code&gt;/tmp/librabc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;A Rust crate that connects to the socket and sends a &lt;code&gt;ping&lt;/code&gt; packet every 2 seconds&lt;/li&gt; &lt;li&gt;A C/Python binding&lt;/li&gt; &lt;li&gt;A command-line interface (CLI) for the client&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Elements of a C binding&lt;/h2&gt; &lt;p&gt;Rust can generate C dynamic libraries (&lt;code&gt;.so&lt;/code&gt; files) as well as static libraries (&lt;code&gt;.a&lt;/code&gt; files), which can be easily wrapped in Go bindings and Python bindings and used in code written in those languages.&lt;/p&gt; &lt;p&gt;You can refer to the full code of the C library written in Rust in &lt;a href="https://github.com/cathay4t/librabc/tree/main/src/clib"&gt;the clib folder of the GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following line in the repository's &lt;code&gt;Cargo.toml&lt;/code&gt; file generates a C binding to your Rust library as a &lt;code&gt;.so&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class="toml"&gt;crate-type = ["cdylib"] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use &lt;code&gt;staticlib&lt;/code&gt; in place of &lt;code&gt;cdylib&lt;/code&gt; to generate a static library.&lt;/p&gt; &lt;p&gt;Two elements of the library code &lt;code&gt;lib.rs&lt;/code&gt; should be noted here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;#[no_mangle]&lt;/code&gt; macro before each function instructs the Rust compiler not to add special characters to symbol names as it does for Rust native code. The symbols are left plain so that C code can link to this file and refer to the symbols.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;extern "C"&lt;/code&gt; keywords on functions instruct Rust to use the system ABI (glibc in the case of Linux) instead of the Rust ABI to accommodate the C linker.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;An example of a function follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[no_mangle] pub extern "C" fn rabc_client_new( client: *mut *mut RabcClient, log: *mut *mut c_char, err_kind: *mut *mut c_char, err_msg: *mut *mut c_char, ) -&gt; u32 { RABC_PASS } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might wonder about the use of &lt;code&gt;*mut *mut&lt;/code&gt;. These are raw pointers pointing to a raw pointer in Rust, like using &lt;code&gt;void **&lt;/code&gt; in C as an output pointer.&lt;/p&gt; &lt;p&gt;The function has two types of output pointer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;*mut *mut c_char&lt;/code&gt; is an output pointer to type String(&lt;code&gt;char *&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;code&gt;*mut *mut RabcClient&lt;/code&gt; is an output pointer to an opaque struct &lt;code&gt;RabcClient&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will look at each of these types.&lt;/p&gt; &lt;h2&gt;Output pointer for a string&lt;/h2&gt; &lt;p&gt;In the C world, &lt;code&gt;char **&lt;/code&gt; in a function argument returns a string (&lt;code&gt;char *&lt;/code&gt;) to the library consumer like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="c"&gt;if (result != NULL) { *result = malloc(strlen("ping") + 1); if (*result != NULL) { snprintf(*result, strlen("ping") + 1, "%s", "ping"); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following Rust code does the same using &lt;a href="https://doc.rust-lang.org/stable/alloc/ffi/struct.CString.html"&gt;&lt;code&gt;std::ffi::CString&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;if !result.is_null() { unsafe { *result = std::ptr::null_mut(); } if let Ok(s) = std::ffi::CString::new("ping") { unsafe { *result = s.into_raw(); } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Rust documentation states: "Failure to call CString::from_raw will lead to a memory leak. To prevent memory leaking, the memory of &lt;code&gt;CString::into_raw()&lt;/code&gt; should be freed (the C &lt;code&gt;free&lt;/code&gt; should &lt;strong&gt;not&lt;/strong&gt; be used) via &lt;code&gt;std::ffi::CString::from_raw&lt;/code&gt;":&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[no_mangle] pub extern "C" fn free_foo( result: *mut libc::c_char, ) { if !result.is_null() { unsafe { std::ffi::CString::from_raw(result) } } } &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Output pointer for an opaque struct&lt;/h2&gt; &lt;p&gt;In the C world, it is common to expose an opaque struct in a library to let the developer maintain a compatible ABI when adding properties to the struct.&lt;/p&gt; &lt;p&gt;In our example, we need to expose the Rust struct &lt;code&gt;RabcClient&lt;/code&gt;. The C compiler does not know the size of the opaque struct, so we can expose it only as a pointer whose size is known to the compiler.&lt;/p&gt; &lt;p&gt;Let's see how this workaround is done in Rust:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[no_mangle] pub extern "C" fn rabc_client_new( client: *mut *mut RabcClient, log: *mut *mut c_char, err_kind: *mut *mut c_char, err_msg: *mut *mut c_char, ) -&gt; u32 { // Many lines omitted if client.is_null() { return RABC_FAIL_NULL_POINTER; } unsafe { *client = std::ptr::null_mut(); } unsafe { *client = Box::into_raw(Box::new(c)); RABC_PASS } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;client: *mut *mut RabcClient&lt;/code&gt; clause is an output pointer to &lt;code&gt;RabcClient&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Because we should never dereference a null pointer, we put in the check &lt;code&gt;client.is_null()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The line &lt;code&gt;*client = std::ptr::null_mut();&lt;/code&gt; makes sure we always set the output pointer to NULL when an error happens.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;Box::into_raw()&lt;/code&gt; function gets the pointer to &lt;code&gt;RabcClient&lt;/code&gt; and removes its memory chunk from the Rust memory management system, trusting the library's user to manage the memory.&lt;/p&gt; &lt;p&gt;After finishing all work with &lt;code&gt;RabcClient&lt;/code&gt;, the user has to call &lt;code&gt;rabc_client_free()&lt;/code&gt;, which frees the memory leaked by &lt;code&gt;Box::into_raw()&lt;/code&gt;. The Rust library claims ownership of this memory chunk and drops it, thus freeing the memory:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[no_mangle] pub extern "C" fn rabc_client_free(client: *mut RabcClient) { if !client.is_null() { unsafe { drop(Box::from_raw(client)); } } } &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Logging in the C binding&lt;/h2&gt; &lt;p&gt;C lacks a standard logging system. So the Rust library in this example stores its JSON-formatted logs in &lt;code&gt;char *&lt;/code&gt; strings as an output pointer of a function. A Python or Go binding reads this string and converts it to their logging system.&lt;/p&gt; &lt;p&gt;The full logging code is in &lt;a href="https://github.com/cathay4t/librabc/blob/main/src/clib/logger.rs"&gt;the repository's logger.rs file&lt;/a&gt;. The code is based on the work of &lt;a href="https://github.com/gahag/memory_logger"&gt;Gabriel Bastos&lt;/a&gt;—many thanks to Gabriel.&lt;/p&gt; &lt;p&gt;Given the Rust &lt;code&gt;log&lt;/code&gt; crate infrastructure, you just need to implement the &lt;code&gt;log::Log&lt;/code&gt; trait by storing logs in a &lt;code&gt;Vec&lt;LogEntry&gt;&lt;/code&gt; in memory, then dumping them to &lt;code&gt;MemoryLogger.drain()&lt;/code&gt;. The most difficult parts are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;struct MemoryLogger&lt;/code&gt; instance should have a &lt;code&gt;static&lt;/code&gt; lifetime so that it could be invoked by &lt;code&gt;log::debug()&lt;/code&gt; and etc functions in any thread context.&lt;/li&gt; &lt;li&gt;Draining the logs must be done in a thread-safe manner.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will look at each of these issues.&lt;/p&gt; &lt;h3&gt;Static lifetime for the MemoryLogger instance&lt;/h3&gt; &lt;p&gt;To give &lt;code&gt;struct MemoryLogger&lt;/code&gt; a static lifetime, use &lt;a href="https://docs.rs/once_cell/latest/once_cell/"&gt;&lt;code&gt;OnceCell.set()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;static INSTANCE: OnceCell&lt;MemoryLogger&gt; = OnceCell::new(); // Many lines omitted fn init_logger() -&gt; Result&lt;&amp;'static MemoryLogger, RabcError&gt; { match INSTANCE.get() { Some(l) =&gt; { Ok(l) } None =&gt; { if INSTANCE.set(MemoryLogger::new()).is_err() { return Err(foo); } if let Some(l) = INSTANCE.get() { if let Err(e) = log::set_logger(l) { Err(foo) } else { Ok(l) } } else { Err(foo) } } } } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Thread-safe draining of logs&lt;/h3&gt; &lt;p&gt;The C library should be thread-safe, so all the data in &lt;code&gt;MemoryLogger&lt;/code&gt; should be an &lt;a href="https://doc.rust-lang.org/std/sync/atomic/"&gt;atomic type&lt;/a&gt; or be protected by a lock. In this example, we protect the data through a &lt;a href="https://doc.rust-lang.org/std/sync/struct.Mutex.html"&gt;&lt;code&gt;std::sync::Mutex&lt;/code&gt;&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;pub(crate) struct MemoryLogger { consumer_count: AtomicU16, logs: Mutex&lt;Vec&lt;LogEntry&gt;&gt;, } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;consumer_count&lt;/code&gt; variable tracks the thread count of log consumers. When the last C function using logs invokes &lt;code&gt;MemoryLogger::drain()&lt;/code&gt;, the library drops the logs from &lt;code&gt;Vec&lt;LogEntry&gt;&lt;/code&gt;. Otherwise, the &lt;code&gt;.drain&lt;/code&gt; call returns a copy of the logs that were logged since the specified time.&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;pub(crate) fn drain(&amp;self, since: SystemTime) -&gt; String { let mut logs = self.logs.lock().expect("inner lock poisoned"); let ret = serde_json::to_string( &amp;logs .as_slice() .iter() .filter(|l| l.time &gt;= since) .collect::&lt;Vec&lt;&amp;LogEntry&gt;&gt;(), ) .unwrap_or_default(); if self.consumer_count.fetch_sub(1, Ordering::SeqCst) == 1 { logs.clear(); } ret } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you try this C library in a multithreaded program, you will find that the logs retrieved are mixed with output from other threads. I don't have a good solution for a thread-local logger yet, but I found suggestions for one at &lt;a href="https://github.com/rust-lang/log/issues/81"&gt;rust-lang/log&lt;/a&gt;. A pull request to &lt;a href="https://github.com/cathay4t/librabc"&gt;this demo GitHub project&lt;/a&gt; will be much appreciated.&lt;/p&gt; &lt;h2&gt;Fixing the SONAME&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/rust-lang/cargo/issues/5045"&gt;Rust does not support the SONAME naming convention yet as I draft this article&lt;/a&gt;. As a workaround, you can define the following in &lt;code&gt;.cargo/config.toml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="toml"&gt;[build] rustflags = "-Clink-arg=-Wl,-soname=libfoo.so.0" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use &lt;code&gt;patchelf&lt;/code&gt; to modify the SONAME after the &lt;code&gt;cargo build&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;patchelf --set-soname libfoo.so.0 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Memory leak test for the C binding&lt;/h2&gt; &lt;p&gt;The C binding uses a lot of &lt;code&gt;unsafe&lt;/code&gt; keywords to work with Rust raw pointers. So I recommend running a memory leak test for the C binding. The example project offers a &lt;code&gt;make clib_check&lt;/code&gt; command that uses &lt;a href="https://valgrind.org"&gt;Valgrind&lt;/a&gt; against a C program linked to our C binding &lt;code&gt;.so&lt;/code&gt; file. The code is in a file named &lt;a href="https://github.com/cathay4t/librabc/blob/main/src/clib/tests/rabc_test.c"&gt;&lt;code&gt;rabc_test.c&lt;/code&gt;&lt;/a&gt; and the project's &lt;a href="https://github.com/cathay4t/librabc/blob/main/src/clib/Makefile"&gt;Makefile&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Packaging for the C binding&lt;/h2&gt; &lt;p&gt;To ship a C binding, you need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The following &lt;code&gt;.so&lt;/code&gt; files: &lt;ul&gt; &lt;li&gt;&lt;code&gt;librabc.so.0.1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;librabc.so.0&lt;/code&gt;, linked to &lt;code&gt;librabc.so.0.1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;librabc.so&lt;/code&gt;, linked to &lt;code&gt;librabc.so.0&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;The C header file: &lt;a href="https://github.com/cathay4t/librabc/blob/main/src/clib/rabc.h"&gt;&lt;code&gt;rabc.h&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;A Pkgconfig file: &lt;a href="https://github.com/cathay4t/librabc/blob/main/src/clib/rabc.pc.in"&gt;&lt;code&gt;rabc.pc.in&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can write a Makefile containing a &lt;code&gt;make install&lt;/code&gt; command to install all those files.&lt;/p&gt; &lt;p&gt;For Fedora RPM packaging, the &lt;code&gt;.cargo/config.toml&lt;/code&gt; fix shown earlier for SONAME will not work, because Fedora uses &lt;code&gt;.cargo/config&lt;/code&gt; to change the dependency source searching folder. Therefore, we will use the &lt;code&gt;patchelf&lt;/code&gt; method for Fedora:&lt;/p&gt; &lt;pre&gt;&lt;code class="rpm"&gt;%prep %setup -q rm .cargo/config.toml %cargo_prep %install env SKIP_PYTHON_INSTALL=1 \ PREFIX=%{_prefix} \ LIBDIR=%{_libdir} \ %make_install patchelf --set-soname librabc.so.2 \ %{buildroot}/%{_libdir}/librabc.so.%{version} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Red Hat Enterprse Linux has no &lt;code&gt;patchelf&lt;/code&gt; in the build root, so you need to merge the flags defined in &lt;code&gt;.cargo/config&lt;/code&gt;. The following snippet shows how I set the flags:&lt;/p&gt; &lt;pre&gt;&lt;code class="rpm"&gt;%prep # Source1 is vendored dependencies %cargo_prep -V 1 _FLAGS=`sed -ne 's/rustflags = "\(.\+\)"/\1/p' .cargo/config.toml` sed -i -e "s/rustflags = \[\(.\+\), \]$/rustflags = [\1, \"$_FLAGS\"]/" \ .cargo/config &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;What's next?&lt;/h2&gt; &lt;p&gt;Requirements might change from time to time, so please refer to the latest &lt;a href="https://docs.fedoraproject.org/en-US/packaging-guidelines/"&gt;Fedora&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/packaging_and_distributing_software/index"&gt;Red Hat Enterprise Linux&lt;/a&gt; packaging guides.&lt;/p&gt; &lt;p&gt;The C binding can also be used in other languages such as Python and Go. The next article in this series demonstrates how to create a &lt;a href="https://developers.redhat.com/articles/2022/08/11/how-create-python-binding-rust-library"&gt;Python binding&lt;/a&gt; on top of the C binding from this article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/05/how-create-c-binding-rust-library" title="How to create C binding for a Rust library"&gt;How to create C binding for a Rust library&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Gris Ge</dc:creator><dc:date>2022-09-05T07:00:00Z</dc:date></entry><entry><title type="html">The future of Keycloak Operator CRs</title><link rel="alternate" href="https://www.keycloak.org/2022/09/operator-crs" /><author><name>Václav Muzikář</name></author><id>https://www.keycloak.org/2022/09/operator-crs</id><updated>2022-09-02T00:00:00Z</updated><content type="html">A while back, we have a new Operator rewritten from scratch to provide the best experience for the Quarkus distribution. While the is now deprecated and , the is already available as a preview, see the . One of the most common concerns around the new Operator is the current lack of the CRDs for managing Keycloak resources, such as realm, users and clients, in a cloud-native way. One of the key aspects of the new Operator will be redesign of managing these Keycloak resources via CRs and git-ops. This new approach will leverage the and future immutability options, making the CRs the declarative single source of truth. In comparison to the legacy Operator, this will bring high robustness, reliability, and predictability to the whole solution. Before we would consider operator ready for leveraging CRs, we expect completing several features including but not limited to: * File store (expected in Keycloak 20) to persist data in a file instead of DB. * Read-only possibilities for administration REST API, UI Console and other interfaces. This is required for the new immutability concept which will be used to ensure any data coming from the CRs (and subsequently from the file store) are read-only from all interfaces. All of this is critical to proper CRs implementation, hence the new Operator is currently missing the CRDs for managing Keycloak resources. The missing CRDs will be added once Keycloak has the necessary support for it, which is currently expected in Keycloak 21. We have prepared a few options to alleviate the situation with missing CRDs in .</content><dc:creator>Václav Muzikář</dc:creator></entry><entry><title>3 essentials for writing a Linux system library in Rust</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/01/3-essentials-writing-linux-system-library-rust" /><author><name>Gris Ge</name></author><id>692bc784-5a5d-453e-9b61-ed3ba64a597a</id><updated>2022-09-01T07:00:00Z</updated><published>2022-09-01T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt; is a good choice for writing a &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; system library due to guarantees on memory and thread safety, a revolutionary approach to memory ownership, and a polished Foreign Function Interface. In spite of a slightly steep learning curve, you will come to love Rust and especially its compiler.&lt;/p&gt; &lt;p&gt;This article is the first in a series focusing on Rust for Linux. Check out the other three articles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2022/07/29/how-create-c-binding-rust-library"&gt;How to create C binding for a Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2022/08/11/how-create-python-binding-rust-library"&gt;How to create Python binding for a Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2022/08/21/build-trust-continuous-integration-your-rust-library"&gt;Build trust in continuous integration for your Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have created a &lt;a href="https://github.com/cathay4t/librabc"&gt;demo Git repository&lt;/a&gt; with simple code to help explain the points in this article. The repository contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An echo server listening on the Unix socket &lt;code&gt;/tmp/librabc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;A Rust crate that connects to the socket and sends a &lt;code&gt;ping&lt;/code&gt; packet every 2 seconds&lt;/li&gt; &lt;li&gt;A C/Python binding&lt;/li&gt; &lt;li&gt;A command-line interface (CLI) for the client&lt;/li&gt; &lt;li&gt;A CI system built on top of Github Action&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Rust community has many great documents and other resources that teach Rust to a variety of audiences. This article assumes you have already triumphed over the Rust learning curve and acquired basic Rust coding experience with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Memory and object ownership&lt;/li&gt; &lt;li&gt;Traits&lt;/li&gt; &lt;li&gt;Cargo and crates.io&lt;/li&gt; &lt;li&gt;Async/await&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you lack experience with any of these topics, &lt;a href="https://doc.rust-lang.org/book/title-page.html"&gt;The Rust Programming Language&lt;/a&gt; could be a good starting point. The &lt;a href="https://rust-lang.github.io/api-guidelines/about.html"&gt;Rust API Guidelines&lt;/a&gt; lists best practices for several aspects of Rust programming.&lt;/p&gt; &lt;h2&gt;From Python to Rust&lt;/h2&gt; &lt;p&gt;The content of this series is inspired by our work rewriting the &lt;a href="https://github.com/nmstate/nmstate/pulls"&gt;Nmstate&lt;/a&gt; networking library as a Rust library. Nmstate was originally in Python, and we rewrote it in the hope that it could be included in &lt;a href="https://getfedora.org/en/coreos?stream=stable"&gt;Fedora CoreOS&lt;/a&gt;, where &lt;a href="https://github.com/coreos/fedora-coreos-tracker/issues/32"&gt;Python is not allowed&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Since one major dependency of Nmstate is written in Rust, we decided to rewrite Nmstate in Rust. (Of course, we never thought about rewriting Nmstate in C/C++, considering the 100+ options of support in nmstate and complex yaml/json parsing workflow. It could be a headache doing that in C and C++.)&lt;/p&gt; &lt;p&gt;Because Nmstate already has an API user in &lt;a href="https://www.ovirhttps://www.ovirt.org.org"&gt;oVirt&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;, we needed to provide an identical Python API with this Rust rewrite. To minimize the Rust dependencies and save Rust compile time, we decided to just ship a C library in Rust and wrap that C library in a Python library. The Go binding of Nmstate is also a wrapper of this C library.&lt;/p&gt; &lt;h2&gt;3 Essential practices for writing a Linux system library in Rust&lt;/h2&gt; &lt;p&gt;This article focuses on these three Rust practices that benefit a Linux system library: backward compatibility, event-driven asynchronous programming, and logging.&lt;/p&gt; &lt;h2&gt;1. Backward compatibility&lt;/h2&gt; &lt;p&gt;In addition to the ironclad features that help keep Rust programs from breaking, are rules that allow programs to keep working after a library has added new elements to a struct (structure) or enum. The best practice, when you code your library, is to decorate your public structs and enums with the &lt;a href="https://doc.rust-lang.org/reference/attributes/type_system.html"&gt;&lt;code&gt;non_exhaustive&lt;/code&gt;&lt;/a&gt; attribute as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[derive(Clone, Copy, Debug, PartialEq, Eq)] #[non_exhaustive] pub enum RabcEvent { IpcIn = 1, Timer, } #[derive(Clone, Debug, PartialEq, Default)] #[non_exhaustive] pub struct RabcFoo { foo1: String, foo2: u32, } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;"Non-exhaustive" means that you might add a new element in the future. The attribute has no effect within your own crate. You can refer to your own &lt;code&gt;RabcEvent&lt;/code&gt; enum using code such as:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;match event { RabcEvent::IpcIn =&gt; {}, RabcEvent::Timer =&gt; {}, } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, if another crate that imports the library tries to issue the same code, the dependent crate gets a compiler error such as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;7 | pub enum RabcEvent { | ^^^^^^^^^^^^^^^^^^ = note: the matched value is of type `RabcEvent`, which is marked as non-exhaustive help: ensure that all possible cases are being handled by adding a match arm with a wildcard pattern or an explicit pattern as shown | 16 ~ RabcEvent::Timer =&gt; (), 17 ~ _ =&gt; todo!(), &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The reason for this message is that the code handled the two values that exist in the enum now, but might break in the future if the library adds another value.&lt;/p&gt; &lt;p&gt;In short, the &lt;code&gt;non_exhaustive&lt;/code&gt; attribute in a library requires the dependent crate to add a default handler. An example referring to the previous struct is:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;let foo = RabcFoo { foo1: "Abc".into(), foo2: 0, ..Default::default() }; let foo = RabcFoo::new(foo1, foo2); &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;2. Event-driven asynchronous programming&lt;/h2&gt; &lt;p&gt;Sockets are widely used in Linux for interprocess communication (IPC). There are two ways to use sockets in Rust:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://rust-lang.github.io/async-book/"&gt;rust async/await&lt;/a&gt;: Asynchronous and feature-rich&lt;/li&gt; &lt;li&gt;&lt;a href="https://man7.org/linux/man-pages/man7/epoll.7.html"&gt;epoll&lt;/a&gt;: Synchronous and lightweight&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;An example of asynchronous communication is the well-known &lt;a href="https://tokio.rs/"&gt;&lt;code&gt;tokio&lt;/code&gt;&lt;/a&gt; library, whereas synchronous communication is illustrated by the &lt;a href="https://github.com/tokio-rs/mio"&gt;&lt;code&gt;mio&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt; &lt;h3&gt;The async/await-based echo server&lt;/h3&gt; &lt;p&gt;The echo server cannot just fork a new thread for each client that connects because this places too much burden on CPU and memory resources and even opens up the risk of Denial-of-Service (DoS) attacks. Instead, the server creates a worker queue to limit resource use. &lt;code&gt;tokio&lt;/code&gt; builds on Rust's async/await feature to provide an asynchronous runtime with numerous control options. This makes &lt;code&gt;tokio&lt;/code&gt; the perfect fit for building an asynchronous echo server.&lt;/p&gt; &lt;p&gt;The code shown in this section comes from the full example in &lt;a href="https://github.com/cathay4t/librabc/blob/main/src/srv/rabcd.rs"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In addition to writing an async function to handle the client connection, add the &lt;code&gt;tokio&lt;/code&gt; macro to build the tokio runtime, as shown in the following example:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[tokio::main(flavor = "current_thread")] async fn main() { // Many lines omitted loop { match listener.accept().await { Ok((stream, _)) =&gt; { tokio::spawn(async move { process_client(stream).await; }); } Err(e) =&gt; { log::error!("Failed to accept connection {}", e); } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;tokio::main(flavor = "current_thread")&lt;/code&gt; macro sets up a single-threaded tokio runtime within the current thread. If you prefer multithreaded execution, you can use:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[tokio::main(flavor = "multi_thread", worker_threads = 10)] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For more control over the async runtime, you can use &lt;a href="https://docs.rs/tokio/latest/tokio/runtime/struct.Builder.html"&gt;&lt;code&gt;tokio::runtime::Builder&lt;/code&gt;&lt;/a&gt;. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;let runtime = tokio::runtime::Builder::new_multi_thread() .worker_threads(10) .build()?; runtime.block_on(your_async_function())?; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the help of the async runtime, you can easily wrap a complex async action. You could even wrap complex async actions into a synchronized API using async/await along with async runtime, but I would strongly recommend against it. Exposing a synchronous function using the tokio &lt;code&gt;block_on()&lt;/code&gt; method inside and the developers writing to your API use &lt;code&gt;tokio::runtime::Runtime::block_on&lt;/code&gt; over your wrapped API, triggers this error: &lt;a href="https://stackoverflow.com/questions/62536566/how-can-i-create-a-tokio-runtime-inside-another-tokio-runtime-without-getting-th"&gt;&lt;code&gt;Cannot start a runtime from within a runtime&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, we are fine using &lt;code&gt;tokio&lt;/code&gt; in the binary for the &lt;code&gt;rabcd&lt;/code&gt; server.&lt;/p&gt; &lt;h3&gt;The epoll-based echo client&lt;/h3&gt; &lt;p&gt;When talking about asynchronous communication, most old-school C programmers instantly come up with the idea of using &lt;code&gt;select&lt;/code&gt; or &lt;code&gt;epoll&lt;/code&gt;. They find these mechanisms cozier than the new &lt;code&gt;tokio&lt;/code&gt; aysnc runtime shown in the previous section.&lt;/p&gt; &lt;p&gt;Some reasons for using this classic &lt;code&gt;epoll&lt;/code&gt; approach include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We save the Rust crate or API consumer from dealing with threading, especially memory sharing and communication between threads.&lt;/li&gt; &lt;li&gt;It's easy to expose this synchronous Rust function to a C API.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can get the full code for this &lt;a href="https://github.com/cathay4t/librabc/blob/main/src/cli/rabcc.rs"&gt;sample application&lt;/a&gt; and &lt;a href="https://github.com/cathay4t/librabc/tree/main/src/lib"&gt;library&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;p&gt;To build the client library, you need to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a file descriptor for Unix socket communication.&lt;/li&gt; &lt;li&gt;Create a &lt;a href="https://man7.org/linux/man-pages/man2/timerfd_create.2.html"&gt;&lt;code&gt;TimerFD&lt;/code&gt;&lt;/a&gt; timer to generate an event every 10 seconds.&lt;/li&gt; &lt;li&gt;Create an &lt;code&gt;epoll&lt;/code&gt; instance to the poll the two previous file descriptors.&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;epoll_event&lt;/code&gt; to distinguish socket communication from the repeat timer. The &lt;code&gt;struct epoll_event&lt;/code&gt; is C structure in &lt;code&gt;epoll&lt;/code&gt;. It should be &lt;code&gt;RabcEvent&lt;/code&gt; in this Rust context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wrapping the two file descriptors into a single opaque struct with a &lt;code&gt;process()&lt;/code&gt; call simplifies the library without the need for multithreading or blocking. The library user can choose how to process the asynchronous events. The &lt;code&gt;RabcClient::process()&lt;/code&gt; will take &lt;code&gt;RabcEvent&lt;/code&gt; and choose the correct file descriptor to proceed with the action base on the specified event type.&lt;/p&gt; &lt;p&gt;If you used &lt;code&gt;mio&lt;/code&gt; to implement this strategy, you would have to implement &lt;code&gt;mio::event::Source&lt;/code&gt; for your &lt;code&gt;TimerFd&lt;/code&gt; file descriptor, which would be just as complex as just wrapping &lt;code&gt;epoll&lt;/code&gt; yourself. But if your project could benefit from the &lt;code&gt;mio&lt;/code&gt; library, please consider using it and contributing to the project.&lt;/p&gt; &lt;h4&gt;IPC between server and client&lt;/h4&gt; &lt;p&gt;In our example, the server accepts client connections on a Unix stream socket at &lt;code&gt;/tmp/librabc&lt;/code&gt;. The socket does not place boundaries around each transaction. To distinguish messages, I normally append the data size before the data transfer. For example, the &lt;code&gt;ipc_send()&lt;/code&gt; function of &lt;code&gt;librabc/src/lib/ipc.rs&lt;/code&gt; has:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;self.stream.write(&amp;data.len().to_ne_bytes())?; self.stream.write(data.as_bytes())?; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But there is a potential problem with this approach. A malicious client could set the size to &lt;code&gt;u32::MAX&lt;/code&gt; causing the server to run out of memory. Instead, place a limit to the size through the &lt;code&gt;RabcConnection.max_size&lt;/code&gt; variable:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;pub fn ipc_send(&amp;mut self, data: &amp;str) -&gt; Result&lt;(), RabcError&gt; { if data.len() &gt; self.max_size { return Err(RabcError::new( ErrorKind::ExceededIpcMaxSize, format!( "Specified data exceeded the max size {} bytes, \ please change the limitation by set_ipc_max_size()", self.max_size ), )); } &lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;The RabcClient struct&lt;/h4&gt; &lt;p&gt;A struct named &lt;code&gt;RabcClient&lt;/code&gt; is the interface used by a client to communicate with the server without considering the underlying IPC. The client has three public functions:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;pub fn new() -&gt; Result&lt;Self, RabcError&gt;; pub fn poll(&amp;mut self, wait_time: u32) -&gt; Result&lt;Vec&lt;RabcEvent&gt;, RabcError&gt;; pub fn process( &amp;mut self, event: &amp;RabcEvent, ) -&gt; Result&lt;Option&lt;String&gt;, RabcError&gt;; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The client uses &lt;code&gt;poll()&lt;/code&gt; to get a list of events and &lt;code&gt;process()&lt;/code&gt; to process each one.&lt;/p&gt; &lt;h2&gt;3. Logging&lt;/h2&gt; &lt;p&gt;A system library should never print any message to &lt;code&gt;stdout&lt;/code&gt; or &lt;code&gt;stderr&lt;/code&gt;. Hence, a good logging system is mandatory. Rust has divided the logging system into a front end and back end:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The logging facade: The set of APIs and traits abstracting the logging implementation.&lt;/li&gt; &lt;li&gt;The logging implementation: The executor that saves the logs using the chosen method (printing to &lt;code&gt;stderr&lt;/code&gt;, saving to &lt;a href="https://man7.org/linux/man-pages/man8/systemd-journald.service.8.html"&gt;journald&lt;/a&gt;, etc).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The library should depend only on the logging facade, allowing the library's consumer to define the logging actions. I use the &lt;code&gt;log&lt;/code&gt; crate in the library and the &lt;code&gt;env_logger&lt;/code&gt; crate in the CLI.&lt;/p&gt; &lt;p&gt;In general, the library simply uses the &lt;code&gt;log::warn!()&lt;/code&gt; macro for logging:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;log::warn!("This is a warning message"); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then the library consumer uses &lt;code&gt;env_logger&lt;/code&gt; to print the logs:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;let mut log_builder = env_logger::Builder::new(); log_builder.filter("foo", log::LevelFilter::Debug); log_builder.init(); &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Robust libraries use asynchronous communication and flexible logging&lt;/h2&gt; &lt;p&gt;This article offered specific guidelines for writing a Linux system library in Rust. The next articles will cover other aspects of integrating the system library into a larger system and the development cycle. Part 2 in this series describes the &lt;a href="https://developers.redhat.com/articles/2022/07/29/how-create-c-binding-rust-library"&gt;C binding&lt;/a&gt; and contains our custom-built, memory-based logging implementation.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/01/3-essentials-writing-linux-system-library-rust" title="3 essentials for writing a Linux system library in Rust"&gt;3 essentials for writing a Linux system library in Rust&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Gris Ge</dc:creator><dc:date>2022-09-01T07:00:00Z</dc:date></entry></feed>
