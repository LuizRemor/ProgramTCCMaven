<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Kafka Monthly Digest: August 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/13/kafka-monthly-digest-august-2022" /><author><name>Mickael Maison</name></author><id>195c9344-8250-48a9-9554-85515b7d00a9</id><updated>2022-09-13T07:00:00Z</updated><published>2022-09-13T07:00:00Z</published><summary type="html">&lt;p&gt;This 55th edition of the &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt; Monthly Digest covers what happened in the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; community in August 2022.&lt;/p&gt; &lt;p&gt;For last month’s digest, see &lt;a href="https://developers.redhat.com/articles/2022/08/04/kafka-monthly-digest-july-2022"&gt;Kafka Monthly Digest: July 2022&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Releases&lt;/h2&gt; &lt;p&gt;There is currently one release in progress, 3.3.0.&lt;/p&gt; &lt;h3&gt;3.3.0&lt;/h3&gt; &lt;p&gt;The release process for 3.3.0 continued. José Armando García Sancio published the first release candidate on August 29. A few issues, including &lt;a href="https://issues.apache.org/jira/browse/KAFKA-14187"&gt;KAFKA-14187&lt;/a&gt; and &lt;a href="https://issues.apache.org/jira/browse/KAFKA-14156"&gt;KAFKA-14156&lt;/a&gt;, were found during testing, so José built RC1 on September 1. The vote is currently ongoing. You can find the &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.3.0"&gt;release plan&lt;/a&gt; in the wiki.&lt;/p&gt; &lt;h2&gt;Kafka Improvement Proposals&lt;/h2&gt; &lt;p&gt;Last month, the community submitted three &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals"&gt;Kafka Improvement Proposals (KIPs)&lt;/a&gt; (KIP-863 to KIP-865). I'll highlight a couple of them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-864%3A+Add+End-To-End+Latency+Metrics+to+Connectors"&gt;KIP-864: Add End-To-End Latency Metrics to Connectors&lt;/a&gt;. This KIP proposes adding a few new metrics to track end-to-end latency for records flowing through Connect. This would also include metrics tracking the time spent in converters.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-865%3A+Support+--bootstrap-server+in+kafka-streams-application-reset"&gt;KIP-865: Support --bootstrap-server in kafka-streams-application-reset&lt;/a&gt;. This very small KIP aims at addressing a discrepancy with the &lt;code&gt;kafka-streams-application-reset.sh&lt;/code&gt; tool. This tool currently uses the &lt;code&gt;--bootstrap-servers&lt;/code&gt; flag, while all other tools use &lt;code&gt;--bootstrap-server&lt;/code&gt;, so it will be updated for consistency.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Community releases&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/tchiotludo/akhq/releases/tag/0.22.0"&gt;akhq 0.22&lt;/a&gt;: AKHQ is a GUI for Apache Kafka. This new version adds a few new features, including support for listing ACLs on Cluster and TransactionalIds and sending Protobuf records via the UI.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/tulios/kafkajs/releases/tag/v2.2.0"&gt;kafkajs 2.2.0&lt;/a&gt;: Kafkajs is a pure JavaScript Kafka client for Node.js. This release adds support for triggering and listing partition reassignments in its Admin API and contains a few fixes.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Blogs&lt;/h2&gt; &lt;p&gt;I selected some interesting blog articles that were published last month:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://towardsdatascience.com/machine-learning-streaming-with-kafka-debezium-and-bentoml-c5f3996afe8f"&gt;Machine Learning Streaming with Kafka, Debezium, and BentoML&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/event-driven-utopia/building-cqrs-views-with-debezium-kafka-materialize-and-apache-pinot-part-1-4f697735b2e4"&gt;Building CQRS Views with Debezium, Kafka, Materialize, and Apache Pinot — Part 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/event-driven-utopia/building-cqrs-views-with-debezium-kafka-materialize-and-apache-pinot-part-2-6899e9efc74e"&gt;Building CQRS Views with Debezium, Kafka, Materialize, and Apache Pinot — Part 2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To learn more about Kafka, visit &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Red Hat Developer's Apache Kafka topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/13/kafka-monthly-digest-august-2022" title="Kafka Monthly Digest: August 2022"&gt;Kafka Monthly Digest: August 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mickael Maison</dc:creator><dc:date>2022-09-13T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus adoption by APHP (Assistance Publique des Hôpitaux de Paris)</title><link rel="alternate" href="https://quarkus.io/blog/aphp-user-story/" /><author><name>Jean-Yves Terrien</name></author><id>https://quarkus.io/blog/aphp-user-story/</id><updated>2022-09-13T00:00:00Z</updated><content type="html">About APHP L’Assistance Publique - Hôpitaux de Paris, AP-HP is an internationally oriented university hospital center. Some numbers (2020) : 38 Hospitals 1 public healthcare service working 24/7 6,9 million patients 100 000 health professionals taking care of our patients - including nearly 12 200 doctors, 4 000 interns, over...</content><dc:creator>Jean-Yves Terrien</dc:creator></entry><entry><title type="html">How to spot Java bugs with SpotBugs</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-spot-java-bugs-with-spotbugs/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-spot-java-bugs-with-spotbugs/</id><updated>2022-09-12T07:47:00Z</updated><content type="html">This article will introduce you to SpotBugs utility project that can assist you to spot Java “bug patterns” in your code which are likely to turn into runtime bugs. Getting started with SpotBugs Firstly, let’s see how SpotBugs works. This tool uses defines a set of Bug patterns that will be scanned in your code ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">RESTEasy 6.2.0.Beta1 Release</title><link rel="alternate" href="https://resteasy.github.io/2022/09/08/resteasy-6.2.0.Beta1-release/" /><author><name /></author><id>https://resteasy.github.io/2022/09/08/resteasy-6.2.0.Beta1-release/</id><updated>2022-09-08T18:11:11Z</updated><dc:creator /></entry><entry><title type="html">This Week in JBoss - 08 September 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-09-08.html" /><category term="quarkus" /><category term="wildfly" /><category term="java" /><category term="wildfly" /><category term="kogito" /><category term="hibernate" /><author><name>Jason Porter</name><uri>https://www.jboss.org/people/jason-porter</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-09-08.html</id><updated>2022-09-08T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, java, wildfly, kogito, hibernate"&gt; &lt;h1&gt;This Week in JBoss - 08 September 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welcome back, glad to have you back with us this week! Progress continues to happen at JBoss, and Red Hat. We have some releases, blogs, and a couple of videos as well.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-12-0-final-released/"&gt;Quarkus 2.12.0&lt;/a&gt; - GraalVM/Mandrel 22.2, Kotlin 1.7, Smallrye Config SecretKeys, and SQL Server JDBC Driver update&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-12-1-final-released/"&gt;Quarkus 2.12.1&lt;/a&gt; - A performance regression fix&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2022/09/08/hibernate-orm-613-final/"&gt;Hibernate ORM 6.1.3&lt;/a&gt; - Maintenance release&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2022/08/30/hibernate-orm-5611-final/"&gt;Hibernate ORM 5.6.11&lt;/a&gt; - Maintenance release (includes performance fixes)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/09/kogito-1-27-0-released.html"&gt;Kogito 1.27.0&lt;/a&gt; - New feature release&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2022/08/31/WildFly2612-Released/"&gt;Wildfly 26.1.2&lt;/a&gt; - Maintenance release&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_blogs"&gt;Blogs&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/09/monitoring-quarkus-applications-with-dashbuilder.html"&gt;Monitoring Quarkus Applications With Dashbuilder&lt;/a&gt; - Building dashboards using DashBuilder&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/redis-api-intro/"&gt;Introducing the new Redis API - How to cache with Redis?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/09/operator-crs"&gt;The future of Keycloak Operator CRs&lt;/a&gt; - Looking at the new way of managing Keycloak resources&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/bearer-only-support-openid-connect/"&gt;Bearer Token Support for the Elytron OIDC Client Subsystem&lt;/a&gt; - Learn how to update your application to support Bearer Tokens using OIDC (OpenID Connect)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/top-five-reasons-to-join-elytron-open-source-day/"&gt;Top 5 Reasons To Join Us At Open Source Day&lt;/a&gt; - September 16 is Open Source Day! Wildfly Electron was selected as a project for this day long hackathon&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/cool-stuff/mongodb/building-java-enteprise-applications-using-mongodb/"&gt;Building Java Enterprise applications using MongoDB&lt;/a&gt; - Dig into MongoDB in a Jakarta EE Application&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/08/25/optimize-loops-long-variables-java"&gt;Optimize loops with long variables in Java&lt;/a&gt; - A look into JVM loop optimizations&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_videos"&gt;Videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=NJglcdL9m7A"&gt;Quarkus Insights #100: EDDI chatbot goes cloud-native with Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_until_next_time"&gt;Until next time!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Everyone stay safe out there, and we look forward to seeing you in two weeks for our next edition!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/jason-porter.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Jason Porter&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Jason Porter</dc:creator></entry><entry><title type="html">Kogito 1.27.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/09/kogito-1-27-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/09/kogito-1-27-0-released.html</id><updated>2022-09-07T12:42:31Z</updated><content type="html">We are glad to announce that the Kogito 1.27.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Adding patch http method for process instance. This allows partial updates of the process model.  * Variable change events are more specific, only the changed value of the Workflow model is published rather than the whole object * Kogito now supports injection of multiple WorkItemHandlerConfig instances * Fix ArrayNode merging to replace the whole array. This prevents duplication of values.  * Clone procedure performance has been improved.    * Exceptions thrown by an action can be stored into a process variable for BPMN.  KNOWN ISSUE(S) * Quarkus SVG addon fails to compile in native mode, see * WorkItemNotFoundException when running OpenAPI generated client in dev mode, see * ClassNotFoundException in enum persistence For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.22.0 artifacts are available at the . A detailed changelog for 1.27.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>Red Hat OpenShift Connectors: Configuring change data capture</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/07/configuring-change-data-capture" /><author><name>Bernard Tison</name></author><id>41f98b40-5aa6-451e-9c6d-2655264e4f6a</id><updated>2022-09-07T07:00:00Z</updated><published>2022-09-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors&lt;/a&gt; is a new cloud service offering from Red Hat. The service provides prebuilt connectors to enable quick and reliable connectivity across data, services, and systems. Each connector is a fully managed service, tightly integrated with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;, Red Hat's managed cloud service for Apache Kafka.&lt;/p&gt; &lt;p&gt;Change data capture (CDC) is a software process that detects changes (inserts, updates, deletes) to data in a database and transforms these changes into event streams that can be consumed by other systems or applications to react to these changes.&lt;/p&gt; &lt;p&gt;Typical use cases for change data capture include: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data replication&lt;/li&gt; &lt;li&gt;Streaming analytics&lt;/li&gt; &lt;li&gt;Event-driven distributed applications&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Red Hat OpenShift Connectors offers several source connectors for change data capture, based on the popular &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt; open source project. OpenShift Connectors support the following databases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PostgreSQL&lt;/li&gt; &lt;li&gt;MySQL&lt;/li&gt; &lt;li&gt;SQL Server&lt;/li&gt; &lt;li&gt;MongoDB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article demonstrates how to configure a source connector to capture data change events from &lt;a href="https://www.mongodb.com/cloud"&gt;MongoDB Atlas&lt;/a&gt;, a fully managed cloud database provided by MongoDB.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;This article assumes that you have created an instance of OpenShift Streams for Apache Kafka and that the instance is in the &lt;code&gt;Ready&lt;/code&gt; state. Please refer to &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; for step-by-step instructions on creating your Kafka instance.&lt;/p&gt; &lt;p&gt;Create a service account and configure the access rules for it. The service account requires privileges to read and write topics and to create new topics. The &lt;a href="https://developers.redhat.com/articles/2022/06/09/get-started-red-hat-openshift-connectors"&gt;Get started with Red Hat OpenShift Connectors&lt;/a&gt; article has detailed instructions on creating and configuring the service account and the access rules.&lt;/p&gt; &lt;h2&gt;Set up a MongoDB Atlas instance&lt;/h2&gt; &lt;p&gt;The following instructions will guide you through the process of setting up a MongoDB Atlas instance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.mongodb.com/cloud/atlas/register"&gt;Sign up&lt;/a&gt; to provision a MongoDB Atlas instance. You can create an Atlas account or sign up with your Google account.&lt;/li&gt; &lt;li&gt;After registration, you are taken to a page where you can choose the type of cloud database you want to provision. MongoDB Atlas offers different tiers, including a free tier for an easy getting-started experience. The free tier is sufficient to follow this demonstration.&lt;/li&gt; &lt;li&gt;When creating your MongoDB Atlas instance, you have to specify the security settings. Choose a username&lt;strong&gt; &lt;/strong&gt;and password and create a user to connect to your MongoDB instance.&lt;/li&gt; &lt;li&gt;Add an IP address to the &lt;strong&gt;IP Access List&lt;/strong&gt;. Enter &lt;code&gt;0.0.0.0/0 &lt;/code&gt; because you don't know the IP address where the managed OpenShift Connector is running. This effectively allows connections to your MongoDB instance from anywhere. You can make the IP Access List more restrictive later on.&lt;/li&gt; &lt;li&gt;Add databases and collections once the MongoDB Atlas instance is up and running.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MongoDB Atlas provides a sample dataset described &lt;a href="https://www.mongodb.com/docs/atlas/sample-data/"&gt;in the documentation&lt;/a&gt;. We will use the &lt;code&gt;sample_mflix&lt;/code&gt; database from the sample dataset. This dataset contains data about movies.&lt;/p&gt; &lt;h2&gt;10 steps to create an instance for change data capture&lt;/h2&gt; &lt;p&gt;Now that you have provisioned and configured a MongoDB Atlas instance and loaded the sample dataset, you can create an OpenShift Connectors source connector to capture change events from the &lt;code&gt;sample_mflix&lt;/code&gt; database. From your &lt;a href="https://console.redhat.com"&gt;Red Hat console&lt;/a&gt;, complete the following 10 steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Application and Data Services&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the &lt;strong&gt;Application and Data Services&lt;/strong&gt; page, select &lt;strong&gt;Connectors&lt;/strong&gt; and click &lt;strong&gt;Create Connectors instance&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;To find the &lt;strong&gt;Debezium MongoDB&lt;/strong&gt; connector, enter &lt;code&gt;mongo&lt;/code&gt; in the search field. Click the &lt;strong&gt;Debezium MongoDB Connector&lt;/strong&gt; card, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the &lt;strong&gt;Streams for Apache Kafka&lt;/strong&gt; instance for the connector. (This is the instance you created in the prerequisites step.) Then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the &lt;strong&gt;Namespace&lt;/strong&gt; page, click &lt;strong&gt;Create preview namespace&lt;/strong&gt; to provision a namespace for hosting the connector instances that you create (or select your existing namespace if you created one earlier). This evaluation namespace will remain available for 48 hours. You can create up to four connector instances per namespace. Once the namespace is available, select it and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide the core configuration for your connector by entering the following values:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;A unique name for the connector.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; of the service account that you created for your connectors.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide the connection configuration for your connector. For the &lt;strong&gt;Debezium MongoDB&lt;/strong&gt; connector, enter the following information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hosts&lt;/strong&gt;: The set of the replicaset public addresses for your MongoDB instance. You can find these on the web console for your MongoDB instance. Click the &lt;strong&gt;Overview&lt;/strong&gt; tab shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mongo-replicaset.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mongo-replicaset.png?itok=CpbJzBQ2" width="600" height="262" alt="A screenshot of the public addresses for the MongoDB replicaset. " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Overview tab on the web console of the MongoDB Atlas instance shows the public addresses of the MongoDB replicaset. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click the first link to get to the status page of the replica. The address of the replica is shown at the top of the page in &lt;code&gt;host:port&lt;/code&gt; format. Copy the address to a text editor.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Repeat the procedure for the other replicas.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Go back to the OpenShift Connectors configuration page and enter the addresses of the three replicas separated by commas in the &lt;strong&gt;Hosts&lt;/strong&gt; field. The entry should look like the following list (your values will be different):&lt;/p&gt; &lt;p&gt;&lt;code class="java"&gt;ac-whrxxxx-shard-00-00.ooulprt.mongodb.net:27017, ac-whrxxxx-shard-00-01.ooulprt.mongodb.net:27017, ac-whrxxxx-shard-00-02.ooulprt.mongodb.net:27017&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Namespace&lt;/strong&gt;: A unique name that identifies this MongoDB instance. For example, enter &lt;code&gt;mongo-mflix&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Password&lt;/strong&gt;: The password of the database user you created previously. Note that this is not the same user with which you logged into MongoDB Atlas.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;: The user name of the database user you created.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Make sure the &lt;strong&gt;Enable SSL connection to MongoDB&lt;/strong&gt; is checked.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 2 shows what a filled-out form looks like.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_12.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image1_12.png?itok=Ebn64xZ_" width="600" height="348" alt="A screenshot of configuration properties for MongoDB Debezium connector." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Enter configuration properties for the MongoDB Debezium connector. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the next page of the wizard, set the &lt;strong&gt;Database filter&lt;/strong&gt; to &lt;code&gt;sample_mflix&lt;/code&gt; and the &lt;strong&gt;Collection filter&lt;/strong&gt; to &lt;code&gt;sample_mflix.movies. &lt;/code&gt; This will ensure you capture change events only from the &lt;code&gt;movies&lt;/code&gt; collection.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Make sure that the &lt;strong&gt;Include&lt;/strong&gt; box is selected for both entries. Click &lt;strong&gt;Apply&lt;/strong&gt; to apply the filter. [Do not change the values on the &lt;strong&gt;Data &amp; runtime&lt;/strong&gt; page of the wizard. These are advanced values that you rarely need to change.]&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Review the summary of the configuration properties. Pay particular attention to the MongoDB &lt;strong&gt;Hosts&lt;/strong&gt; field. Click &lt;strong&gt;Create Connector&lt;/strong&gt; to create the connector.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your connector instance will be added to the table of connectors. After a couple of seconds, the status of your connector instance should change to &lt;code&gt;Ready&lt;/code&gt;. If your connector ends up in an &lt;code&gt;Error&lt;/code&gt; state, you can click the options icon (the three vertical dots) next to the connector. Then edit the configuration and restart the connector.&lt;/p&gt; &lt;h2&gt;Capture data change events from MongoDB&lt;/h2&gt; &lt;p&gt;Once the Debezium MongoDB connector is ready, it connects to the MongoDB database, creates a snapshot of the collections it monitors, and creates data change events for every record present in the collection.&lt;/p&gt; &lt;p&gt;To verify, use the message viewer in the OpenShift Streams for Apache Kafka web console.&lt;/p&gt; &lt;p&gt;Head over to the &lt;strong&gt;Application and Data Services&lt;/strong&gt; page of the Red Hat console and select &lt;strong&gt;Streams for Apache Kafka→Kafka Instances&lt;/strong&gt;. Click the name of the Streams for the Apache Kafka instance that you created for connectors. Select the &lt;strong&gt;Topics&lt;/strong&gt; tab.&lt;/p&gt; &lt;p&gt;You should see four new topics. Debezium connectors run on top of Kafka Connect (in contrast to the other OpenShift Connectors instances, which are based on Camel-K). Kafka Connect creates three topics to maintain its internal state. These are the topics ending with &lt;code&gt;-config&lt;/code&gt;, &lt;code&gt;-offset&lt;/code&gt;, and &lt;code&gt;-status&lt;/code&gt; shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/connectors-dbz-topics.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/connectors-dbz-topics.png?itok=Fq3nwp4K" width="1440" height="534" alt="The Kafka Connect connector creates four topics." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The Kafka Connect connector creates four topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The other topic, named &lt;code&gt;mongo-mflix.sample_mflix.movies&lt;/code&gt;, holds the data change events from the movies collection. Click the topic name and select the &lt;strong&gt;Messages&lt;/strong&gt; tab. You should see the most recent ten messages in the topic, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/data-change-event-messages.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/data-change-event-messages.png?itok=EXj1cQO8" width="1440" height="793" alt="A topic's Messages tab displays recent messages in the topic." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: A topic's Messages tab displays recent messages in the topic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The offset of the last message in Figure 4 is 23529, which indicates that there are 23530 events in the topic. This corresponds to the number of records in the &lt;code&gt;movies&lt;/code&gt; collection.&lt;/p&gt; &lt;p&gt;Each data change message has a JSON payload with the following structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;after&lt;/strong&gt;: Contains the latest state of the document.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;source&lt;/strong&gt;: Contains metadata about the connector and the MongoDB instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;op&lt;/strong&gt;: Specifies the operation that created this change. In this case, the operation is &lt;code&gt;r&lt;/code&gt;, which stands for &lt;code&gt;read&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please refer to the &lt;a href="https://debezium.io/documentation/reference/1.9/"&gt;Debezium documentation&lt;/a&gt; for more information on Debezium and the structure of the data change events it produces.&lt;/p&gt; &lt;p&gt;At this point, you can add new records to the MongoDB collection or modify existing records. The easiest way to do so is through the MongoDB Atlas web console:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On the &lt;strong&gt;Database&lt;/strong&gt; page, select the &lt;strong&gt;Collections&lt;/strong&gt; tab&lt;/li&gt; &lt;li&gt;Then select the &lt;code&gt;sample_mflix.movies&lt;/code&gt; collection.&lt;/li&gt; &lt;li&gt;From here you can add new records to the collection or modify existing ones. Every time you make a change, a data change event is produced with the latest state of the document and an operation equal to &lt;code&gt;c&lt;/code&gt; for creates and &lt;code&gt;u&lt;/code&gt; for updates.&lt;/li&gt; &lt;li&gt;You can verify that change events are generated by checking the message viewer of the data change event topic in the Red Hat console.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;An end-to-end data pipeline example&lt;/h2&gt; &lt;p&gt;Capturing data change events from a database with an OpenShift Connector instance is just the first step in using the data. Typically, the data change events are consumed by other services or applications to, for instance, replicate the data or build a local view of the data.&lt;/p&gt; &lt;p&gt;The following video contains a demo of what an end-to-end data pipeline could look like. The demo uses OpenShift Connectors to stream the data change events to &lt;a href="https://aws.amazon.com/kinesis/"&gt;AWS Kinesis&lt;/a&gt;. The events trigger an AWS Lambda function that extracts the state of the document from the event and updates an AWS OpenSearch index. Figure 5 shows the pipeline.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/connectors-data-pipeline_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/connectors-data-pipeline_0.png?itok=CD0w0bc5" width="958" height="544" alt="A flow chart illustrating OpenShift Connectors feeds change events." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Red Hat OpenShift Connectors feeds change events from the database to consumers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h3&gt;Watch this quick CDC pipeline demo:&lt;/h3&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;OpenShift Connectors speed up data collection applications&lt;/h2&gt; &lt;p&gt;We hope you found this demonstration informative and easy to follow. Try &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors&lt;/a&gt; for yourself and see how they speed up data collection applications. Feel free to reach out to us. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/07/configuring-change-data-capture" title="Red Hat OpenShift Connectors: Configuring change data capture"&gt;Red Hat OpenShift Connectors: Configuring change data capture&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bernard Tison</dc:creator><dc:date>2022-09-07T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.12.1.Final released - Fixes a performance regression</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-12-1-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-12-1-final-released/</id><updated>2022-09-07T00:00:00Z</updated><content type="html">2.12.1.Final is the first maintenance release of the 2.12 release train. If you have already upgraded to 2.12, we highly recommend this upgrade as it fixes, amongst other things, a performance regression introduced in Quarkus 2.12.0.Final. It is a safe upgrade for anyone using 2.12. Migration Guide If you are...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">Monitoring Quarkus applications with Dashbuilder</title><link rel="alternate" href="https://blog.kie.org/2022/09/monitoring-quarkus-applications-with-dashbuilder.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/09/monitoring-quarkus-applications-with-dashbuilder.html</id><updated>2022-09-06T18:18:42Z</updated><content type="html">Dalle-2 generated Chart image Most of extensions allow users to expose information in JSON format or metrics to be used by Prometheus. What I miss is a tool that allows us to visualize data from Quarkus or even embed dashboards in my Quarkus application. In this article we will share a tool called Dashbuilder to help us to build dashboards that run on the client-side, without the requirement of installing anything and be used to monitor Quarkus. DASHBOARDS WITH DASHBUILDER Dashbuilder is a Web Application that runs Dashboards in format. It can consume data from JSON arrays, CSV and metrics (the same metrics scrapped by Prometheus). You can transform JSON to arrays using JSONAta expressions. The data can be displayed in pages composed of Displayers. Displayers can be anything that shows the data, such as a Bar Chart or a Heatmap. To create dashboards you can use the great . Once the YAML is done then you can deploy the dashboard using the Dashbuilder web app from NPM package with the instructions from . Now you may be asking yourself how Dashbuilder can help us with Quarkus applications. Any application that has metrics exposed can be monitored by Dashbuilder without the requirement of using Prometheus or Grafana. DASHBOARDS FOR QUARKUS MONITORING To monitor quarkus with Dashbuilder only two configurations are required on the Quarkus side: * Add the extension quarkus-micrometer-registry-prometheus * If the dashboard is not embedded in Quarkus, then you need to enable cors using  quarkus.http.cors=true in application.properties Now you can use the Dashbuilder to build dashboards: * Start your quarkus app in dev mode:  * mvn clean compile quarkus:dev * Check if the metrics are available at http://localhost:8080/q/metrics * Go to the Dashbuilder , create a new Dashboard and use the following YAML content: datasets: - uuid: metrics url: http://localhost:8080/q/metrics pages: - components: - settings: lookup: uuid: metrics Now you should be able to see a table with all metrics: From here you can explore the to build more complex dashboards. In our online editor you can start This dashboard reads the metrics for real time information. Users can change a property on top of the YAML file to make it auto-update (data polling). Notice that this dashboard does not require the use of Prometheus, but we don’t keep the track of the metrics, however, it is possible to use Dashbuilder with Prometheus as described in . EMBEDDING DASHBOARDS IN A QUARKUS APPLICATION Dashboards can be embedded in a Quarkus application by simply unzipping Dashbuilder content into the static content folder (resources/META-INF/resources). To make it easy to use Dashbuilder we created a for the NPM package.  You can check a sample application in or follow the steps below to embed dashbuilder in your application: * Create the files setup.js and your YAML dashboard in the directory that will contain dashbuilder content. In our case the directory is dashbuilder: src/main/resources/META-INF/resources/dashbuilder/setup.js dashbuilder = { dashboards: ["hello.dash.yaml"] } src/main/resources/META-INF/resources/dashbuilder/hello.dash.yaml datasets: - uuid: metrics url: /q/metrics pages: - components: - settings: lookup: uuid: metrics * Add the Dashbuilder web jar version to the properties section of your pom.xml &lt;dashbuilder.version&gt;0.22.0&lt;/dashbuilder.version&gt; * Add the Web Jar as a dependency in the dependencies section: &lt;dependency&gt; &lt;groupId&gt;org.webjars.npm&lt;/groupId&gt; &lt;artifactId&gt;kie-tools__dashbuilder-client&lt;/artifactId&gt; &lt;version&gt;${dashbuilder.version}&lt;/version&gt; &lt;/dependency&gt; * Add a plugin to unpack Dashbuilder in the plugins section:       &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;unpack-dashbuilder&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;unpack&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactItems&gt; &lt;artifactItem&gt; &lt;groupId&gt;org.webjars.npm&lt;/groupId&gt; &lt;artifactId&gt;kie-tools__dashbuilder-client&lt;/artifactId&gt; &lt;version&gt;${dashbuilder.version}&lt;/version&gt; &lt;overWrite&gt;true&lt;/overWrite&gt; &lt;outputDirectory&gt;${project.build.directory}/dashbuilder&lt;/outputDirectory&gt; &lt;/artifactItem&gt; &lt;/artifactItems&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;overWriteSnapshots&gt;true&lt;/overWriteSnapshots&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; * Finally copy the contents from the dashbuilder package to the destination folder &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dashbuilder-resources&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;${project.build.outputDirectory}/META-INF/resources/dashbuilder&lt;/outputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;${project.build.directory}/dashbuilder/META-INF/resources/webjars/kie-tools__dashbuilder-client/${dashbuilder.version}/dist/&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; That’s all! When you start your app then the dashboard will be available in localhost:8080/dashbuilder CONCLUSION In this post we shared Dashbuilder, an alternative to monitor Quarkus applications! Check the sample project at . The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Build trust in continuous integration for your Rust library</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/06/build-trust-continuous-integration-your-rust-library" /><author><name>Gris Ge</name></author><id>af4e4d46-5834-4409-9225-eb8bfeaf27b1</id><updated>2022-09-06T07:00:00Z</updated><published>2022-09-06T07:00:00Z</published><summary type="html">&lt;p&gt;This article concludes the 4-part series about how to take advantage of the recent Rust support added to Linux. I hope you have read the previous articles in the series:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2022/07/15/3-essentials-writing-linux-system-library-rust"&gt;3 essentials for writing a Linux system library in Rust&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2022/07/29/how-create-c-binding-rust-library"&gt;How to create C binding for a Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2022/08/11/how-create-python-binding-rust-library"&gt;How to create Python binding for a Rust library&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article will demonstrate how to build trust in a Continuous Integration (CI) system for your Rust library.&lt;/p&gt; &lt;p&gt;You can download the demo code from its &lt;a href="https://github.com/cathay4t/librabc"&gt;GitHub repository&lt;/a&gt;. The package contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An echo server listening on the Unix socket &lt;code&gt;/tmp/librabc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;A Rust crate that connects to the socket and sends a &lt;code&gt;ping&lt;/code&gt; packet every 2 seconds&lt;/li&gt; &lt;li&gt;A C/Python binding&lt;/li&gt; &lt;li&gt;A command-line interface (CLI) for the client&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The CI system of this &lt;a href="https://github.com/cathay4t/librabc"&gt;GitHub repository&lt;/a&gt; is built on Github Action with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rust lint check using &lt;code&gt;cargo fmt&lt;/code&gt; and &lt;code&gt;cargo clippy&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Python lint check using &lt;code&gt;pylint&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Rust unit test.&lt;/li&gt; &lt;li&gt;C memory leak test.&lt;/li&gt; &lt;li&gt;Integration test on CentOS stream 8 and CentOS stream 9.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We find that &lt;code&gt;pytest&lt;/code&gt; framework provides more control over how the test case runs than Rust native &lt;code&gt;cargo test&lt;/code&gt;. Hence, we will use &lt;code&gt;pytest&lt;/code&gt; as an integration test framework.&lt;/p&gt; &lt;h2&gt;How to build trust in continuous integration in 4 steps:&lt;/h2&gt; &lt;p&gt;Open source projects receive contributions around the world from contributors with different code skill sets and habits. Therefore, we strongly recommend maintaining trust in the CI system for every critical systems project. The goal of a CI system is to build trust by ensuring that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The pending patch will not introduce any regression.&lt;/li&gt; &lt;li&gt;A test case attached to the pending patch proves what it fixed.&lt;/li&gt; &lt;li&gt;New features are tested with commonly used test cases.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Step 1. Isolation test environment and test setup&lt;/h3&gt; &lt;p&gt;Effective isolation is the first thing to consider when you design the CI system. When the CI system utilizes a large bash script to set up a complex environment for your test case to run, porting this CI to a new CI platform or debugging a certain test case would be difficult. This arduous task would result in the following issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mixing CI platform-related code with test setup code complicates the efforts of developers debugging specific test cases in their local environment.&lt;/li&gt; &lt;li&gt;New contributors would have to complete a lengthy document for their first contribution with a test case attached, making the project less friendly to the open source community.&lt;/li&gt; &lt;li&gt;A large bash script could become bloated with enormous race problems.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our &lt;a href="https://github.com/cathay4t/librabc"&gt;demo project&lt;/a&gt; comprises three isolated layers for CI setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Layer 1: The &lt;a href="https://github.com/cathay4t/librabc/blob/main/.github/workflows/main.yaml"&gt;&lt;code&gt;.github/workflows/main.yaml&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/cathay4t/librabc/blob/main/.github/runtest.sh"&gt;&lt;code&gt;.github/runtest.sh&lt;/code&gt;&lt;/a&gt; contain the CI platform (Github Action) specific setup codes that: test artifacts, run test case run on the matrix of toolsets combinations, invoke the test in different containers, and install the package of the current project.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The first layer is CI platform specific. Thus, you should refer to their documentation for detail.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Layer 2: The &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/runtest.sh"&gt;&lt;code&gt;tests/runtest.sh&lt;/code&gt;&lt;/a&gt; contains the test environment setup codes that include a helper for running the test in developer mode, and a specific argument passing to &lt;code&gt;pytest&lt;/code&gt;. It has &lt;strong&gt;zero&lt;/strong&gt; lines of code for the environment setup of a specific test case.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The second layer is test framework specific, the developer should find out the suitable test framework.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Layer 3: The &lt;code&gt;rabc_daemon()&lt;/code&gt; pytest fixture in &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/integration/rabc_test.py"&gt;&lt;code&gt;tests/integration/rabc_test.py&lt;/code&gt;&lt;/a&gt; contains the environment setup for a certain test case.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's elaborate on the third layer's pytest fixture which is designed to setup the environment and clean up after the test case finishes (fail or pass).&lt;/p&gt; &lt;p&gt;To use pytest fixture to setup the test environment, we have the following lines in &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/integration/rabc_test.py"&gt;&lt;code&gt;tests/integration/rabc_test.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="python"&gt;@pytest.fixture(scope="session", autouse=True) def rabc_daemon(): daemon = subprocess.Popen( "rabcd", stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid, ) yield os.killpg(daemon.pid, signal.SIGTERM) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the &lt;code&gt;@pytest.fixture(scope="session", autouse=True)&lt;/code&gt; decorator, the whole test session will have &lt;code&gt;rabcd&lt;/code&gt; daemon started beforehand and stopped afterward. The&lt;code&gt;pytest&lt;/code&gt; will handle the failure of daemon start/stop properly.&lt;/p&gt; &lt;p&gt;The pytest also has scopes for &lt;code&gt;module&lt;/code&gt;, &lt;code&gt;class&lt;/code&gt;, and &lt;code&gt;function&lt;/code&gt; for test environment setup/cleanup.&lt;/p&gt; &lt;p&gt;By chaining the pytest fixtures, we could split test fixtures into small parts and reuse them between test cases. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class="python"&gt;@pytest.fixture def setup_a(): start_a() yield stop_a() @pytest.fixture def setup_b(): start_b() yield stop_b() @pytest.fixture def setup_ab(setup_a, setup_b): yield @pytest.fixture def setup_ba(setup_b, setup_a): yield &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the above example code, the fixtures &lt;code&gt;setup_ab&lt;/code&gt; and &lt;code&gt;setup_ba&lt;/code&gt; are holding different orders of setup and cleanup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;setup_ab&lt;/code&gt; will run &lt;code&gt;setup_a()&lt;/code&gt; and then &lt;code&gt;setup_b()&lt;/code&gt; before test starts.&lt;/li&gt; &lt;li&gt;&lt;code&gt;setup_ab&lt;/code&gt; will run &lt;code&gt;stop_b()&lt;/code&gt; and then &lt;code&gt;stop_a()&lt;/code&gt; after test ends.&lt;/li&gt; &lt;li&gt;&lt;code&gt;setup_ba&lt;/code&gt; will run &lt;code&gt;setup_b()&lt;/code&gt; and then &lt;code&gt;setup_a()&lt;/code&gt; before test starts.&lt;/li&gt; &lt;li&gt;&lt;code&gt;setup_ba&lt;/code&gt; will run &lt;code&gt;stop_a()&lt;/code&gt; and then &lt;code&gt;stop_b()&lt;/code&gt; after test ends.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Step 2. Minimize the use of the internet during the test&lt;/h3&gt; &lt;p&gt;A single glitch of internet access in the CI platform might fail our test and will waste our efforts on debugging that failure. So we take the following actions to minimize the use of the internet during tests:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use a prebuild container with all required packages.&lt;/li&gt; &lt;li&gt;Use CI platform-specific way for test host setup.&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;rpm&lt;/code&gt;/&lt;code&gt;dpdk&lt;/code&gt; instead of &lt;code&gt;dnf&lt;/code&gt;/&lt;code&gt;apt-get&lt;/code&gt; to avoid repository problems.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the demo project, we use &lt;a href="https://github.com/cathay4t/librabc/blob/main/tests/container/Dockerfile.c9s-rabc-ci"&gt;&lt;code&gt;tests/container/Dockerfile.c9s-rabc-ci&lt;/code&gt;&lt;/a&gt; to define all the required packages necessary for building the project and running the test. The &lt;a href="https://quay.io/repository/librabc/c9s-rabc-ci"&gt;quay.io&lt;/a&gt; will automatically rebuild the container on every merged commit. Some CI platforms can even cache the container to speed up the test run.&lt;/p&gt; &lt;p&gt;Using containers could also eliminate the failure caused by the upgrade of the test framework (e.g. pytest, tox, etc).&lt;/p&gt; &lt;p&gt;Instead of using your own script to test the project on multiple branches of Rust or Python versions, you can trust the CI platform-specific way which is fast, well tested, and well maintained. For example, we could install Rust in Github Action within one second via:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;- name: Install Rust stable uses: actions-rs/toolchain@v1 with: toolchain: stable override: true components: rustfmt, clippy &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3. Group test cases into tiers&lt;/h3&gt; &lt;p&gt;Normally, I group test cases into these tiers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;tier1&lt;/code&gt;: Test cases for real use cases learned from project consumers. This tier is used for gating on building a downstream package or running downstream tests.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;tier2&lt;/code&gt;: Not real use case but for code coverage.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;slow&lt;/code&gt;: Slow test cases require massive CPU and memory resources. This tier is used for running special test cases on dedicated test hosts.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You could put &lt;code&gt;@pytest.mark.tier1&lt;/code&gt; decorator on your &lt;code&gt;pytest&lt;/code&gt; test cases, and invoke them via &lt;code&gt;pytest -m tier1&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Step 4. Enforcing the merging rules&lt;/h3&gt; &lt;p&gt;Once the CI system is up and running, developers with commit rights should enforce the following rules to maintain trust in the CI system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A patch can only be merged with CI passing or explained CI failure.&lt;/li&gt; &lt;li&gt;Each bug fix should contain a test case proving what it fixed. The patch reviewer should run the test case without the fix to reproduce the original problem. A unit test case is required when possible.&lt;/li&gt; &lt;li&gt;Each new feature should contain an integration test case explaining the common use case of this feature.&lt;/li&gt; &lt;li&gt;Fix the random failures as soon as possible.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Without enforcement, a CI system can violate this trust due to random failures and code coverage deficit.&lt;/p&gt; &lt;p&gt;In the &lt;a href="https://github.com/cathay4t/librabc"&gt;demo git project&lt;/a&gt;, we have a CI system built up with these guidelines.&lt;/p&gt; &lt;p&gt;You may check the test results in the &lt;code&gt;Checks&lt;/code&gt; tab of &lt;a href="https://github.com/cathay4t/librabc/pull/2"&gt;this pull request&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Tips for the CI system of the Rust project&lt;/h2&gt; &lt;p&gt;Here are some tips for testing the system library in Rust:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run a Rust unit test for non-public API.&lt;/li&gt; &lt;li&gt;Do a memory leak check for C binding written in Rust.&lt;/li&gt; &lt;li&gt;Pytest log collection&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Rust Unit Test for non-public API&lt;/h3&gt; &lt;p&gt;The unit test cases are supposed to test isolated function input and output without running the whole project in a real environment.&lt;/p&gt; &lt;p&gt;The Rust official document demonstrates how &lt;a href="https://doc.rust-lang.org/book/ch11-00-testing.html"&gt;&lt;code&gt;cargo test&lt;/code&gt;&lt;/a&gt; uses the automation test. But that is for the integration test, hence your test code can only access &lt;code&gt;pub&lt;/code&gt; functions and structures.&lt;/p&gt; &lt;p&gt;Since we are building up unit test cases, testing on &lt;code&gt;pub(crate)&lt;/code&gt; functions and structures is also required. We can place our test code as &lt;code&gt;mod&lt;/code&gt; of the current crate and mark test functions with &lt;code&gt;#[test]&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You may refer to the entire test code at &lt;code&gt;src/lib/unit_tests&lt;/code&gt; of &lt;a href="https://github.com/cathay4t/librabc/tree/main/src/lib/unit_tests"&gt;github repo&lt;/a&gt; or follow these steps:&lt;/p&gt; &lt;p&gt;In &lt;code&gt;lib.rs&lt;/code&gt;, we have the unit test case folder included as a normal crate internal module:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;mod unit_tests; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In &lt;code&gt;unit_tests/mod.rs&lt;/code&gt;, we mark &lt;code&gt;unit_tests/timer.rs&lt;/code&gt; as the test module:&lt;/p&gt; &lt;pre&gt;&lt;code class="rust"&gt;#[cfg(test)] mod timer; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, in &lt;code&gt;unit_tests/timer.rs&lt;/code&gt;, you can access &lt;code&gt;pub(crate)&lt;/code&gt; structure using the code &lt;code&gt;use crate::timer::RabcTimer&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Memory leak check for C binding written in Rust&lt;/h3&gt; &lt;p&gt;I recommend a memory leak check since we used &lt;code&gt;unsafe&lt;/code&gt; keywords for the Rust raw pointer in the C binding.&lt;/p&gt; &lt;p&gt;You may refer to the complete test code at &lt;code&gt;src/clib&lt;/code&gt; of the &lt;a href="https://github.com/cathay4t/librabc/tree/main/src/lib/unit_tests"&gt;github repo&lt;/a&gt; or follow this example.&lt;/p&gt; &lt;p&gt;At the top of &lt;code&gt;Makefile&lt;/code&gt;, we defined &lt;code&gt;make clib_check&lt;/code&gt; as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;.PHONY: clib_check clib_check: $(CLIB_SO_DEV_DEBUG) $(CLIB_HEADER) $(DAEMON_DEBUG) $(eval TMPDIR := $(shell mktemp -d)) cp $(CLIB_SO_DEV_DEBUG) $(TMPDIR)/$(CLIB_SO_FULL) ln -sfv $(CLIB_SO_FULL) $(TMPDIR)/$(CLIB_SO_MAN) ln -sfv $(CLIB_SO_FULL) $(TMPDIR)/$(CLIB_SO_DEV) cp $(CLIB_HEADER) $(TMPDIR)/$(shell basename $(CLIB_HEADER)) cc -g -Wall -Wextra -L$(TMPDIR) -I$(TMPDIR) \ -o $(TMPDIR)/rabc_test src/clib/tests/rabc_test.c -lrabc $(DAEMON_DEBUG) &amp; LD_LIBRARY_PATH=$(TMPDIR) \ valgrind --trace-children=yes --leak-check=full \ --error-exitcode=1 \ $(TMPDIR)/rabc_test 1&gt;/dev/null rm -rf $(TMPDIR) pkill $(DAEMON_EXEC) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Generally, it links the &lt;code&gt;src/clib/tests/rabc_test.c&lt;/code&gt; to the Rust C binding stored in a temporary folder and runs &lt;code&gt;valgrind&lt;/code&gt; for the memory check.&lt;/p&gt; &lt;h3&gt;Pytest log collection&lt;/h3&gt; &lt;p&gt;Many CI platforms support uploading test artifacts. Instead of outputting everything to the console, please store debug logs to files and only print necessary lines to the console. With that, we can identify what went wrong at the first glance of the test console output and still able to investigate it with the debug log in test artifacts.&lt;/p&gt; &lt;p&gt;In pytest, we use these options:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pytest -vvv --log-file-level=DEBUG \ --log-file-date-format='%Y-%m-%d %H:%M:%S' \ --log-file-format='%(asctime)s %(filename)s:%(lineno)d %(levelname)s %(message)s' \ --log-file=${TEST_ARTIFACTS_FOLDER}/rabc_test.log \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will store the DEBUG+ logs to &lt;code&gt;${TEST_ARTIFACTS_FOLDER}/rabc_test.log&lt;/code&gt; file instead of dumping into the console. It will be uploaded by &lt;a href="https://github.com/cathay4t/librabc/blob/main/.github/runtest.sh"&gt;&lt;code&gt;.github/runtest.sh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The Rust language is an elegant language for a Linux system library. During my work on &lt;a href="https://github.com/nmstate/nmstate"&gt;nmstate&lt;/a&gt; and &lt;a href="https://github.com/nispor/nispor/"&gt;nispor&lt;/a&gt; projects, it saved us from the worry of thread and memory safety. A trustworthy CI system enables us to embrace open source contributions around the world with confidence.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/06/build-trust-continuous-integration-your-rust-library" title="Build trust in continuous integration for your Rust library"&gt;Build trust in continuous integration for your Rust library&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Gris Ge</dc:creator><dc:date>2022-09-06T07:00:00Z</dc:date></entry></feed>
