<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Build a Kogito Serverless Workflow using Serverless Framework</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" /><author><name>Daniele Martinoli</name></author><id>59a057cf-6cdf-418b-8bb1-b494473d43b7</id><updated>2022-09-28T07:00:00Z</updated><published>2022-09-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="http://serverlessworkflow.io/"&gt;Serverless Workflow&lt;/a&gt; is a standard from the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF). &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-orchestrating-serverless"&gt;Kogito&lt;/a&gt; implements the Serverless Workflow specifications to define workflows for event-driven, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications using a DSL-based model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.serverless.com/"&gt;Serverless Framework&lt;/a&gt; is an open source framework that builds, compiles, and packages code for serverless deployment. The framework provides implementations for different cloud providers, including &lt;a href="https://knative.dev"&gt;Knative&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article walks you through the steps to integrate Kogito with Serverless Framework to build a working example on the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; Platform. The article is based on code you can find in &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework"&gt;my GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To run the demo in this article, you need the following tools on your local system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maven (at least 3.8.6)&lt;/li&gt; &lt;li&gt;Java SDK 11+&lt;/li&gt; &lt;li&gt;Docker&lt;/li&gt; &lt;li&gt;Bash terminal&lt;/li&gt; &lt;li&gt;The &lt;a href="https://www.serverless.com/framework/docs/getting-started"&gt;serverless command-line interface&lt;/a&gt; (CLI) from the Serverless Framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You also need accounts on the following systems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenShift 4.8+ (logged in as an account with the &lt;code&gt;cluster-admin&lt;/code&gt; role)&lt;/li&gt; &lt;li&gt;A &lt;a href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt; credential&lt;/li&gt; &lt;li&gt;A &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt; account&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Introducing the Kogito Newsletter Subscription Showcase example&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription"&gt;Kogito Newsletter Subscription Showcase&lt;/a&gt; is a demo based on the Kogito implementation of the Serverless Workflow specification. This example consists of two applications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;subscription-flow&lt;/code&gt;: The workflow orchestrator, defined using the Serverless Workflow specification&lt;/li&gt; &lt;li&gt;&lt;code&gt;subscription-service&lt;/code&gt;: A &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; application implementing the orchestrated services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 1 shows the system architecture of the example application.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/architecture_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/architecture_2.png?itok=D3tkbG8u" width="695" height="251" alt="The Newsletter Subscription Showcase is made of two applications, the Subscription Flow and the Subscription Service, an event Broker and a Persistent Storage " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt; KIE group &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-credit-line field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Credit Line&lt;/span&gt; &lt;span class="rhd-media-credit field__item"&gt; Thanks to KIE https://www.kie.org/ &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription &lt;/span&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The architecture of the Newsletter Subscription Showcase example. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The documentation in the repository describes how to &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription#running-on-knative"&gt;deploy and run the application on Knative&lt;/a&gt;, using the YAML configurations generated by the Maven build of the project, through the &lt;code&gt;knative&lt;/code&gt; build profile, running on &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;minikube&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article shows how to implement the same deployment in the cloud using the Serverless Framework. Our target is a Knative environment installed on OpenShift, but the principles extend to other cloud settings as well.&lt;/p&gt; &lt;h2 id="buildingkogito"&gt;Images for the Kogito Newsletter Subscription Showcase&lt;/h2&gt; &lt;p&gt;The first step is to build and publish the images of the two applications that make up the Newsletter Subscription Showcase example: &lt;code&gt;subscription-flow&lt;/code&gt; and &lt;code&gt;subscription-service&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You can download prebuilt images or build new ones from source. The prebuilt images for our example are available in the &lt;a data-saferedirecturl="https://www.google.com/url?q=http://quay.io/dmartino&amp;amp;source=gmail&amp;amp;ust=1664330157515000&amp;amp;usg=AOvVaw2UuZVMmvMJ1VTnzF74pQqa" href="http://quay.io/dmartino" target="_blank"&gt;quay.io/dmartino&lt;/a&gt; repository; if you download them, you can skip ahead to the next section, entitled "Installing Knative on OpenShift."&lt;/p&gt; &lt;p&gt;If you prefer to build the images yourself, you'll need to clone the Kogito Examples repository, build the applications using the &lt;code&gt;knative&lt;/code&gt; profile, and finally push the Docker images to your Quay repository, using the following commands. Replace &lt;code&gt;QUAY_USER_ID&lt;/code&gt; with your actual ID.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/kiegroup/kogito-examples.git cd kogito-examples git checkout stable cd serverless-workflow-examples/serverless-workflow-newsletter-subscription docker login quay.io mvn clean install -DskipTests -Pknative \ -Dquarkus.container-image.registry=quay.io \ -Dquarkus.container-image.group=QUAY_USER_ID \ -Dquarkus.container-image.push=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify that two images have been generated with the expected tag (as of today, it is &lt;code&gt;1.25.0.Final&lt;/code&gt;) on your &lt;a href="https://quay.io/repository/"&gt;Quay.io&lt;/a&gt; account, and modify the visibility of the images to make them publicly accessible.&lt;/p&gt; &lt;h2&gt;Installing Knative on OpenShift&lt;/h2&gt; &lt;p&gt;Knative can be easily installed on OpenShift using the OpenShift Serverless Operator, and this is the recommended approach we are going to follow.&lt;/p&gt; &lt;p&gt;Install the Red Hat Serverless Operator from the administrator console (Figure 2). The sequence of menu items to choose is &lt;strong&gt;OperatorHub→Red Hat OpenShift Serverless→Install.&lt;/strong&gt; Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/RHServerless.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/RHServerless.png?itok=XcrgtpH3" width="1440" height="747" alt="Administrator console -&gt; OperatorHub -&gt; Red Hat OpenShift Serverless -&gt; Install using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Select the Red Hat OpenShift Serverless operator from the OperatorHub page and install it using the default settings. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;One instance of the &lt;code&gt;KnativeServing&lt;/code&gt; custom resource is required to manage Knative serverless applications (Figure 3). Create the instance in the &lt;code&gt;knative-serving&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-serving&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Serving→Create KnativeServing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeServing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeServing1.png?itok=ASMZ9s82" width="1440" height="430" alt="Administrator console -&gt; Project: knative-serving -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Serving -&gt; Create KnativeServing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Install the KnativeServing custom resource from the knative-serving project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Additionally, one instance of the &lt;code&gt;KnativeEventing&lt;/code&gt; Custom Resource is required to manage the events around the Knative serverless applications (Figure 4). Create the instance in the &lt;code&gt;knative-eventing&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-eventing&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Eventing→Create KnativeEventing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeEventing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeEventing1.png?itok=01Fpe29z" width="1440" height="403" alt="Administrator console -&gt; Project: knative-eventing -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Eventing -&gt; Create KnativeEventing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Install the KnativeEventing custom resource from the knative-eventing project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Installing the example application&lt;/h2&gt; &lt;p&gt;To run the application, you need to install a PostgreSQL database. The application also requires some configuration changes to bring it up to date.&lt;/p&gt; &lt;h3&gt;Installing the newsletter-postgres service&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;newsletter-postgres&lt;/code&gt; service is a regular OpenShift deployment of PostgreSQL in a namespace called &lt;code&gt;newsletter-subscription-db&lt;/code&gt;. Execute the following instructions to install the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework.git cd kogito-serverless-workflow-with-serverless-framework oc create namespace newsletter-subscription-db oc adm policy add-scc-to-user anyuid -z default -n newsletter-subscription-db oc apply -f newsletter-postgres/newsletter-postgres.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Preparing the Serverless Framework&lt;/h3&gt; &lt;p&gt;To successfully run the example on OpenShift, we had to apply a few changes to the original implementation of the Knative Cloud Provider in the Serverless Framework. Such updates are needed to align the original Knative version to the one installed with the OpenShift Serverless Operator, and to introduce some extensions that support new settings and fix a few issues. Details are available in a &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework#updates-to-the-knative-provider"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because the example would not run using the default implementation of the Knative Cloud Provider, the &lt;code&gt;package.json&lt;/code&gt; descriptor includes the following dependency:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; "devDependencies": { "serverless-knative": "https://github.com/dmartinol/serverless-knative.git" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The necessary changes are available in the following GitHub repositories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/serverless-knative"&gt;Serverless Knative Plugin&lt;/a&gt;: The Knative Cloud Provider implementation for the Serverless Framework&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-serving"&gt;knative-serving&lt;/a&gt;: A Node.js module to manage Knative Serving instances&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-eventing"&gt;knative-eventing&lt;/a&gt;: A Node.js module to manage Knative Eventing instances&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Unwrapping the Serverless Framework descriptor&lt;/h3&gt; &lt;p&gt;The heart of the Serverless Framework deployment is the &lt;code&gt;serverless.yml&lt;/code&gt; file that sits at the local root of the &lt;code&gt;kogito-serverless-workflow-with-serverless-framework&lt;/code&gt; repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;service: newsletter frameworkVersion: '3' provider: name: knative # optional Docker Hub credentials you need if you're using local Dockerfiles as function handlers docker: username: ${env:DOCKER_HUB_USERNAME} password: ${env:DOCKER_HUB_PASSWORD} functions: event-display: handler: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display@sha256:a214514d6ba674d7393ec8448dd272472b2956207acb3f83152d3071f0ab1911 # autoscaler field is managed by knative provider # Just add any autoscaling related annotation and it will be propagated to the deployed Service and Revision # The plugin automatically adds the 'autoscaling.knative.dev/' prefix to the annotation name autoscaler: min-scale: 1 max-scale: 2 events: - custom: name: new.subscription.2.event-display filter: attributes: type: new.subscription - custom: name: confirm.subscription.2.event-display filter: attributes: type: confirm.subscription subscription-service: handler: Dockerfile.jvm context: ./subscription-service subscription-flow: handler: Dockerfile.jvm context: ./subscription-flow events: - custom: filter: attributes: type: confirm.subscription - sinkBinding: {} plugins: - serverless-knative&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To replicate the architecture of the original example, this deployment includes the functions listed in the following sections (the equivalents of the Knative Service resources).&lt;/p&gt; &lt;h4&gt;event-display&lt;/h4&gt; &lt;p&gt;This is an event logger application implemented with a prebuilt image from the Google Cloud Container Registry. The image is configured with a minimum of one instance to simplify logging activity, and has two &lt;code&gt;custom&lt;/code&gt; events that are mapped onto two Knative Trigger instances.&lt;/p&gt; &lt;h4&gt;subscription-service&lt;/h4&gt; &lt;p&gt;This is the service running the original &lt;code&gt;subscription-service&lt;/code&gt; application. The original source code was copied from the Kogito Examples repository under the &lt;code&gt;subscription-service&lt;/code&gt; folder, to show the option to locally build an application and deploy the serverless service using the Serverless Framework CLI.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;Dockerfile.jvm&lt;/code&gt; file defined by the &lt;code&gt;handler&lt;/code&gt; property builds the Quarkus application and injects the binding properties to connect to the &lt;code&gt;newsletter-postgres&lt;/code&gt; database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;ENV POSTGRES_PASSWORD=cGFzcwo= ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;subscription-flow&lt;/h4&gt; &lt;p&gt;This function runs the &lt;code&gt;subscription-flow&lt;/code&gt; image that you previously built, but with overridden properties to locate the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM quay.io/dmartino/serverless-workflow-newsletter-subscription-flow:1.25.0.Final ENV SUBSCRIPTION_API_URL=http://newsletter-subscription-service.sls-newsletter-dev.svc.cluster.local ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This serverless function is configured with one &lt;code&gt;custom&lt;/code&gt; event, mapped to a Knative Trigger instance, and one &lt;code&gt;sinkBinding&lt;/code&gt; event that generates the Knative SinkBinding to connect the Knative Service with the &lt;code&gt;default&lt;/code&gt; Knative Broker. The &lt;code&gt;default&lt;/code&gt; Knative Broker is automatically created by the &lt;code&gt;eventing.knative.dev/injection&lt;/code&gt; annotation attached to the Knative Trigger instances.&lt;/p&gt; &lt;h2&gt;Deploying the application with the Serverless Framework&lt;/h2&gt; &lt;p&gt;The first step is to build the &lt;code&gt;subscription-service&lt;/code&gt; application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd subscription-service $ mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following instructions assume that you have already installed the &lt;code&gt;serverless&lt;/code&gt; CLI, and set the environment variables &lt;code&gt;DOCKER_HUB_USERNAME&lt;/code&gt; and &lt;code&gt;DOCKER_HUB_PASSWORD&lt;/code&gt; to define the access credentials to the Docker Hub repository. Now deployment is just a matter of running the &lt;code&gt;serverless deploy&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless deploy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info&lt;/code&gt; command returns the deployment status of your application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless info ... Service Information service: newsletter namespace: sls-newsletter-dev Deployed functions event-display: - url: https://newsletter-event-display-sls-newsletter-dev.DOMAIN - custom - custom subscription-service: - url: https://newsletter-subscription-service-sls-newsletter-dev.DOMAIN subscription-flow: - url: https://newsletter-subscription-flow-sls-newsletter-dev.DOMAIN - custom - sinkBinding &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Validating the applications&lt;/h3&gt; &lt;p&gt;Run the applications using the default browser by executing the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ open -t $(oc get ksvc newsletter-subscription-flow -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}') $ open -t $(oc get ksvc newsletter-subscription-service -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}')&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can monitor the &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt; events through the logs of the &lt;code&gt;event-display&lt;/code&gt; pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -l serving.knative.dev/service=newsletter-event-display -f -n sls-newsletter-dev -c user-container&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Serverless Framework supports cloud deployments&lt;/h2&gt; &lt;p&gt;By using the Serverless Framework software, we successfully deployed a serverless application on Red Hat OpenShift, using Knative as the serverless framework.&lt;/p&gt; &lt;p&gt;The default implementation of the Knative Cloud Provider is missing some features and is not compatible with the Red Hat OpenShift Serverless Operator, so a patched implementation was used for the purposes of this article.&lt;/p&gt; &lt;p&gt;The application is defined using the Kogito implementation of the CNCF Serverless Workflow specification, a DSL-based model that targets the serverless technology domain.&lt;/p&gt; &lt;p&gt;Serverless Framework claims to be a cloud-agnostic tool, so nothing prevents us from extending the exercise in the future and adapting this deployment model to run on another cloud platform such as AWS or Azure.&lt;/p&gt; &lt;p&gt;For more information, please read the blog posting &lt;a href="https://knative.dev/blog/articles/event-drive-app-knative-eventing-kogito/"&gt;Orchestrating Events with Knative and Kogito&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" title="Build a Kogito Serverless Workflow using Serverless Framework"&gt;Build a Kogito Serverless Workflow using Serverless Framework&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniele Martinoli</dc:creator><dc:date>2022-09-28T07:00:00Z</dc:date></entry><entry><title type="html">Jakarta Persistence 3.1 new features</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-persistence-3-1-new-features/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-persistence-3-1-new-features/</id><updated>2022-09-27T10:12:40Z</updated><content type="html">This tutorial introduces Jakarta Persistence API 3.1 as a standard for management of persistence and O/R mapping in Java environments. We will discuss the headlines with a simple example that you can test on a Jakarta EE 10 runtime. New features added in Jakarta Persistence 3.1 There are several new features available in Jakarta Persistence ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>End-to-end field-level encryption for Apache Kafka Connect</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/27/end-end-field-level-encryption-apache-kafka-connect" /><author><name>Hans-Peter Grahsl</name></author><id>ab96f1c9-28ec-49c7-b59d-50bfb106f33c</id><updated>2022-09-27T07:00:00Z</updated><published>2022-09-27T07:00:00Z</published><summary type="html">&lt;p&gt;Encryption is valuable in &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt;, as with other communication tools, for protecting data that might be sent to unanticipated or untrustworthy recipients. This series of articles introduces the open source &lt;a href="https://github.com/hpgrahsl/kryptonite-for-kafka"&gt;Kryptonite for Kafka&lt;/a&gt; library, which is a community project I wrote. Kryptonite for Kafka requires no changes to source code, as it works entirely through configuration files. It currently does so by encrypting data through integration with &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Apache Kafka Connect&lt;/a&gt;, but there are plans to extend the scope of the project to other integration strategies for Kafka.&lt;/p&gt; &lt;p&gt;This first article in the series demonstrates encryption on individual fields in structured data, using a relational database and a NoSQL database as examples. The second article focuses on files and introduces some additional sophistication, such as using different keys for different fields.&lt;/p&gt; &lt;h2&gt;How Kryptonite for Kafka reduces the risk of a data breach&lt;/h2&gt; &lt;p&gt;Kafka can take advantage of &lt;a href="https://kafka.apache.org/documentation/#security_overview"&gt;several security features&lt;/a&gt;, ranging from &lt;a href="https://kafka.apache.org/documentation/#security_sasl"&gt;authentication&lt;/a&gt; and &lt;a href="https://kafka.apache.org/documentation/#security_authz"&gt;authorization&lt;/a&gt; to TLS-based, over-the-wire &lt;a href="https://kafka.apache.org/documentation/#security_ssl"&gt;traffic encryption&lt;/a&gt; of data on its way in and out of Kafka topics. Although these measures secure data in transit, they take place within Kafka, so there is always a stage where the broker has to see plaintext data and temporarily keep it in memory.&lt;/p&gt; &lt;p&gt;This stage can be considered a blind spot in Kafka security. The data might be encrypted on disk, but the Kafka brokers see the plaintext data right before storing it, and decrypt the data temporarily every time they read it back from disk. Therefore, disk encryption alone cannot protect against RAM scraping and other clever attack vectors.&lt;/p&gt; &lt;p&gt;Kryptonite for Kafka plugs this in-memory loophole and offers added sophistication, such as the ability to encrypt specific fields in structured data.&lt;/p&gt; &lt;h2&gt;Encryption outside the Kafka brokers&lt;/h2&gt; &lt;p&gt;Let's imagine a generic Kafka Connect data integration. Assume we would like to encrypt a certain subset of sensitive fields found in a &lt;code&gt;ConnectRecord&lt;/code&gt; payload originating from any data source (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/01a_concept_field_level_enc_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/01a_concept_field_level_enc_1.png?itok=siy2xFy1" width="1440" height="605" alt="In the source record, the social security number needs to be protected through encryption." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: In the source record, the social security number needs to be protected through encryption. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;On the consumer side, we need to decrypt the same subset of previously encrypted fields in a &lt;code&gt;ConnectRecord&lt;/code&gt; payload directed to the data sink (Figure 2).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/01b_concept_field_level_dec_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/01b_concept_field_level_dec_0.png?itok=hp-Mmori" width="1440" height="604" alt="Before delivering the record to the data sink, the social security number is automatically decrypted." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Before delivering the record to the data sink, the social security number is automatically decrypted. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The social security number must be decrypted before it's delivered to the data sink.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;To make sure the Kafka brokers themselves never get to see—let alone directly store—the original plaintext for sensitive data fields, the encryption and decryption must happen outside of the brokers, a step represented by the pink question marks in Figures 1 and 2.&lt;/p&gt; &lt;h3&gt;Kryptonite for Kafka's approach to data encryption: An overview&lt;/h3&gt; &lt;p&gt;Before Kryptonite for Kafka, no flexible and convenient way to accomplish client-side field-level cryptography was available for Kafka in &lt;a href="https://developers.redhat.com/topics/open-source"&gt;free and open source software&lt;/a&gt;, neither with the standard features in Kafka Connect nor with additional libraries or free tools from Kafka Connect's open-source ecosystem.&lt;/p&gt; &lt;p&gt;Encryption could happen directly in the original client or in some intermediary process like a sidecar or a proxy, but an intermediary would likely impose higher deployment effort and an additional operational burden. To avoid this, Kryptonite for Kafka currently performs the encryption and decryption during a Kafka Connect integration. The worker nodes of a Kafka Connect cluster encrypt the fields designated as sensitive within &lt;code&gt;ConnectRecord&lt;/code&gt; instances.&lt;/p&gt; &lt;p&gt;For that purpose, the library provides a turnkey ready single message &lt;a href="https://kafka.apache.org/documentation/#connect_transforms"&gt;transform&lt;/a&gt; (SMT) to apply field-level encryption and decryption to Kafka Connect records. The system is agnostic to the type of message serialization chosen. It uses authenticated encryption with associated data (AEAD), and in particular applies &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/chap-encryption_standards#sec-AES"&gt;AES&lt;/a&gt; in either &lt;a href="https://www.ibm.com/docs/en/zos/2.3.0?topic=operation-galoiscounter-mode-gcm"&gt;GCM&lt;/a&gt; or &lt;a href="https://www.rfc-editor.org/rfc/rfc5297"&gt;SIV&lt;/a&gt; mode.&lt;/p&gt; &lt;p&gt;Each encrypted field is represented in the output as a Base64-encoded string that contains the ciphertext of the field's value along with metadata. The metadata consists of a version identifier for the Kryptonite for Kafka library itself, a short identifier for the encryption algorithm, and an identifier for the secret key material. The metadata is authenticated but not encrypted, so Kryptonite for Kafka on the consumer side can read it and use it to decrypt the data.&lt;/p&gt; &lt;p&gt;For schema-aware message formats such as AVRO, the original schema of a data record is redacted so that encrypted fields can be stored in Base64-encoded form, changing the original data types for the affected fields.&lt;/p&gt; &lt;p&gt;In a nutshell, the configurable &lt;code&gt;CipherField&lt;/code&gt; SMT can be plugged into arbitrary Kafka Connect pipelines, safeguarding sensitive and precious data against any form of uncontrolled or illegal access on the data's way into and out of Kafka brokers (Figures 3 and 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/03a_kryptonite_smt_source_enc.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/03a_kryptonite_smt_source_enc.png?itok=hIO6xIi5" width="1440" height="604" alt="Kafka Connect encrypts the data by piping it through the SMT before delivery to the brokers." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Kafka Connect encrypts the data by piping it through the SMT before delivery to the brokers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/03b_kryptonite_smt_sink_dec.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/03b_kryptonite_smt_sink_dec.png?itok=hBp39Gq5" width="1440" height="602" alt="Kafka Connect decrypts the data by piping it through the SMT before sink connectors deliver it to the target system." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Kafka Connect decrypts the data by piping it through the SMT before sink connectors deliver it to the target system. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Advantages of encryption outside the Kafka brokers&lt;/h3&gt; &lt;p&gt;With the Kryptonite for Kafka approach, sensitive fields are automatically and always secured, not only in transit but also at rest, whenever sensitive data is outside the Kafka Connect environment. Protection is guaranteed for all target systems and for whatever downstream consumers eventually get their hands on the data. Even if someone has access to the brokers, they cannot misuse the sensitive parts of the data unless they manage to steal the secret keys from (usually remote) client environments or break the underlying cryptography itself.&lt;/p&gt; &lt;p&gt;In short, Kryptonite for Kafka offers an additional layer of data security independent of whoever owns or operates the Kafka cluster, thus protecting the data against internal attackers. Encryption also protects data against external attackers who might get fraudulent access to the Kafka topic data in the future.&lt;/p&gt; &lt;p&gt;Given such a setup, you can precisely define which downstream consumers can read the sensitive data fields. In a Kafka Connect data integration pipeline, only sink connectors explicitly given access to the secret keys can successfully decrypt the protected data parts.&lt;/p&gt; &lt;h2 id="high-level-view"&gt;Example: Exchanging structured data between a relational database and a NoSQL database&lt;/h2&gt; &lt;p&gt;This example demonstrates field-level data protection during replication between two different types of databases. The source is &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; and the sink is &lt;a href="https://www.mongodb.com/"&gt;MongoDB&lt;/a&gt;. The exchange uses the open source &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt; platform for log-based change data capture (CDC). Kryptonite for Kafka post-processes Debezium's &lt;a href="https://debezium.io/documentation/reference/stable/transformations/event-flattening.html"&gt;MySQL CDC event payloads&lt;/a&gt; to encrypt certain fields before the data leaves Kafka Connect on the client side and reaches the Kafka brokers. The MongoDB sink connector that reads these Kafka records gets access to pre-processed, properly decrypted records, and at the end of the process stores plaintext documents into MongoDB collections (Figure 5).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/04_use_case_1_overview.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/04_use_case_1_overview.png?itok=f8JCYSFH" width="1440" height="812" alt="Kafka Connect encrypts data passed in from MySQL through Debezium and decrypts data just before passing it to MongoDB, so that the Kafka brokers see only encrypted data." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Kafka Connect encrypts data passed in from MySQL through Debezium and decrypts data just before passing it to MongoDB, so that the Kafka brokers see only encrypted data. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Producer-side encryption&lt;/h3&gt; &lt;p&gt;The MySQL instance stores, among other data, an &lt;code&gt;addresses&lt;/code&gt; table containing a couple of rows representing different kinds of fictional customer addresses (Figure 6).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/04_use_case_1_db_source_table.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/04_use_case_1_db_source_table.png?itok=S8TcICcU" width="1440" height="351" alt="A column for the customer's street is one of several columns in a MySQL table." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: A column for the customer's street is one of several columns in a MySQL table. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The following Debezium MySQL source connector captures all existing addresses, together with any future changes from the table:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"name": &lt;span class="hljs-string"&gt;"mysql-source-001", &lt;span class="hljs-attr"&gt;"config": { &lt;span class="hljs-attr"&gt;"connector.class": &lt;span class="hljs-string"&gt;"io.debezium.connector.mysql.MySqlConnector", &lt;span class="hljs-attr"&gt;"value.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"value.converter.schemas.enable": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"key.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"key.converter.schemas.enable": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"tasks.max": &lt;span class="hljs-string"&gt;"1", &lt;span class="hljs-attr"&gt;"database.hostname": &lt;span class="hljs-string"&gt;"mysql", &lt;span class="hljs-attr"&gt;"database.port": &lt;span class="hljs-string"&gt;"3306", &lt;span class="hljs-attr"&gt;"database.user": &lt;span class="hljs-string"&gt;"root", &lt;span class="hljs-attr"&gt;"database.password": &lt;span class="hljs-string"&gt;"debezium", &lt;span class="hljs-attr"&gt;"database.server.id": &lt;span class="hljs-string"&gt;"1234567", &lt;span class="hljs-attr"&gt;"database.server.name": &lt;span class="hljs-string"&gt;"mysqlhost", &lt;span class="hljs-attr"&gt;"database.whitelist": &lt;span class="hljs-string"&gt;"inventory", &lt;span class="hljs-attr"&gt;"table.whitelist": &lt;span class="hljs-string"&gt;"inventory.addresses", &lt;span class="hljs-attr"&gt;"database.history.kafka.bootstrap.servers": &lt;span class="hljs-string"&gt;"kafka:9092", &lt;span class="hljs-attr"&gt;"database.history.kafka.topic": &lt;span class="hljs-string"&gt;"mysqlhost-schema", &lt;span class="hljs-attr"&gt;"transforms": &lt;span class="hljs-string"&gt;"unwrap", &lt;span class="hljs-attr"&gt;"transforms.unwrap.type": &lt;span class="hljs-string"&gt;"io.debezium.transforms.ExtractNewRecordState", &lt;span class="hljs-attr"&gt;"transforms.unwrap.drop.tombstones": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"transforms.unwrap.delete.handling.mode": &lt;span class="hljs-string"&gt;"drop" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The preceding configuration is pretty straightforward, serializing CDC events as JSON without explicit schema information and unwrapping the Debezium payloads. Unsurprisingly, the resulting plaintext record values stored in the corresponding Kafka topic (called &lt;code&gt;mysqlhost.inventory.addresses&lt;/code&gt;) will look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"3787 Brownton Road", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's assume the task is now to make sure that values originating from the &lt;code&gt;addresses&lt;/code&gt; table's &lt;code&gt;street&lt;/code&gt; column (Figure 7) for all these CDC event payloads must not be stored in Kafka topics as plaintext.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Thanks to the field-level encryption capabilities of the custom &lt;code&gt;CipherField&lt;/code&gt; SMT, fine-grained protection can be easily achieved without writing any additional code. Simply add the following to the Debezium MySQL source connector configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-string"&gt;"name": &lt;span class="hljs-string"&gt;"mysql-source-enc-001", &lt;span class="hljs-string"&gt;"config": { &lt;span class="hljs-comment"&gt;/* ... */ &lt;span class="hljs-string"&gt;"transforms": &lt;span class="hljs-string"&gt;"unwrap,cipher", &lt;span class="hljs-comment"&gt;/* ... */ &lt;span class="hljs-string"&gt;"transforms.cipher.type": &lt;span class="hljs-string"&gt;"com.github.hpgrahsl.kafka.connect.transforms.kryptonite.CipherField$Value", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_mode": &lt;span class="hljs-string"&gt;"ENCRYPT", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_data_keys": &lt;span class="hljs-string"&gt;"&lt;span class="hljs-subst"&gt;${file:/secrets/classified.properties:cipher_data_keys}", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_data_key_identifier": &lt;span class="hljs-string"&gt;"my-demo-secret-key-123", &lt;span class="hljs-string"&gt;"transforms.cipher.field_config": &lt;span class="hljs-string"&gt;"[{\"name\&lt;span class="hljs-string"&gt;":\"street\&lt;span class="hljs-string"&gt;"}]", &lt;span class="hljs-string"&gt;"transforms.cipher.predicate":&lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-string"&gt;"transforms.cipher.negate":&lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-string"&gt;"predicates": &lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-string"&gt;"predicates.isTombstone.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.predicates.RecordIsTombstone" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The SMT configuration contains &lt;code&gt;transform.cipher.*&lt;/code&gt; properties with the following meanings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Operate on the records' values (&lt;code&gt;type: "CipherField$Value"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Encrypt data (&lt;code&gt;cipher_mode: "ENCRYPT"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Load the secret key material from an external key file (&lt;code&gt;cipher_data_keys: "${file:/secrets/classified.properties:cipher_data_keys}&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use a specific secret key based on its ID (&lt;code&gt;cipher_data_key_identifier: "my-demo-secret-key-123"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Process only the &lt;code&gt;street&lt;/code&gt; field (&lt;code&gt;field_config: "[{\"name\":\"street\"}]"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Ignore any tombstone records (see predicate definitions).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Maintaining the secrecy of the secret key materials is of utmost importance, because leaking any of the secret keys renders encryption useless. Secret key exchange is a complex topic of its own. This example obtains the keys indirectly from an external file, which contains a single property called &lt;code&gt;cipher_data_keys&lt;/code&gt; that in turn holds an array of key definition objects (&lt;code&gt;identifier&lt;/code&gt; and &lt;code&gt;material&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-properties"&gt;cipher_data_keys=[ { &lt;span class="hljs-string"&gt;"identifier": &lt;span class="hljs-string"&gt;"my-demo-secret-key-123", &lt;span class="hljs-string"&gt;"material": { &lt;span class="hljs-string"&gt;"primaryKeyId": &lt;span class="hljs-number"&gt;1000000001, &lt;span class="hljs-string"&gt;"key": [ { &lt;span class="hljs-string"&gt;"keyData": { &lt;span class="hljs-string"&gt;"typeUrl": &lt;span class="hljs-string"&gt;"type.googleapis.com/google.crypto.tink.AesGcmKey", &lt;span class="hljs-string"&gt;"value": &lt;span class="hljs-string"&gt;"GhDRulECKAC8/19NMXDjeCjK", &lt;span class="hljs-string"&gt;"keyMaterialType": &lt;span class="hljs-string"&gt;"SYMMETRIC" }, &lt;span class="hljs-string"&gt;"status": &lt;span class="hljs-string"&gt;"ENABLED", &lt;span class="hljs-string"&gt;"keyId": &lt;span class="hljs-number"&gt;1000000001, &lt;span class="hljs-string"&gt;"outputPrefixType": &lt;span class="hljs-string"&gt;"TINK" } ] } } ] &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details about externalizing sensitive configuration parameters in Kafka Connect can be found in the &lt;a href="https://github.com/hpgrahsl/kafka-connect-transform-kryptonite#externalize-configuration-parameters"&gt;library's documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The resulting source connector configuration achieves the goal of partially encrypting all Debezium CDC event payloads before they are sent to and stored by the Kafka brokers. The resulting record values in the topic &lt;code&gt;mysqlhost.inventory.addresses&lt;/code&gt; reflect the encryption:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"NLWw4AshpLIIBjLoqM0EgDiUGooYH3jwDnW71wdInMGomFVLHo9AQ6QPEh6fmLRJKVwE3gwwsWux", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Consumer-side decryption&lt;/h3&gt; &lt;p&gt;The sink connector that picks up these partially encrypted Kafka records needs to properly apply the custom &lt;code&gt;CipherField&lt;/code&gt; SMT as well. Only then can the connector get access to the previously encrypted Debezium CDC payload fields before writing them into the targeted data store, which is MongoDB in this case.&lt;/p&gt; &lt;p&gt;The MongoDB sink connector configuration for this example might look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"name": &lt;span class="hljs-string"&gt;"mongodb-sink-dec-001", &lt;span class="hljs-attr"&gt;"config": { &lt;span class="hljs-attr"&gt;"topics": &lt;span class="hljs-string"&gt;"mysqlhost.inventory.addresses", &lt;span class="hljs-attr"&gt;"connector.class": &lt;span class="hljs-string"&gt;"com.mongodb.kafka.connect.MongoSinkConnector", &lt;span class="hljs-attr"&gt;"key.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"key.converter.schemas.enable":&lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"value.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"value.converter.schemas.enable":&lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"tasks.max": &lt;span class="hljs-string"&gt;"1", &lt;span class="hljs-attr"&gt;"connection.uri":&lt;span class="hljs-string"&gt;"mongodb://mongodb:27017", &lt;span class="hljs-attr"&gt;"database":&lt;span class="hljs-string"&gt;"kryptonite", &lt;span class="hljs-attr"&gt;"document.id.strategy":&lt;span class="hljs-string"&gt;"com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy", &lt;span class="hljs-attr"&gt;"delete.on.null.values": &lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-attr"&gt;"transforms": &lt;span class="hljs-string"&gt;"createid,removefield,decipher", &lt;span class="hljs-attr"&gt;"transforms.createid.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.ReplaceField$Key", &lt;span class="hljs-attr"&gt;"transforms.createid.renames": &lt;span class="hljs-string"&gt;"id:_id", &lt;span class="hljs-attr"&gt;"transforms.removefield.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.ReplaceField$Value", &lt;span class="hljs-attr"&gt;"transforms.removefield.blacklist": &lt;span class="hljs-string"&gt;"id", &lt;span class="hljs-attr"&gt;"transforms.decipher.type": &lt;span class="hljs-string"&gt;"com.github.hpgrahsl.kafka.connect.transforms.kryptonite.CipherField$Value", &lt;span class="hljs-attr"&gt;"transforms.decipher.cipher_mode": &lt;span class="hljs-string"&gt;"DECRYPT", &lt;span class="hljs-attr"&gt;"transforms.decipher.cipher_data_keys": &lt;span class="hljs-string"&gt;"${file:/secrets/classified.properties:cipher_data_keys}", &lt;span class="hljs-attr"&gt;"transforms.decipher.field_config": &lt;span class="hljs-string"&gt;"[{\"name\":\"street\"}]", &lt;span class="hljs-attr"&gt;"transforms.decipher.predicate":&lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-attr"&gt;"transforms.decipher.negate":&lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-attr"&gt;"predicates": &lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-attr"&gt;"predicates.isTombstone.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.predicates.RecordIsTombstone" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The interesting part again is the configuration's &lt;code&gt;transform.decipher.*&lt;/code&gt; properties, which are defined as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Operate on the records' values (&lt;code&gt;type: "CipherField$Value"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Decrypt data (&lt;code&gt;cipher_mode: "DECRYPT"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Load the secret key material from an external key file (&lt;code&gt;cipher_data_keys: "${file:/secrets/classified.properties:cipher_data_keys}&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use a specific secret key based on its ID (&lt;code&gt;cipher_data_key_identifier: "my-secret-key-123"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Process only the &lt;code&gt;name&lt;/code&gt; field (&lt;code&gt;field_config: "[{\"name\":\"street\"}]"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Ignore any tombstone records (see predicate definitions).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this configuration in place, Kafka Connect processes all CDC events by first applying the custom &lt;code&gt;CipherField&lt;/code&gt; SMT to decrypt selected fields, and then handing them over to the sink connector itself to write the plaintext documents into a MongoDB database collection called &lt;code&gt;kryptonite.mysqlhost.inventory.addresses&lt;/code&gt;. An example document for the address having &lt;code&gt;_id=13&lt;/code&gt; is shown here in its JSON representation:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"_id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"3787 Brownton Road", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example has shown how end-to-end change data capture pipelines between heterogenous databases can be secured by explicitly protecting sensitive CDC payload fields. Proper configuration of a custom SMT is sufficient to achieve client-side field-level encryption and decryption of Kafka Connect records on their way in and out of Kafka topics. A &lt;a href="https://github.com/hpgrahsl/rhd-csflc-kafka-connect-demos/tree/main/use_case_1"&gt;fully working example&lt;/a&gt; of this database integration scenario can be found in the accompanying &lt;a href="https://github.com/hpgrahsl/rhd-csflc-kafka-connect-demos"&gt;demo scenario repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The second article in this series will show the use of Kryptonite for Kafka with files, introduce more features, and discuss plans for the future of the project.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/27/end-end-field-level-encryption-apache-kafka-connect" title="End-to-end field-level encryption for Apache Kafka Connect"&gt;End-to-end field-level encryption for Apache Kafka Connect&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Hans-Peter Grahsl</dc:creator><dc:date>2022-09-27T07:00:00Z</dc:date></entry><entry><title>Find errors in packages through mass builds</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/26/find-errors-packages-through-mass-builds" /><author><name>Frédéric Bérat</name></author><id>0ebf20fc-5f39-4cff-bf81-9b729880a1b3</id><updated>2022-09-26T07:00:00Z</updated><published>2022-09-26T07:00:00Z</published><summary type="html">&lt;p&gt;Even after thorough unit testing on both applications and their library dependencies, builds often go wrong and turn up hidden errors. This article introduces a new tool, the &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;Mass Prebuilder&lt;/a&gt; (MPB), that automates builds on enormous numbers of reverse dependencies to find problems that are not caught through package testing.&lt;/p&gt; &lt;p&gt;Let's look at a simple example. Roughly 1,200 packages in Red Hat-based distributions depend on &lt;a href="https://www.gnu.org/software/autoconf/"&gt;GNU Autoconf&lt;/a&gt;. Knowing all of the packages by heart is unlikely, building them manually would take ages, and judging whether a failure is due to a change in GNU Autoconf is very difficult. A job for the Mass Prebuilder!&lt;/p&gt; &lt;h2&gt;What is the Mass Prebuilder?&lt;/h2&gt; &lt;p&gt;The Mass Prebuilder is an open source set of tools that help developers create mass rebuilds around a limited set of packages, in order to assess the stability of a given update.&lt;/p&gt; &lt;p&gt;The idea is rather simple. The packages your team is working on and want to release are the &lt;em&gt;engineering packages&lt;/em&gt;. Given a package or a set of packages, called the &lt;em&gt;main packages&lt;/em&gt;, the Mass Prebuilder calculates the list of their direct reverse dependencies: packages that explicitly mark one of the main packages in their &lt;code&gt;BuildRequires&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;The Mass Prebuilder first builds the main packages using the distribution's facilities, which should include a set of test cases that validate general functioning. Assuming these packages are built successfully, they are then used as base packages to build the reverse dependencies and execute their own test cases.&lt;/p&gt; &lt;p&gt;This process yields a first set of results, which include successful builds (hopefully the majority), but also failures that might or might not be the result of changes introduced by modifications to the main packages. To clarify the source of the problem, as soon as a failure is detected, the Mass Prebuilder creates another mass build that includes only the reverse dependencies that failed to build during the original run. This new build runs in parallel to the original, but without the changes that were introduced into the main packages—in other words, it's a pristine build.&lt;/p&gt; &lt;p&gt;Once all the package builds are done, they can be broken down into the following categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Successfully built packages&lt;/li&gt; &lt;li&gt;Packages that failed to build only with the modified packages&lt;/li&gt; &lt;li&gt;Packages that failed to build with both the modifications and the pristine version&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The first category can likely be ignored, because it consists of packages that don't seem to have been affected by the changes.&lt;/p&gt; &lt;p&gt;The second category needs much more attention. These are the ones that cry, "Hey, there seems to be a big problem with your changes." The failures need to be analyzed to figure out the root cause of the problem—they could arise from changes that have been introduced, for instance, or maybe from a mistake made by the final user (e.g., use of a deprecated feature that got removed).&lt;/p&gt; &lt;p&gt;The last category of failures is a bit trickier. Since the build failed with the pristine packages, there may be hidden errors in them that were revealed by the new changes.&lt;/p&gt; &lt;h2&gt;How does the Mass Prebuilder work?&lt;/h2&gt; &lt;p&gt;Now let's see a bit more in detail what the Mass Prebuilder does under the hood.&lt;/p&gt; &lt;p&gt;The MPB is a set of &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; scripts that abstract the use of commonly available infrastructures. Although the infrastructure used initially is &lt;a href="https://pagure.io/copr/copr"&gt;Copr&lt;/a&gt;, there are plans to implement support for other infrastructures such as &lt;a href="https://pagure.io/koji/"&gt;Koji&lt;/a&gt; and potentially &lt;a href="https://beaker-project.org/"&gt;Beaker&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If these infrastructures are used to build the packages, the MPB is in charge of orchestrating the builds by preparing the project, calculating the list of packages to be built, and providing a simple report once everything is done.&lt;/p&gt; &lt;p&gt;Figure 1 shows a simplified overview of the system.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_0.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_0.jpg?itok=z9gvBXlt" width="600" height="276" alt="Diagram showing the Mass Prebuilder running on top of build environments and interacting with a dedicated database and configuration" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Mass Prebuilder runs on top of build environments and interacts with a dedicated database and configuration. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Under the hood, the tool has a back-end/front-end design. Most of the work is done as generically as possible, so that the back ends implement only the direct interfaces with the infrastructures.&lt;/p&gt; &lt;h2&gt;The Mass Prebuilder process&lt;/h2&gt; &lt;p&gt;The MPB set of scripts is currently available in &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;version 0.2.0&lt;/a&gt;, which allows you to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a dedicated Copr project where an engineering version of your packages can be built. This package can be provided as a source RPM (SRPM), a &lt;a href="https://github.com/release-engineering/dist-git"&gt;DistGit repository&lt;/a&gt; with a specific tag, a Git repository, or a URL providing access to an SRPM.&lt;/li&gt; &lt;li&gt;Trigger the build for these packages, and report its success or failure.&lt;/li&gt; &lt;li&gt;Automatically calculate direct reverse dependencies for the main packages, try to group them by priority based on their interdependencies, and trigger the builds for these reverse dependencies.&lt;/li&gt; &lt;li&gt;Generate a simple report on the build status for the reverse dependencies. If a reverse dependency isn't building, that triggers a new build on a pristine Copr project (without your engineering packages), and checks to see whether the packages fail there too.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As explained earlier, failures are then split into the following categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Failed: Packages that only failed in the project that included your engineering packages&lt;/li&gt; &lt;li&gt;Manual confirmation needed: Packages that failed on both sides&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At the end of the process, the tool retrieves all the available data for the packages that failed so that you can walk through them and verify whether failures are related to your package update.&lt;/p&gt; &lt;p&gt;The process is therefore decomposed into multiple stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preparation (your engineering packages are built)&lt;/li&gt; &lt;li&gt;Checking the preparation&lt;/li&gt; &lt;li&gt;Build&lt;/li&gt; &lt;li&gt;Checking the build&lt;/li&gt; &lt;li&gt;Collecting and reporting data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The process can be interrupted safely during the checks and the collecting operations. If you interrupt the Mass Prebuilder, it shows you the command to restart it later on. This checkpointing is quite useful, as some packages take a long time to build and you might not want your machine running for 30 hours straight.&lt;/p&gt; &lt;h2&gt;Configuration&lt;/h2&gt; &lt;p&gt;Prior to using the Mass Prebuilder, make sure you can communicate with the appropriate infrastructure. Prepare a small configuration file specifying what you want to build and how. Copr itself requires a &lt;code&gt;~/.config/copr&lt;/code&gt; file (as given by &lt;a href="https://copr.fedorainfracloud.org/api/"&gt;the Copr API&lt;/a&gt; when logged in) to ensure that the &lt;code&gt;copr whoami&lt;/code&gt; command gives the expected output.&lt;/p&gt; &lt;p&gt;The Mass Prebuilder configuration file can be provided in the following ways. It looks for the files in the order shown, and uses the first file it finds:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;A local &lt;code&gt;mpb.config&lt;/code&gt; file&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Through a command-line option:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mpb --config=~/work/mpb/autoconf/my_config_file&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;In the default path: &lt;code&gt;~/.mpb/config&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The following example uses a local file to show how to use the tool to check your use of the Autoconf:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir -p ~/work/mpb/autoconf $ cd ~/work/mpb/autoconf $ cat &gt; mpb1.config &lt;&lt; EOF &gt; arch: x86_64 chroot: fedora-rawhide packages: autoconf: src_type: file src: /home/fberat/work/fedora/autoconf/autoconf-2.72c-1.fc37.src.rpm data: /home/fberat/work/mpb/autoconf verbose: 1 &gt; EOF $ mpb Loading mpb.config Using copr back-end Populating package list with autoconf Executing stage 0 (prepare) Prepared build mpb.18 (ID: 18) Executing stage 1 (check_prepare) Checking build for mpb.18 (ID: 18) You can now safely interrupt this stage Restart it later using one of the following commands: "mpb --buildid 18" "mpb --buildid 18 --stage 1" Build status: / 0 out of 1 builds are done. Pending: 0 Running: 1 Success: 0 Under check: 0 Manual confirmation needed: 0 Failed: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration file can contain a lot of parameters. We'll focus here on &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;reversedeps&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;The name configuration parameter&lt;/h3&gt; &lt;p&gt;You can specify a &lt;code&gt;name&lt;/code&gt; that will help you identify your project. This name will replace &lt;code&gt;mpb.18&lt;/code&gt; from the previous example. Be careful, though, because this parameter will also be the name used in your Copr project. If the name already exists, specifying it might lead to unexpected behavior. If you choose to set the name yourself instead of having it automatically generated, I recommend adding a line to the configuration file as follows. Replace &lt;code&gt;&lt;N&gt;&lt;/code&gt; with the number given by the build, which was 18 in the previous example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ echo "build_id: &lt;N&gt;" &gt;&gt; mpb.config&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Restart the MPB later using either of the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mpb --buildid &lt;N&gt; $ mpb --buildid &lt;N&gt; --stage 1 $ mpb # If you stored the build_id in the configuration file&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The reversedeps configuration parameter&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;reversedeps&lt;/code&gt; parameter contains a list of reverse dependencies. If you specify this parameter, the Mass Prebuilder uses your list instead of calculating the reverse dependencies by checking the main packages. This parameter is useful if you want to rebuild only a subset of packages instead of, say, the 6,000+ ones for GCC.&lt;/p&gt; &lt;p&gt;An example using &lt;code&gt;reversedeps&lt;/code&gt; follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;&gt; arch: x86_64 chroot: fedora-rawhide packages: autoconf: src_type: file src: /home/fberat/work/fedora/autoconf/autoconf-2.72c-1.fc37.src.rpm reversedeps: list: libtool: priority: 0 automake: priority: 1 name: autoconf272-1 data: /home/fberat/work/mpb/autoconf verbose: 1 &gt; EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can include the &lt;code&gt;priority&lt;/code&gt; field in the reverse dependencies to specify a build order. The previous example builds &lt;code&gt;libtool&lt;/code&gt; before &lt;code&gt;automake&lt;/code&gt;. If you don't need to control the build order, the previous configuration can be simplified to:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; reversedeps: list: libtool automake&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Getting results from the Mass Prebuilder&lt;/h2&gt; &lt;p&gt;Let's come back to our Autoconf build. After a while (about 30 minutes during one test run), the tool can move to the next stage and calculate the reverse dependencies that would be valid for an x86-64 processor:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; Executing stage 2 (build) Calculating reverse dependencies. Level 0 depth for x86_64 Checking 2316 packages. Retrieved 1151 packages. Prepare discriminator for priorities calculation 100% done Setting priorities 7 pass done. Populating package list with [package names here]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, 1,151 packages are built for x86-64. The &lt;code&gt;build&lt;/code&gt; stage is followed by a &lt;code&gt;check_build&lt;/code&gt; stage:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; Executing stage 3 (check_build) Checking build for autoconf272-1 (ID: 18) You can now safely interrupt this stage Restart it later using one of the following commands: "mpb --buildid 18" "mpb --buildid 18 --stage 3" Build status: \ 3 out of 1151 builds are done. Pending: 1146 Running: 1 Success: 3 Under check: 1 Manual confirmation needed: 0 Failed: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After all these builds finish, the Mass Prebuilder collects data that you can retrieve from the following path, based on your configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;&lt;data field&gt;/&lt;name of the MPB project&gt;/&lt;build status&gt;/&lt;chroot&gt;/&lt;name of the package&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For my particular run, the paths are:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;~/work/mpb/autoconf/autoconf272-1/FAILED/fedora-rawhide-x86_64/ ~/work/mpb/autoconf/autoconf272-1/UNCONFIRMED/fedora-rawhide-x86_64/&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;A real-life example: Errors building with autoconf2.72c&lt;/h2&gt; &lt;p&gt;As of June 2022, GNU Autoconf 2.72 is still under development and is therefore unstable. Yet the upstream maintainers are tagging the mainline with intermediate versions that could be useful to test early, in order to limit the problems when the final release comes out.&lt;/p&gt; &lt;p&gt;This is where the MPB comes in handy.&lt;/p&gt; &lt;p&gt;In Fedora 36 x86_64, about 1,151 packages depend directly on Autoconf. When built with Autoconf 2.72c pre-release, we currently get the following result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Success: 1,033&lt;/li&gt; &lt;li&gt;Manual confirmation needed: 47&lt;/li&gt; &lt;li&gt;Failed: 71&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's skip the "Manual confirmation needed" packages, which failed to build with both the 2.72c version and the original 2.71 version. There may be failures to fix among them, but the "Failed" packages offer enough interesting examples for this article to keep us busy.&lt;/p&gt; &lt;p&gt;A common pattern turns up in the 71 failures: 65 of them are due to a malformed configuration script. A sample error found for the PHP package is:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;./configure: line 104153: syntax error: unexpected end of file&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although this seems to be quite a common failure, it went through the internal tests from Autoconf without being noticed.&lt;/p&gt; &lt;p&gt;Six more failures need deeper analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;am-utils: A straightforward failure due to a hard requirement for the Autoconf version (it requires 2.69 or 2.71 exclusively).&lt;/li&gt; &lt;li&gt;cyrus-imapd: A missing &lt;code&gt;krb.h&lt;/code&gt; header. Even though the configure stage reports that the file isn't available, the application still tries to use it. It's strange not to find this header, and the message may hide a bigger problem.&lt;/li&gt; &lt;li&gt;libmng: A &lt;code&gt;zlib library not found&lt;/code&gt; error because zlib-devel got installed as part of the dependencies.&lt;/li&gt; &lt;li&gt;libverto: This failure seems unrelated to Autoconf; one of its internal libraries seems to have changed its name. Yet it is strange that this failure appeared only with Autoconf 2.72c.&lt;/li&gt; &lt;li&gt;mingw-libmng: Unresolved symbols during linking. May be unrelated to Autoconf, unless a change in the configuration modified the build process.&lt;/li&gt; &lt;li&gt;nfdump: An error revealed during the configure run; the &lt;code&gt;FT2NFDUMP&lt;/code&gt; conditional was never defined. This error might also be due to a malformed configure script.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next steps, which are beyond the scope of this article, are to research the errors to gain a deeper understanding of the problems and create appropriate bug tickets for the corresponding components. Although in the current case most of the problems are likely in Autoconf itself, there may be some cases where the issues are in the user's component (such as am-utils here).&lt;/p&gt; &lt;p&gt;Overall, this sample run of the Mass Prebuilder gives a good idea of the kind of failures that would have been missed by simply building the Autoconf package and relying on its internal tests for gatekeeping. The large panel view provided by running with a distribution is quite beneficial, and can improve the overall quality of the packages being provided.&lt;/p&gt; &lt;h2&gt;Where can you find the Mass Prebuilder?&lt;/h2&gt; &lt;p&gt;As of July 2022, the tool is available as a package at &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;my Copr repository&lt;/a&gt;. Packages are available for &lt;a href="https://developers.redhat.com/articles/2022/06/07/thousands-pypi-and-rubygems-rpms-now-available-rhel-9"&gt;Extra Packages for Enterprise Linux&lt;/a&gt; (EPEL) 9 and 8, and for Fedora 35, 36, and 37.&lt;/p&gt; &lt;p&gt;The sources can be found in &lt;a href="https://gitlab.com/fberat/mass-prebuild"&gt;my GitLab repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Current limitations&lt;/h2&gt; &lt;p&gt;The Mass Prebuilder at the time of writing is still under heavy development. That means that there might be missing features you'd like to see, that there might be bugs (though I've done my best to limit them), and that interfaces may have to change a bit by the time stable releases come.&lt;/p&gt; &lt;p&gt;While I tested the tool, it appeared that builds are not fully reliable. Although reported successes are trustworthy, reported failures might not be. There were some cases where the failure cropped up because the infrastructure was incapable of installing build dependencies, even on a stable release such as Fedora 35. This failure is hard to diagnose because it doesn't make sense. The same build with no changes could result in a different status if started with a few seconds delay.&lt;/p&gt; &lt;p&gt;The data that can be collected out of a failed Copr build is relatively limited. For instance, in the problem just described, there is no way to retrieve the failing configuration script. A local build needs to be made for that.&lt;/p&gt; &lt;p&gt;If you have any suggestions, if you find an issue, or if the tool doesn't behave the way you expect, don't hesitate to contact me and give me feedback, or leave a comment at the bottom of the article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/26/find-errors-packages-through-mass-builds" title="Find errors in packages through mass builds"&gt;Find errors in packages through mass builds&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Frédéric Bérat</dc:creator><dc:date>2022-09-26T07:00:00Z</dc:date></entry><entry><title type="html">RESTEasy 6.2.0.Final Release</title><link rel="alternate" href="https://resteasy.github.io/2022/09/23/resteasy-6.2.0.Final-release/" /><author><name /></author><id>https://resteasy.github.io/2022/09/23/resteasy-6.2.0.Final-release/</id><updated>2022-09-23T18:11:11Z</updated><dc:creator /></entry><entry><title>Join the Red Hat team at NodeConf EU 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/23/join-red-hat-team-nodeconf-eu-2022" /><author><name>Lucas Holmquist</name></author><id>af2d04c4-c425-4b66-b2db-7b7fc5ba23c0</id><updated>2022-09-23T07:00:00Z</updated><published>2022-09-23T07:00:00Z</published><summary type="html">&lt;p&gt;It's that time of the year again, and NodeConf EU is almost upon us. This annual event is one of the leading &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="43652567-d1ab-4765-a588-4e905032ad7f" href="https://developers.redhat.com/topics/nodejs" title="Node.js: Develop server-side JavaScript applications"&gt;Node.js&lt;/a&gt; events in Europe. It brings together contributors and innovators from the Node.js community to deliver a wide range of talks and workshops.&lt;/p&gt; &lt;p&gt;The conference will be back in person this year after being virtual for the past two years on October 3rd–5th in Kilkenny, Ireland.&lt;/p&gt; &lt;p&gt;The Node.js team here at Red Hat will be talking about lesser-known Node.js Core modules as well as guiding attendees through a workshop that will get you familiar with cloud-native development with Node.js. &lt;/p&gt; &lt;h2&gt;Talk: Journey into mystery: Lesser-known Node Core modules and APIs&lt;/h2&gt; &lt;p&gt;Wednesday, October 4th, 2022, 9:30 UTC&lt;/p&gt; &lt;p&gt;Presenter: Luke Holmquist (&lt;a href="https://twitter.com/sienaluke"&gt;@sienaluke&lt;/a&gt;), Senior Software Engineer, Red Hat&lt;/p&gt; &lt;p&gt;One of the key concepts of Node.js is its modular architecture, and Node makes it very easy to use a wide variety of modules and &lt;a href="https://developers.redhat.com/topics/api-management"&gt;APIs&lt;/a&gt; from the community. Some of the modules and APIs that are part of Node.js Core are very familiar, like HTTP and Events. But what about those lesser-known core modules just waiting to be used? This talk will journey into mystery as we explore some of the lesser-known Core modules and APIs that Node.js offers.&lt;/p&gt; &lt;h2&gt;Workshop: Elevating Node.js applications to the cloud&lt;/h2&gt; &lt;p&gt;Wednesday, October 4th, 2022, 3:00 UTC&lt;/p&gt; &lt;p&gt;Presenters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bethany Griggs, Senior Software Engineer, Red Hat&lt;/li&gt; &lt;li&gt;Michael Dawson (&lt;a href="https://twitter.com/mhdawson1"&gt;@mhdawson1&lt;/a&gt;), Node.js Lead, Red Hat&lt;/li&gt; &lt;li&gt;Luke Holmquist (&lt;a href="https://twitter.com/sienaluke"&gt;@sienaluke&lt;/a&gt;), Senior Software Engineer, Red Hat&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This workshop provides an introduction to cloud-native development with Node.js. We will walk you through building cloud-native Node.js applications, incorporating typical components, including observability components for &lt;a href="https://developers.redhat.com/articles/2021/05/10/introduction-nodejs-reference-architecture-part-2-logging-nodejs"&gt;logging&lt;/a&gt;, metrics, and more. Next, we'll show you how to deploy your application to cloud environments. The workshop will cover cloud-native concepts and technologies, including health checks, metrics, building &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and deployments to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For a full list of the various talks and workshops, check out the &lt;a href="https://www.nodeconf.eu/agenda"&gt;NodeConf EU 2022 agenda&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Collaborator Summit&lt;/h2&gt; &lt;p&gt;There will also be a OpenJS Collaborator Summit in Dublin, Ireland on October 1-2, 2022, two days before NodeConf EU. We hope to see you there to discuss all things &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; and Node.js. Our team members will be leading or active participants in many sessions.&lt;/p&gt; &lt;p&gt;The Collab Summit is for maintainers or core contributors of an OpenJS project, plus any open source enthusiast interested in participating. This is the time for deep dives on important topics and to meet with people working across your favorite JavaScript projects. Get more details on the &lt;a href="https://openjsf.org/blog/2022/09/01/openjs-collaborator-summit-join-us-in-dublin-virtual-october-1-2%EF%BF%BC/"&gt;OpenJS website&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;More Node.js resources&lt;/h2&gt; &lt;p&gt;Don't miss the latest installments of our series on the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to learn more about Red Hat and IBM’s involvement in the Node.js community and what we are working on, check out our topic pages at &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Red Hat Developer&lt;/a&gt; and &lt;a href="https://developer.ibm.com/languages/node-js/"&gt;IBM Developer&lt;/a&gt;. &lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/23/join-red-hat-team-nodeconf-eu-2022" title="Join the Red Hat team at NodeConf EU 2022"&gt;Join the Red Hat team at NodeConf EU 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2022-09-23T07:00:00Z</dc:date></entry><entry><title type="html">Creating your first cloud-agnostic serverless application with Java</title><link rel="alternate" href="https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html" /><author><name>Helber Belmiro</name></author><id>https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html</id><updated>2022-09-22T10:35:00Z</updated><content type="html">If you are new to Serverless Workflow or serverless in general, creating a simple application for a serverless infrastructure is a good place to start. In this article, you will run through the steps to create your first serverless Java application that runs on any cloud. WHAT IS SERVERLESS? Contrary to what the name says, there are still servers in serverless, but you don’t need to worry about managing them. You just need to deploy your containers and the serverless infrastructure is responsible for providing resources to your application scale up or down. The best part is that it automatically scales up when there is a high demand or scales to zero when there is no demand. This will reduce the amount of money you spend with the cloud. WHAT WILL YOU CREATE? You will use Quarkus to create a simple Java application that returns a greeting message to an HTTP request and deploy it to Knative. WHY KNATIVE? In the beginning, serverless applications used to consist of small pieces of code that were run by a cloud vendor, like AWS Lambda. In this first phase, the applications had some limitations and were closely coupled to the vendor libraries. Knative enables developers to run serverless applications on a Kubernetes cluster. This gives you the flexibility to run your applications on any cloud, on-premises, or even mix all of them. WHY QUARKUS? Because serverless applications need to start fast. Since the biggest advantage of serverless is scale up and down (even zero) according to demand, serverless applications need to start fast when scaling up, otherwise, requests would be denied. One of the greatest characteristics of Quarkus applications is their super fast start-up. Also, Quarkus is , which means that it’s easy to deploy Quarkus applications to Kubernetes without having to understand the intricacies of the underlying Kubernetes framework. REQUIREMENTS * A local Knative installation. See . * This article uses minikube as the local Kubernetes cluster. * kn CLI installed. See . * JDK 11+ installed with JAVA_HOME configured appropriately. * Apache Maven 3.8.1+. * GraalVM (optional to deploy a native image). CREATE A QUARKUS APPLICATION &gt; NOTE: If you don’t want to create the application, you can just clone it &gt; from  and skip to  mvn io.quarkus.platform:quarkus-maven-plugin:2.11.2.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=knative-serving-quarkus-demo cd knative-serving-quarkus-demo RUN YOUR APPLICATION LOCALLY To verify that you created the project correctly, run the project locally by running the following command: mvn quarkus:dev After downloading the dependencies and building the project, you should see an output similar to: __ ____ __ _____ ___ __ ____ ______ --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,&lt; / /_/ /\ \ --\___\_\____/_/ |_/_/|_/_/|_|\____/___/ 2022-08-15 16:50:25,135 INFO [io.quarkus] (Quarkus Main Thread) knative-serving-quarkus-demo 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.11.2.Final) started in 1.339s. Listening on: http://localhost:8080 2022-08-15 16:50:25,150 INFO [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated. 2022-08-15 16:50:25,150 INFO [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, resteasy-reactive, smallrye-context-propagation, vertx] On a different terminal window or in the browser, you can access the application by sending a request to the  endpoint: curl -X 'GET' 'http://localhost:8080/hello' -H 'accept: text/plain' If you see the following output, then you have successfully created your application: Hello from RESTEasy Reactive Hit Ctrl + C to stop the application. PREPARE YOUR APPLICATION FOR DEPLOYMENT TO KNATIVE ADD THE REQUIRED DEPENDENCIES Add the following dependencies to the pom.xml file: &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-kubernetes&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-container-image-jib&lt;/artifactId&gt; &lt;/dependency&gt; CONFIGURE THE APPLICATION FOR DEPLOYMENT TO KNATIVE Add the following configuration to the src/main/resources/application.properties file: quarkus.kubernetes.deployment-target=knative quarkus.container-image.group=dev.local/hbelmiro &gt; NOTE: In the quarkus.container-image.group property, replace hbelmiro with &gt; your container registry username. DEPLOY YOUR APPLICATION TO KNATIVE START THE MINIKUBE TUNNEL &gt; NOTE: This step is only necessary if you are using minikube as the local &gt; Kubernetes cluster. On a different terminal window, run the following command to start the minikube tunnel: minikube tunnel --profile knative You should see an output similar to the following: Status: machine: knative pid: 223762 route: 10.96.0.0/12 -&gt; 192.168.49.2 minikube: Running services: [kourier] errors: minikube: no errors router: no errors loadbalancer emulator: no errors Leave the terminal window open and running the above command. CONFIGURE THE CONTAINER CLI TO USE THE CONTAINER ENGINE INSIDE MINIKUBE eval $(minikube -p knative docker-env) DEPLOY THE APPLICATION Run the following command to deploy the application to Knative: mvn clean package -Dquarkus.kubernetes.deploy=true You should see an output similar to the following: [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Deploying to knative server: https://192.168.49.2:8443/ in namespace: default. [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: Service knative-serving-quarkus-demo. [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 8952ms [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ CHECK THE KNATIVE DEPLOYED SERVICES Run the following command to check the Knative deployed services: kn service list You should see your application listed on the deployed services like the following: NAME URL LATEST AGE CONDITIONS READY REASON knative-serving-quarkus-demo http://knative-serving-quarkus-demo.default.10.106.207.219.sslip.io knative-serving-quarkus-demo-00001 23s 3 OK / 3 True &gt; IMPORTANT: In the above output, check the READY status of the application. If &gt; the status is not True, then you need to wait for the application to be ready, &gt; or there is a problem with the deployment. SEND A REQUEST TO THE DEPLOYED APPLICATION Use the URL returned by the above command to send a request to the deployed application. curl -X 'GET' 'http://knative-serving-quarkus-demo.default.10.106.207.219.sslip.io/hello' -H 'accept: text/plain' You should see the following output: Hello from RESTEasy Reactive GOING NATIVE You can create a native image of your application to make it start even faster. To do that, deploy your application by using the following command: mvn clean package -Pnative -Dquarkus.native.native-image-xmx=4096m -Dquarkus.native.remote-container-build=true -Dquarkus.kubernetes.deploy=true &gt; IMPORTANT: -Dquarkus.native.native-image-xmx=4096m is the amount of memory &gt; Quarkus can use to generate the native image. You should adjust it or &gt; completely remove it depending on your local machine’s specifications. NOW YOU ARE READY TO RUN SERVERLESS APPLICATIONS USING JAVA Easy, isn’t it? Quarkus and Knative give you the freedom to run serverless applications using Java on-premises or in the cloud, no matter the vendor. You can even mix more than one cloud vendor with your on-premises infrastructure. This flexibility brings you agility and reduces your costs with infrastructure. NEXT STEP If you want to go further on serverless with more exciting stuff, check out  The post appeared first on .</content><dc:creator>Helber Belmiro</dc:creator></entry><entry><title>Learn about the new BGP capabilities in Red Hat OpenStack 17</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/22/learn-about-new-bgp-capabilities-red-hat-openstack-17" /><author><name>Daniel Alvarez Sanchez</name></author><id>d3766211-f376-45f2-b86d-2b3cbe44900a</id><updated>2022-09-22T07:00:00Z</updated><published>2022-09-22T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://www.redhat.com/en/technologies/linux-platforms/openstack-platform"&gt;Red Hat OpenStack Platform&lt;/a&gt; is an Infrastructure-as-a-Service (IaaS) offering from Red Hat. Version 17.0 of the platform includes dynamic routing for both the control and data planes. This lets you deploy a cluster in a pure layer-3 (L3) data center, overcoming the scaling issues of traditional layer-2 (L2) infrastructures such as large failure domains, large broadcast traffic, or long convergence times in the event of failures.&lt;/p&gt; &lt;p&gt;This article will illustrate this new feature by outlining a simple three-rack spine and leaf topology, where the layer-2 boundaries are within each rack on the Red Hat OpenStack Platform. The control plane spans the three racks, and each rack also hosts a compute node. Figure 1 illustrates our topology.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_12.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_12.png?itok=5G4F2AYN" width="600" height="285" alt="Diagram showing two leaf nodes connecting each control node to the spines." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Two leaf nodes connect each control node to the spines. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;The main characteristics of this deployment are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Border Gateway Protocol (BGP) is running on every element in the network: controllers, computes, leaves, and spine. The Red Hat OpenStack Platform uses &lt;a href="https://frrouting.org/"&gt;FRRouting&lt;/a&gt; (FRR) to enable BGP in the overcloud nodes, and it operates here as follows: &lt;ul&gt; &lt;li&gt;Leaves are configured as route reflectors, re-advertising learned routes to the spine.&lt;/li&gt; &lt;li&gt;The IPv6 link-local address of each interface uses &lt;em&gt;BGP Unnumbered&lt;/em&gt; to establish BGP sessions. There is no need to assign and configure unique IP addresses on these interfaces, simplifying the deployment.&lt;/li&gt; &lt;li&gt;FRR advertises all local IP addresses (that is, /32 on IPv4 or /128 on IPv6) as directly connected host routes.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Each device has outgoing default &lt;a href="https://study-ccna.com/ecmp-equal-cost-multi-path/"&gt;equal-cost multi-path routing&lt;/a&gt; (ECMP) routes for load balancing and high availability (no L2 bonds).&lt;/li&gt; &lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/rfc5880/"&gt;Bidirectional Forwarding Detection&lt;/a&gt; (BFD), which is &lt;a href="https://opendev.org/openstack/tripleo-ansible/src/commit/7da489819193352f009949f10fe988809a607ab7/tripleo_ansible/roles/tripleo_frr/defaults/main.yml#L23-L32"&gt;configurable&lt;/a&gt;, is used for network failure detection for fast convergence times.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.hpc.cam.ac.uk/cloud/userguide/02-neutron.html"&gt;OpenStack Neutron&lt;/a&gt; and &lt;a href="https://www.ovn.org/en/"&gt;Open Virtual Network&lt;/a&gt; (OVN) are agnostic and require no changes or configuration. &lt;ul&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Constraints and limitations&lt;/h3&gt; &lt;p&gt;Before we move on, it's worth noting the constraints and limitations of the implementation shown in this article:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This feature will only work with the Neutron &lt;a href="https://docs.openstack.org/neutron/latest/admin/config-ml2.html"&gt;ML2/OVN&lt;/a&gt; mechanism driver.&lt;/li&gt; &lt;li&gt;Workloads in provider networks and floating IP addresses are advertised. Routes to these workloads go directly to the compute node hosting the virtual machine (VM).&lt;/li&gt; &lt;li&gt;Tenant networks can &lt;a href="https://opendev.org/openstack/tripleo-ansible/src/commit/2381a7c3b246713744ab259ea8ac22be826344cb/tripleo_ansible/roles/tripleo_frr/defaults/main.yml#L69"&gt;optionally be advertised&lt;/a&gt;, but: &lt;ul&gt; &lt;li&gt;Overlapping CIDRs are not supported. Tenants need to ensure uniqueness (e.g., through the use of &lt;a href="https://docs.openstack.org/neutron/wallaby/admin/config-address-scopes.html"&gt;address scopes&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Traffic to workloads in tenant networks traverses the gateway node.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;An &lt;a href="https://opendev.org/x/ovn-bgp-agent"&gt;agent&lt;/a&gt; is required to run on each overcloud node. This agent is responsible for steering the traffic to or from the OVN overlay, as well as triggering FRR to advertise the IPv4 or IPv6 addresses of the workloads.&lt;/li&gt; &lt;li&gt;The provider bridge (typically &lt;code&gt;br-ex&lt;/code&gt; or &lt;code&gt;br-provider&lt;/code&gt;) is not connected to a physical NIC or bond. Instead, egress traffic from the local VMs is processed by an extra routing layer in the Linux kernel. Similarly, ingress traffic is processed by this extra routing layer and forwarded to OVN through the provider bridge.&lt;/li&gt; &lt;li&gt;There is no support for datapath acceleration, because the agent relies on kernel networking to steer the traffic between the NICs and OVN. Acceleration mechanisms such as &lt;a href="https://docs.openvswitch.org/en/latest/intro/install/dpdk/"&gt;Open vSwitch with DPDK&lt;/a&gt; or &lt;a href="https://docs.openstack.org/neutron/rocky/admin/config-ovs-offload.html"&gt;OVS hardware offloading&lt;/a&gt; are not supported. Similarly, &lt;a href="https://www.networkworld.com/article/3535850/what-is-sr-iov-and-why-is-it-the-gold-standard-for-gpu-sharing.html"&gt;SR-IOV&lt;/a&gt; is not compatible with this configuration because it skips the hypervisor.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Control plane&lt;/h2&gt; &lt;p&gt;With this configuration, the control plane no longer has to be in the same L3 network as the endpoints, because endpoints are advertised via BGP and traffic is &lt;em&gt;routed&lt;/em&gt; to the nodes hosting the services.&lt;/p&gt; &lt;p&gt;&lt;em&gt;High availability&lt;/em&gt; (HA) is provided fairly simply. Instead of announcing the VIP location upon failover by sending broadcast GARPs to the upstream switch, &lt;a href="https://clusterlabs.org/pacemaker/doc/2.1/Pacemaker_Explained/singlehtml/"&gt;Pacemaker&lt;/a&gt; just configures the VIP addresses in the loopback interface, which triggers FRR to advertise a directly connected host route to it.&lt;/p&gt; &lt;h3&gt;Sample traffic route&lt;/h3&gt; &lt;p&gt;Let's take the example of the control plane's &lt;a href="http://www.haproxy.org"&gt;HAproxy &lt;/a&gt;endpoint and check its Pacemaker configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@ctrl-1-0 ~]# pcs constraint colocation config Colocation Constraints: ip-172.31.0.1 with haproxy-bundle (score:INFINITY) [root@ctrl-1-0 ~]# pcs resource config ip-172.31.0.1 Resource: ip-172.31.0.1 (class=ocf provider=heartbeat type=IPaddr2) Attributes: cidr_netmask=32 ip=172.31.0.1 nic=lo Meta Attrs: resource-stickiness=INFINITY Operations: monitor interval=10s timeout=20s (ip-172.31.0.1-monitor-interval-10s) start interval=0s timeout=20s (ip-172.31.0.1-start-interval-0s) stop interval=0s timeout=20s (ip-172.31.0.1-stop-interval-0s) [root@ctrl-1-0 ~]# ip addr show lo 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet 172.31.0.1/32 scope global lo valid_lft forever preferred_lft forever ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After Pacemaker configures the VIP in one of the nodes, it configures this IP address in the &lt;code&gt;lo&lt;/code&gt; interface, triggering FRR to advertise a directly connected route on that node:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@ctrl-1-0 ~]# podman exec -it frr vtysh -c "show ip bgp" | grep 172.31.0.1 *&gt; 172.31.0.1/32 0.0.0.0 0 0 32768 ?&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we can explore the route to this IP address, which is hosted by &lt;code&gt;ctrl-1-0&lt;/code&gt;, from the &lt;code&gt;leaf-2-1&lt;/code&gt; leaf node in &lt;code&gt;rack-2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# for i in leaf-2-1 spine-2 spine-1 leaf-1-1 leaf-1-2; do ssh $i ip route show 172.31.0.1; done Warning: Permanently added 'leaf-2-1' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 330 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:fefe:158a dev eth2 weight 1 nexthop via inet6 fe80::5054:ff:fe55:bdf dev eth1 weight 1 Warning: Permanently added 'spine-2' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 161 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:feb4:d2d0 dev eth3 weight 1 nexthop via inet6 fe80::5054:ff:fec5:7bad dev eth2 weight 1 Warning: Permanently added 'spine-1' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 439 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:fe6f:466b dev eth3 weight 1 nexthop via inet6 fe80::5054:ff:fe8d:c63b dev eth2 weight 1 Warning: Permanently added 'leaf-1-1' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 142 via 100.65.1.2 dev eth3 proto bgp metric 20 Warning: Permanently added 'leaf-1-2' (ECDSA) to the list of known hosts. 172.31.0.1 nhid 123 via 100.64.0.2 dev eth3 proto bgp metric 20&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Traffic directed to the OpenStack control plane VIP (172.31.0.1) from &lt;code&gt;leaf-2-1&lt;/code&gt; goes through either the &lt;code&gt;eth1&lt;/code&gt; (on &lt;code&gt;spine-1&lt;/code&gt;) or &lt;code&gt;eth2&lt;/code&gt; (on &lt;code&gt;spine-2&lt;/code&gt;) ECMP routes. The traffic continues from &lt;code&gt;spine-1&lt;/code&gt; on ECMP routes again to &lt;code&gt;leaf-1-1&lt;/code&gt; , or from &lt;code&gt;spine-2&lt;/code&gt; to &lt;code&gt;leaf1-2&lt;/code&gt;. Finally, the traffic goes through &lt;code&gt;eth3&lt;/code&gt; to the controller hosting the service, &lt;code&gt;ctrl-1-0&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;High availability through BFD&lt;/h3&gt; &lt;p&gt;As mentioned earlier, BFD is running in the network to detect network failures. In order to illustrate its operation, following the example in the previous section, let's take down the NIC in &lt;code&gt;leaf-1-1&lt;/code&gt; that connects to the controller node, and see how the routes adjust on the &lt;code&gt;spine-1&lt;/code&gt; node to go through the other leaf in the same rack.&lt;/p&gt; &lt;p&gt;Initially, there is an ECMP route in the &lt;code&gt;spine-1&lt;/code&gt; node to the VIP that sends the traffic to both leaves in rack 1:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@spine-1 ~]# ip route show 172.31.0.1 172.31.0.1 nhid 179 proto bgp metric 20 nexthop via inet6 fe80::5054:ff:fe6f:466b dev eth3 weight 1 nexthop via inet6 fe80::5054:ff:fe8d:c63b dev eth2 weight 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's bring down the interface that connects &lt;code&gt;leaf-1-1&lt;/code&gt; to &lt;code&gt;ctrl-1-0&lt;/code&gt;, which is hosting the VIP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[root@leaf-1-1 ~]# ip link set eth3 down&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The BFD state changes to &lt;code&gt;down&lt;/code&gt; for this interface, and the route has been withdrawn in the spine, which now goes only through &lt;code&gt;leaf-1-2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; [root@leaf-1-1 ~]# tail -f /var/log/frr/frr.log | grep state-change 2022/09/08 12:14:47 BFD: [SEY1D-NT8EQ] state-change: [mhop:no peer:100.65.1.2 local:100.65.1.1 vrf:default ifname:eth3] up -&gt; down reason:control-expired [root@spine-1 ~]# ip route show 172.31.0.1 172.31.0.1 nhid 67 via inet6 fe80::5054:ff:fe6f:466b dev eth3 proto bgp metric 20&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Similarly, if we bring up the interface again, BFD will detect this condition and the ECMP route will be re-installed.&lt;/p&gt; &lt;p&gt;The newly introduced &lt;code&gt;frr&lt;/code&gt; container runs in all controller, network, and compute nodes. Its configuration can be queried through the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo podman exec -it frr vtysh -c 'show run'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Data plane&lt;/h2&gt; &lt;p&gt;The data plane refers here to the workloads running in the OpenStack cluster. This section describes the main pieces introduced in this configuration to allow VMs to communicate in a Layer-3 only datacenter.&lt;/p&gt; &lt;h3&gt;OVN BGP Agent&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://opendev.org/x/ovn-bgp-agent"&gt;OVN BGP Agent&lt;/a&gt; is a &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;-based daemon that runs on every compute and network node. This agent connects to the OVN southbound database and keeps track of when a workload is spawned or shut down on a particular hypervisor. The agent then triggers FRR to advertise or withdraw its IP addresses, respectively. The agent is also responsible for configuring the extra routing layer between the provider bridge (&lt;code&gt;br-ex&lt;/code&gt; or &lt;code&gt;br-provider&lt;/code&gt;) and the physical NICs.&lt;/p&gt; &lt;h3&gt;BGP advertisement&lt;/h3&gt; &lt;p&gt;The same principle shown earlier for the control plane applies to the data plane. The difference is that for the control plane, Pacemaker configures the IP addresses to the loopback interface, whereas for the data plane, the OVN BGP Agent adds the addresses to a local &lt;a href="https://access.redhat.com/solutions/5855721"&gt;VRF&lt;/a&gt;. The VRF is used for isolation, because we don't want these IP addresses to interfere with the host routing table. We just want to trigger FRR to advertise and withdraw the addresses as appropriate (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_10.png?itok=fFZZXz67" width="600" height="479" alt="Diagram showing that the OVN BGP Agent controls FRR in order to advertise/withdraw routes." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The OVN BGP Agent controls FRR in order to advertise/withdraw routes. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;div&gt; &lt;/div&gt; &lt;h3&gt;Traffic routing&lt;/h3&gt; &lt;p&gt;As mentioned earlier, OVN has not been modified in any way to support this configuration. Thus, OVN believes that the L2 broadcast domain of the provider networks spans multiple hypervisors, but this is not true anymore. Both ingress and egress traffic require an extra layer of routing. The OVN BGP Agent is responsible for configuring this layer through the following actions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Enable an ARP/NDP proxy in the provider bridge. Requests don't hit the destination because there's no L2 connectivity, so they're answered locally by the kernel:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sysctl net.ipv4.conf.br-ex.proxy_arp net.ipv4.conf.br-ex.proxy_arp = 1 $ sysctl net.ipv6.conf.br-ex.proxy_ndp net.ipv6.conf.br-ex.proxy_ndp = 1&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;For ingress traffic, add host routes in the node to forward the traffic to the provider bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo ip rule show | grep br-ex 32000: from all to 172.24.100.217 lookup br-ex $ sudo ip route show table br-ex default dev br-ex scope link 172.24.100.217 dev br-ex scope link&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;For egress traffic, add flows that change the destination MAC address to that of the provider bridge, so that the kernel will forward the traffic using the default outgoing ECMP routes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ip link show br-ex 7: br-ex: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 3e:cc:28:d7:10:4e brd ff:ff:ff:ff:ff:ff $ sudo ovs-ofctl dump-flows br-ex cookie=0x3e7, duration=48.114s, table=0, n_packets=0, n_bytes=0, priority=900,ip,in_port="patch-provnet-b" actions=mod_dl_dst:3e:cc:28:d7:10:4e,NORMAL cookie=0x3e7, duration=48.091s, table=0, n_packets=0, n_bytes=0, priority=900,ipv6,in_port="patch-provnet-b" actions=mod_dl_dst:3e:cc:28:d7:10:4e,NORMAL cookie=0x0, duration=255892.138s, table=0, n_packets=6997, n_bytes=1368211, priority=0 actions=NORMAL $ ip route show default default nhid 34 proto bgp src 172.30.2.2 metric 20 nexthop via 100.64.0.5 dev eth1 weight 1 nexthop via 100.65.2.5 dev eth2 weight 1 &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This example is for a VM on a provider network and applies as well to Floating IP addresses. However, for workloads in tenant networks, host routes are advertised from network and compute nodes using the Neutron gateway IP address as the next hop. From the gateway node, the traffic reaches the destination compute node through the Geneve tunnel (L3) as usual.&lt;/p&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;p&gt;More information can be found at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://opendev.org/x/ovn-bgp-agent/src/commit/1fa471083c4fdbdac8d2781822c55eb7b8069fa2/doc/source/contributor/bgp_mode_design.rst"&gt;OVN BGP Agent upstream documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://ltomasbo.wordpress.com/2021/02/04/ovn-bgp-agent-in-depth-traffic-flow-inspection/"&gt;OVN BGP Agent: In-depth traffic flow inspection blogpost&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=eKH14UN856o"&gt;OpenInfra Summit Berlin '22 - Using BGP to interconnect workloads across clouds&lt;/a&gt; (video)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=91daVTMt9AA"&gt;Devconf 2021 - Layer 3 Networking with BGP in hyperscale DCx&lt;/a&gt; (video)&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/22/learn-about-new-bgp-capabilities-red-hat-openstack-17" title="Learn about the new BGP capabilities in Red Hat OpenStack 17"&gt;Learn about the new BGP capabilities in Red Hat OpenStack 17&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Alvarez Sanchez</dc:creator><dc:date>2022-09-22T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 22 September 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-09-22.html" /><category term="quarkus" /><category term="resteasy" /><category term="kie" /><category term="keycloak" /><category term="wildfly" /><author><name>Romain Pelisse</name><uri>https://www.jboss.org/people/romain-pelisse</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-09-22.html</id><updated>2022-09-22T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus,resteasy,kie,keycloak,wildfly"&gt; &lt;h1&gt;This Week in JBoss - 22 September 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Hi everyone and welcome to the latest installment of JBoss editorial! Today’s stars of the show: Quarkus and KIE (Kogito/Drools)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus"&gt;Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Quarkus is quite busy this month! Just yesterday, the project released &lt;a href="https://quarkus.io/blog/quarkus-2-12-3-final-released/"&gt;Quarkus 2.12.3.Final&lt;/a&gt;, the third round of bugfixes and performance enhance of for the 2.12, which we mentioned in our previous editorial. But that’s not all, Quarkus tooling also got some love with the release of &lt;a href="https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/"&gt;Quarkus Tools for IntelliJ 1.13.0 released!&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Beyond the publication of new software and bugfixes, James Cobb also took the time to publish the 24th installment of the &lt;a href="https://quarkus.io/newsletter/24/"&gt;Quarkus Newsletter&lt;/a&gt;, a must-read for anyone who wants to follow or play with Quarkus! And to this point, an interesting new player has joined the project’s community: &lt;a href="https://quarkus.io/blog/aphp-user-story/"&gt;Quarkus adoption by APHP (Assistance Publique des Hôpitaux de Paris)&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Of course, if you are already familiar with Quarkus, you may want something more technical to quench your thirst and Clément Escoffier has just the article for you: &lt;a href="https://quarkus.io/blog/redis-job-queue/"&gt;How to implement a job queue with Redis&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_kie"&gt;KIE&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;KIE community has been quite active too in the last days and produced quite an amount of interesting articles about their technology. First, we’ll suggest you’ll dive into this one about &lt;a href="https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html"&gt;Creating your first cloud-agnostic serverless application with Java&lt;/a&gt;. It’s a good place to start!&lt;/p&gt; &lt;p&gt;Another one, called &lt;a href="https://blog.kie.org/2022/09/new-visualizer-for-the-serverless-workflow-editor.html"&gt;New visualizer for the Serverless Workflow Editor&lt;/a&gt; provides a nice overview of this new tool and we’ll certainly learn more about it and use it. If you are more interested into technical details and implementation, you are in luck, there is a rather detailed overview of the &lt;a href="https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html"&gt;Efesto refactoring&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Wait, that’s not all! Check out this article, and the video it links to: &lt;a href="https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html"&gt;Transparent ML, integrating Drools with AIX360&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_techbytes"&gt;Techbytes&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;If KIE and Quarkus have been the most prolific of the last two weeks, there is still a few more articles, coming from other projects, that you may want to check out: * &lt;a href="http://www.mastertheboss.com/java/how-to-spot-java-bugs-with-spotbugs/"&gt;How to spot Java bugs with SpotBugs&lt;/a&gt; * &lt;a href="http://www.mastertheboss.com/jboss-frameworks/resteasy/getting-started-with-jakarta-restful-services/"&gt;Getting started with Jakarta RESTful Services&lt;/a&gt; * &lt;a href="https://infinispan.org/blog/2022/09/12/infinispan-14-console-wizard"&gt;Creating cache with wizard - Infinispan 14&lt;/a&gt; * &lt;a href="https://www.wildfly.org//news/2022/09/14/Remote-dev-watch/"&gt;Remote dev-watch development with WildFly Jar Maven Plugin&lt;/a&gt; * &lt;a href="https://blog.kie.org/2022/09/multiple-repositories-pull-request-chaos-crawl-them-all-in-one-single-place.html"&gt;Multiple repositories Pull Request chaos, crawl them all in one single place&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases…​&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;As always, the JBoss community has been quite active and a few projects published new version in the last two weeks:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-5-3-final-released/"&gt;Quarkus 2.12.2.Final released&lt;/a&gt; followed by &lt;a href="https://quarkus.io/blog/quarkus-2-12-3-final-released/"&gt;Quarkus 2.12.3.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/"&gt;Quarkus Tools for IntelliJ 1.13.0 released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.dev/2022/09/08/resteasy-6.2.0.Beta1-release/"&gt;RESTEasy 6.2.0.Beta1 Release&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/09/keycloak-1902-released"&gt;Keycloak 19.0.2 released&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_decaf"&gt;Decaf'&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Feeling too jittery? Enough Java for now? Get refreshed with these two next articles about &lt;strong&gt;regular expressions&lt;/strong&gt;:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/09/14/beginners-guide-regular-expressions-grep"&gt;A beginner’s guide to regular expressions with grep&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/09/16/regex-how-quantifiers-pattern-collections-and-word-boundaries"&gt;Regex how-to: Quantifiers, pattern collections, and word boundaries&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again next time for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/romain-pelisse.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Romain Pelisse&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Romain Pelisse</dc:creator></entry><entry><title type="html">Efesto refactoring &amp;#8211; technical details</title><link rel="alternate" href="https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html</id><updated>2022-09-21T11:09:56Z</updated><content type="html">This post is meant as a description of the APIs and other technical details of the Efesto framework. It continues the introduction made in the BASE CONCEPTS. There are some concepts around which the APIs are implemented: * Generated resource * Unique identifier * Context of execution The framework provides and manage default implementations of the classes representing those concepts. Those classes could be extended by different engines for their specific needs (e.g. the Kie-drl compilation plugin define a context that contains a KnowledgeBuilderConfiguration) but this specific addition should never leak out of the engine itself, and the functionality of the framework itself should never rely on such "custom" details. GENERATED RESOURCE A represent the result of a compilation. By itself is just a marker interface because there are different kind of generated resources: * executable resources () * redirect resources () * “container” resources (like ). Executable resources represents the "entry point" for execution at runtime, and it contains information required to "instantiate" the executable unit. For some code-generation models (e.g. rules, predictions) this means store the class to instantiate at runtime, that will be used to start the evaluation. For models that does not rely on code-generation for execution (e.g. decisions), this resource contains the name of the class to be instantiated and/or the methods/parameters to be invoked. Redirect resources contains information needed to forward the execution request to a different engine, and it contains the informatio about the ewngine to be invoked. Container resources are meant to store other informations needed at runtime (e.g. the classes generated during compilation). UNIQUE IDENTIFIER The unique identifier () contains the information required to uniquely identify an executable or redirect generated resource. ModelLocalUriId contains information about: * the model/engine to invoke * the full path to the given resource The unique identifier is represented a "path" whose root is the model/engine to invoke, and the path describe all the elements required to get to the specific resource. Stateless engines (e.g. DMN, PMML) describe that as "/namespace/model_name" or "/filename/model_name". Statefull engines would require further path compoenents to identify the specific "state" to be invoked (e.g. "/drl/rule_base/session_name/session_identifier"). ModelLocalUriId is a property of both GeneratedExecutableResource and GeneratedRedirectResource, since both of them have to be retrieved during runtime execution. ModelLocalUriId implements and is a feature that was initially implemented in the Kogito Incubation API, for which an explanation is available . For each module, client code should be able to invoke a method like that to retrieve the unique identifier: ModelLocalUriId modelLocalUriId = appRoot("") .get(PmmlIdFactory.class) .get(fileNameNoSuffix, modelName); This is a fluent API, and each get invocation corresponds to an element in the generated path. The appRoot parameter is only used to differentiate multiple applications (e.g. in distributed context). The first get is needed to start the path building. Each module should implement its own factory extending , that, in turn, will be used to generate the full path. Each of the following get should return an object that extends ModelLocalUriId, since each it represent the path until that specific segment. Each module may provide its own strategy to define such paths, so each module may implement its own subclasses, depending on the needs. Since the The ModelLocalUriId constructor requires a instance, any of its subclasses should implement a way to call that constructor with such instance. In the following example: public class PmmlIdFactory implements ComponentRoot { public LocalComponentIdPmml get(String fileName, String name) { return new LocalComponentIdPmml(fileName, name); } } the PmmlIdFactory expose a get method ( the fluent API) that requires fileName and name parameters. This, in turns, are used to invoke the LocalComponentIdPmml constructor. public class LocalComponentIdPmml extends ModelLocalUriId { public static final String PREFIX = "pmml"; public LocalComponentIdPmml(String fileName, String name) { super(LocalUri.Root.append(PREFIX).append(fileName).append(name)); } } This snippet: LocalUri.Root.append(PREFIX).append(fileName).append(name) will lead to the creation of the following path: /{PREFIX}/{fileName}/{name} CONTEXT OF EXECUTION. The contains basic information about the current execution. It contains informations about the generated classes and the unique identifiers generated during compilation. is the specialization used at runtime to retrieve the generated classes. is the default implementation. Engines may extends the above as per their needs. For example, (the EfestoCompilationContext used inside the rule engine) defines KnowledgeBuilderConfiguration for its needs. COMPILATION CONTEXT is the specialization used at compile time, and it is used to store the classes generated during compilation. is the default implementation. provide a static method to retrieve the default implementation () with all the classes eventually compiled from a previous compilation. That static method, behind the scenes, invokes the constructor that scan the classloader to look for efesto-related classes. RUNTIME CONTEXT is the specialization used at runtime to retrieve the generated classes. is the default implementation. provide a static method to retrieve the default implementation () with all the efesto-related compiled classes. That static method, behind the scenes, invokes the constructor that scan the classloader to look for efesto-related classes. PUBLIC APIS The framework consists basically of two set of APIs, the "compilation" and the "runtime" ones. Those APIs are defined inside and . Those are the APIs that "client code" is expected to invoke. Said differently, "client code" is expected to interact with engines only through those APIs. COMPILATION API void processResource(EfestoCompilationContext context, EfestoResource... toProcess); This is the method that "External applications" (e.g. kie-maven-plugin) should invoke to create executables units out of given models. is the DTO wrapping a single model to be processed. Its only method T getContent(); is invoked by the compilation manager to get the object to be processed. The more common usage is to provide an actual File to the compilation manager, in which case there already is an implementation, . is a specific abstract implementations that wraps a Set of models. As for the previous, there already exist an implementation to manage FIles, . RUNTIME API Collection&lt;EfestoOutput&gt; evaluateInput(EfestoRuntimeContext context, EfestoInput... toEvaluate); This is the method that "External applications" (e.g. kogito execution) should invoke to retrieve a result out of executable units generated at compile-time. is the DTO wrapping a the data to be evaluated and the unique identifier of the executable units. It has two methods: ModelLocalUriId getModelLocalUriId(); T getInputData(); the former returns the unique identifier of the executable units; the latter returns the data to use for evaluation. Currently there are no "default" implementations of it, since the input structure is generally model-specific; so, every plugin should provide its own implementation. INTERNAL APIS Behind the scenes, when CompilationManager and RuntimeManager receives a request, they scan the classloader for engine plugins. Such plugins should implement, respectively, the and the . COMPILERSERVICE API declares three methods: boolean canManageResource(EfestoResource toProcess); List&lt;E&gt; processResource(EfestoResource toProcess, U context); String getModelType(); The first one is invoked by the CompilationManager to verify if the specific implementation is able to manage the given resource. The evaluation could be based on the actual type of the resource, on some details of the content, or on a mix of them. It is responsibility of the implementation to find the appropriate logic. The only requirement to keep in mind is that, during execution, there should be at most one implementation that return true for a given EfestoResource, otherwise an exception is thrown. The following snippet is an example where a given EfestoResource is considered valid if it is an DecisionTableFileSetResource: @Override public boolean canManageResource(EfestoResource toProcess) { return toProcess instanceof DecisionTableFileSetResource; } The above implementation works because DecisionTableFileSetResource is a class specifically defined by the plugin itself, so there are no possible "overlaps" with other implementations. On the other side, the following snippet is an example where a given EfestoResource is considered valid if it is an EfestoFileResource and if the contained model is a PMML: @Override public boolean canManageResource(EfestoResource toProcess) { return toProcess instanceof EfestoFileResource &amp;amp;&amp; ((EfestoFileResource) toProcess).getModelType().equalsIgnoreCase(PMML_STRING); } In this case, the actual class of EfestoResource is not enough, since EfestoFileResource is one of the default implementations provided by the framework. So, a further check is needed, that is about the model that is wrapped in the resource. A single plugin may manage multiple representations of the same model. For example, a plugin may manage both an EfestoFileResource and an EfestoInputStreamResource. There are different possible strategies to do that. For example, the plugin may provide one single "compilation-module" with two classes implementing the KieCompilerService; or it may define two "compilation-modules", each of which with one implementation, or one single class may manage both kind of inputs. Again, this is responsibility of the plugin itself. This also push toward code reusage. For a given model, there could be a common path that provide the final compilation output, and different entry point depending on the model representation. It is so possible that multiple compilation models creates a compilation output that, in turns, it is also an EfestoResource. Then, there could be another implementation that accept as input the above intermediate resuorce, and transform it to the final compilation outpout. This chaining is managed by the efesto framework out of the box. An example of that is featured by drools-related pmml models. During compilation, the PMML compiler generates an that is both an EfestoResource and an EfestoCompilationOutput. When the CompilationManager retrieves that compilation output, being it an EfestoResource, scans the plugins to find someone that is able to compile it. The fullfill this requirement, and proceed with drl-specific compilation. One thing to notice here is that different modules should limit as much as possible direct dependency between them. The second method is invoked by the compilation manager if the previous one returned true. That method receives also an EfestoCompilationContext as parameter. Code-generating implementations should rely on that context for compilation and classloading. The third method is used by the framework to discover, at execution time, which models can actually be managed. Thanks to that method, there is a complete de-coupling between the framework and the implementation themselves, since the framework can discover dynamically the available models, and every plugin may freely define its own model. Last critical bit is that every compilation module should contain an org.kie.efesto.compilationmanager.api.service.KieCompilerService file inside src/main/resources/META-INF directory, and that file should contain all the KieCompilationService implementations provided by that module. RUNTIMESERVICE API declares three methods: boolean canManageInput(EfestoInput toEvaluate, K context); Optional&lt;E&gt; evaluateInput(T toEvaluate, K context); String getModelType(); The first one is invoked by the RuntimeManager to verify if the specific implementation is able to manage the given input. The evaluation could be based on the actual type of the resource, on some details of the content, or on a mix of them. It is responsibility of the implementation to find the appropriate logic. The only requirement to keep in mind is that, during execution, there should be at most one implementation that return true for a given EfestoInput, otherwise an exception is thrown. The following snippet is an example where a given EfestoInput is considered valid if it is an EfestoInputPMML and the given identifier has already been compiled: public static boolean canManage(EfestoInput toEvaluate, EfestoRuntimeContext runtimeContext) { return (toEvaluate instanceof EfestoInputPMML) &amp;amp;&amp; isPresentExecutableOrRedirect(toEvaluate.getModelLocalUriId(), runtimeContext); } The above implementation works because EfestoInputPMML is a class specifically defined by the plugin itself, so there are no possible "overlaps" with other implementations. The difference with the compilation side is that the KieRuntimeService implementation should also check that the model related to the given unique identifier has already been compiled. A single plugin may manage different types of input for the same model. For example, the rule plugin may manage both an EfestoInputDrlKieSessionLocal and an AbstractEfestoInput that contains an EfestoMapInputDTO. There are different possible strategies to do that. For example, the plugin may provide one single "runtime-module" with two classes implementing the KieRuntimeService; or it may define two "runtime-modules", each of which with one implementation, or one single class may manage both kind of inputs. Again, this is responsibility of the plugin itself. This also push toward code reusage. For a given model, there could be a common code-path that provides the final runtime result, and different entry point depending on the input format. It is so possible that a runtime implementation would need a result from another implementation. In that case, the calling runtime will create a specifically-crafted EfestoInput and will ask the RuntimeManage the result for it. This chaining is managed by the efesto framework out of the box. An example of that is featured by drools-related pmml models. During execution, the PMML runtime generates an EfestoInput&lt;EfestoMapInputDTO&gt; and send it to the RuntimeManager. The RuntimeManager scans the plugins to find someone that is able to execute it. The fullfill this requirement, and proceed with drl-specific execution. One thing to note here is thet modules should limit as much as possible direct dependency between them! The second method is invoked by the runtime manager if the previous one returned true. That method receives also an EfestoRuntimeContext as parameter. Code-generating implementations should rely on that context to retrieve/load classes generated during compilation. The third method is used by the framework to discover, at execution time, which models can actually be managed. Thanks to that method, there is a complete de-coupling between the framework and the implementation themselves, since the framework can discover dynamically the available models, and every plugin may freely define its own model. Last critical bit is that every compilation module should contain an org.kie.efesto.runtimemanager.api.service.KieRuntimeService file inside src/main/resources/META-INF directory, and that file should contain all the KieRuntimeService implementations provided by that module. CONCLUSION This post was meant to provide more technical details on what have been introduced in the . Following ones will provide concrete step-by-step tutorial and real uses-cases so… stay tuned!!! The post appeared first on .</content><dc:creator>Gabriele Cardosi</dc:creator></entry></feed>
