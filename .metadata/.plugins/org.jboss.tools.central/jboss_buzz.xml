<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Transparent ML, integrating Drools with AIX360</title><link rel="alternate" href="https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html</id><updated>2022-09-20T12:10:33Z</updated><content type="html">Following up from about integrating Drools with the Open Prediction Service, in this new post we want to share the current results from another exploration work: this time integrating Drools with research on Transparent Machine Learning by IBM. INTRODUCTION Transparency is a key requirement in many business sectors, from FSI (Financial Services Industry), to Healthcare, to Government institutions, and many others. In more recent years, a generalized need for increased transparency in the decision making processes has gained a great deal of attention from several different stakeholders, especially when it comes to automated decisioning and AI-based decision services. Specifically in the Eurozone, this ties with the and the requirement for explainability in the way businesses automate processes and decision making. Additionally, an “” is proposed and currently under discussion at the European Commission: under the current status of the proposal several risk levels are identified. The integration of AI in the business process and decision model will likely require explainability, transparency and a conformity assessment, depending on the applicable risk level: In other parts of the world, similar legislations are coming into effect or are currently being proposed. You can read more details in . With these considerations in mind, we will explore how to leverage rule induction strategies and specific types of machine learning models, with the intent of producing predictive models which can integrate with effective results into this general context. TRANSPARENT ML WITH DROOLS AND AIX360 One way to address some of the problems and requirements highlighted in the previous section is to use Machine Learning to generate specific types of models that are inherently readable and transparent. As we will see in this blog post, a transparent predictive model can be handed over easily to the next phase as a decision model, in order to be evaluated as-is, but most importantly for the ability to be inspected and authored directly! Comparing a Transparent ML approach with the broader general Machine Learning, we can highlight some of its characteristics: General Machine Learning evaluation:Transparent ML approach:All supported model types, but black box evaluationModel can be inspected, authored, evaluatedAccuracy focusedTransparency focusedeXplainable AI complements, such as Intrinsically eXplainableMLOps —governed by data scienceBusiness centric governanceMultiple runtimesPotentially single runtime Naturally the transparent ML approach has its limitations; we will discuss alternative approaches in the conclusions of this blog post. An example pipeline can be summarized as follows: For the examples in this blog post, we will use the dataset  (predicting if income exceeds $50K/yr from census data). Let’s get started! RULE SET INDUCTION In this section we will make use of the , an open-source library that supports interpretability and explainability of datasets and machine learning models. Our goal in this phase is to generate a predictive model from the UCI Adult dataset, using Machine Learning techniques: To generate a transparent predictive model, we can drive the generation of a RuleSet , as explained in the following Jupyter notebook : As a result of this, we have now generated a set of rules, in the form of a PMML RuleSet, which represents the transparent predictive model for the Adult dataset: If you are interested to delve into more details about using AIX360 and related algorithms, you can check out . DROOLS In this section, we will transform the result from the previous steps into an executable decision model, which can also be directly authored. Please note: in a different context, where the only requirement is the execution of predictive models in general, you can simply make reference to the PMML support for Drools from the , or to integration blueprints such as the integration of Drools with IBM Open Prediction Service from a . In this article instead, as premised, we’re interested in the result of a transparent prediction model, which can be fully inspected, authored and (naturally!) evaluated. Specifically, we will transform the transparent predictive model serialized as a RuleSet, into a DMN model with DMN Decision Tables. To perform this transformation, we will make use of the kie-dmn-ruleset2dmn utility; this is available as a developer API, and as a command line utility too. You can download a published version of the command line utility (executable .jar) from ; otherwise, you can lookup a more recent version directly from . To transform the RuleSet file into a DMN model, you can issue the following command: $ java -jar kie-dmn-ruleset2dmn-cli-8.27.0.Beta.jar adult.pmml --output=adult.dmn This will result in a .dmn file generated, which you can author with the Kogito Tooling and evaluate as usual with the ! We can upload the generated .dmn file onto the sandbox: We can make use of the Kie Sandbox extended services, to evaluate locally the DMN model, as-is or authored as needed! It’s interesting to note the static analysis of the DMN decision table identifies potential gaps in the table, and subsumptions in the rules inducted during the Machine Learning phase; this is expected, and can be authored directly depending on the overall business requirements. From the model evaluation perspective, overlapping rules are not a problem, as they would evaluate to the same prediction; this is a quite common scenario when the ML might have identified overlapping “clusters” or grouping over a number of features, leading to the same output. From a decision table perspective however, overlapping rules can be simplified, as a more compact representation of the same table semantic is often preferable in decision management. Here it is up to the business to decide if to keep the table as translated from the original predictive model, or to leverage the possibilities offered by the transparent ML approach, and simplify/compact the table for easier read and maintenance by the business analyst. DEPLOY We can deploy directly from the KIE Sandbox: Our Transparent prediction and decision model is available as a deployment on OpenShift ! As you can see, with just the click of a button in the KIE Sandbox, our transparent ML model has been easily deployed on OpenShift. If you want to leverage the serverless capabilities of Knative for auto-scaling (including auto scale to zero!) for the same predictive model, you can consider packaging it as a Kogito application. You can find more information in this . CONCLUSION We have seen how a Transparent ML approach can provide solutions to some of the business requirements and conformance needs to regulations such as GDPR or AI Act; we have seen how to drive rule induction by generating predictive models which are inherently transparent, can be authored directly as any other decision model, and can be deployed on a cloud-native OpenShift environment. In this post, we have focused ourselves on using directly upstream AIX360 and Drools. You can refer to the above diagram for commercial solutions by IBM and Red Hat that include these projects too, such as , , . If you are interested in additional capabilities for eXplainable AI solutions, check-out the ! The Transparent ML predictive model, now available as a decision service, can be integrated in other DMN models and other applications, as needed. For example, the transparent prediction on the Adult dataset (predicting if income exceeds $50K/yr) could become invocable as part of another decision service that decides on the applicability for the requests of issuing a certain type of credit card. Another possible integration could be to employ a transparent ML predictive model in the form of scorecards, inside a broader DMN model for segmentation; that is, first identify the applicable category/segment based on the input data, and then apply one of several score cards for the specific segment. Don’t miss on checking out the on related Transparent ML topics! Hope you have enjoyed this blog post, showcasing integration of several technologies to achieve a transparent ML solution! Questions? Feedback? Let us know with the comment section below! Special thanks for Greger Ottosson and Tibor Zimanyi for their help while crafting this content. The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>How hashing and cryptography made the internet possible</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/20/how-hashing-and-cryptography-made-internet-possible" /><author><name>Andy Oram</name></author><id>60312bd5-c40d-4f54-9a4e-fbce728d8518</id><updated>2022-09-20T07:00:00Z</updated><published>2022-09-20T07:00:00Z</published><summary type="html">&lt;p&gt;A lot of technologies, business choices, and public policies gave us the internet we have today—a tremendous boost to the spread of education, culture, and commerce, despite its well-documented flaws. But few people credit two deeply buried technologies for making the internet possible: hashing and cryptography.&lt;/p&gt; &lt;p&gt;If more people understood the role these technologies play, more money and expertise would go toward uncovering and repairing security flaws. For instance, we probably would have fixed the &lt;a href="http://heartbleed.com/"&gt;Heartbleed&lt;/a&gt; programming error much earlier and avoided widespread vulnerabilities in encrypted traffic.&lt;/p&gt; &lt;p&gt;This article briefly explains where hashing and cryptography come from, how they accomplish what they do, and their indelible effect on the modern internet.&lt;/p&gt; &lt;h2&gt;Hashing&lt;/h2&gt; &lt;p&gt;Hashing was &lt;a href="https://www.geeksforgeeks.org/importance-of-hashing/"&gt;invented in the 1950s&lt;/a&gt; at the world's pre-eminent computer firm of that era, IBM, by Hans Peter Luhn. What concerned him at the time was not security—how many computer scientists thought about that?—but saving disk space and memory, the most costly parts of computing back then.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;hash&lt;/em&gt; is a way of reducing each item of data to a small, nearly unique, semi-random string of bits. For instance, if you are storing people's names, you could turn each name into the numerical value of the characters and run a set of adds, multiplies, and shift instructions to produce a 16-bit value. If the hash is good, there will be very few names that produce the same 16-bit value—very few &lt;em&gt;collisions&lt;/em&gt;, as that situation is called.&lt;/p&gt; &lt;p&gt;Now suppose you want to index a database for faster searching. Instead of indexing the names directly, it's much simpler and more efficient to make the index out of 16-bit values. That was one of the original uses for hashes. But they turned out to have two properties that make them valuable for security: No one can produce the original value from the hash, and no one can substitute a different value that produces the same hash. (It is theoretically possible to do either of those things, but doing so would be computationally infeasible, so they're impossible in practice.)&lt;/p&gt; &lt;p&gt;Early Unix systems made use of this property to preserve password security. You created a password along with your user account and gave it to the computer, but the operating system never stored the password itself—it stored only a hash. Every time you entered your password after that, the operating system ran the hash function and let you log in if the resulting hash matched the one in the system. If the password file were snatched up by a malicious intruder, all they would get is a collection of useless hashes. (This clever use of hashes eventually turned out not to be secure enough, so it was replaced with &lt;em&gt;encryption,&lt;/em&gt; which we'll discuss in more detail in the next section of this article.)&lt;/p&gt; &lt;p&gt;Hashes are also good for ensuring that no one has tampered with a document or software program. Injecting malware into free software on popular repositories is not just a theoretical possibility—&lt;a href="https://github.blog/2022-05-26-npm-security-update-oauth-tokens/"&gt;it can actually happen&lt;/a&gt;. Therefore, every time a free software project releases code, the team runs it through a hash function. Every user who downloads the software can run it through the same function to make sure nobody has intercepted the code and inserted malware. If someone changed even one bit and ran the hash function, the resulting hash would be totally different.&lt;/p&gt; &lt;p&gt;Git is another of the myriad tools that use hashes to ensure the integrity of the repository, as well as to enable quick checks on changes to the repository. You can see a hash (a string of random characters) each time you issue a push or log command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;commit 2de089ad3f397e735a45dda3d52d51ca56d8f19a Author: Andy Oram &lt;andyo@example.com&gt; Date: Sat Sep 3 16:28:41 2022 -0400 New material related to commercialization of cryptography. commit f39e7c87873a22e3bb81884c8b0eeeea07fdab48 Author: Andy Oram &lt;andyo@example.com&gt; Date: Fri Sep 2 07:47:42 2022 -0400 Fixed typos. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hash functions can be broken, so &lt;a href="https://valerieaurora.org/hash.html"&gt;new ones are constantly being invented&lt;/a&gt; to replace the functions that are no longer safe.&lt;/p&gt; &lt;h2&gt;Cryptography&lt;/h2&gt; &lt;p&gt;Mathematically speaking, the goal of cryptography has always been to produce output where each bit or character has an equal chance of being another character. If someone intercepted a message and saw the string "xkowpvi," the "x" would have an equal chance of representing an A, a B, a C, and so on.&lt;/p&gt; &lt;p&gt;In digital terms, every bit in an encrypted message has a 50% chance of representing a 0 and a 50% chance of representing a 1.&lt;/p&gt; &lt;p&gt;This goal is related to hashing, and there is a lot of overlap between the fields. Security experts came up with several good ways to create encrypted messages that couldn't be broken—that is, where the decryption process would be computationally infeasible without knowing the secret key used to encrypt the message. But for a long time these methods suffered from an "initial exchange" problem: The person receiving the message needed to somehow also learn what that secret encryption key was, and learn it in a way that didn't reveal the key to anybody else. Whether you're a spy in World War II Berlin trying to communicate with your U.S. buddies, or a modern retail site trying to confirm a customer's credit card online, getting the shared secret securely is a headache.&lt;/p&gt; &lt;p&gt;The solution by now is fairly familiar. The solution creates a pair of keys, one of which you keep private and the other of which you can share freely. Like a hash, the public key is opaque, and no one can determine your private key from it. (The number of bits in the key has to be doubled every decade or so as computers get more powerful.) This solution is generally &lt;a href="https://cryptography.fandom.com/wiki/Diffie%E2%80%93Hellman_key_exchange"&gt;attributed to Whitfield Diffie, Martin Hellman, and Ralph Merkle&lt;/a&gt;, although a British intelligence agent thought of the solution earlier and kept it secret.&lt;/p&gt; &lt;p&gt;Diffie in particular was acutely conscious of social and political reasons for developing public key encryption. In the 1970s, I think that few people thought of doing online retail sales or services using encryption. It was considered a tool of spies and criminals—but also of political dissidents and muckraking journalists. These associations explain why the U.S. government tried to suppress it, or at least keep it from being exported, for decades.&lt;/p&gt; &lt;p&gt;Diffie is still quite active in the field. The most recent article I've seen with him listed as an author was published on July 18, 2022.&lt;/p&gt; &lt;p&gt;The linchpin of internet cryptography came shortly afterward with &lt;a href="https://www.telsy.com/rsa-encryption-cryptography-history-and-uses/"&gt;RSA encryption&lt;/a&gt;, invented by Ron Rivest, Adi Shamir, and Len Adleman. RSA encryption lets two parties communicate without previously exchanging keys, even public keys. (They were prevented from reaping much profit from this historic discovery because the U.S. government prevented the export of RSA technology during most of the life of their patent.)&lt;/p&gt; &lt;p&gt;A big problem in key exchange remains: If someone contacts you and says they are Andy Oram, proffering what they claim to be Andy Oram's public key, how do you know they're really me? The two main solutions (web of trust and certificate authorities) are beyond the scope of this article, and each has vulnerabilities and a lot of overhead. Nevertheless, the internet seems to work well enough with certificate authorities.&lt;/p&gt; &lt;h2&gt;The internet runs on hashes and cryptography&lt;/h2&gt; &lt;p&gt;The internet essentially consists of huge computer farms in data centers, to which administrators and other users have to log in. For many years, the universal way to log into another system was Telnet, now abandoned almost completely because it's insecure. If you use Telnet, someone down the hall can watch your password cross the local network and steal the password. Anyone else who can monitor the network could do the same.&lt;/p&gt; &lt;p&gt;Nowadays, all communication between users and remote computers goes over the secure shell protocol (SSH), which was invented &lt;a href="https://www.oreilly.com/library/view/ssh-the-secure/0596008953/ch01s05.html"&gt;as recently as 1995&lt;/a&gt;. All the cloud computing and other data center administration done nowadays depend on it.&lt;/p&gt; &lt;p&gt;Interestingly, 1995 also saw the advent of the &lt;a href="https://www.techtarget.com/searchsecurity/definition/Secure-Sockets-Layer-SSL"&gt;secure sockets layer&lt;/a&gt; (SSL) protocol, which marks the beginning of web security. Now upgraded to Transport Layer Security (TLS), this protocol is used whenever you enter a URL beginning with HTTPS instead of HTTP. The protocol is so important that &lt;a href="https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html"&gt;Google penalizes web sites that use unencrypted HTTP&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because most APIs now use web protocols, TLS also protects distributed applications. In addition to SSH and TLS, encryption can be found everywhere modern computer systems or devices communicate. That's because the modern internet is beset with attackers, and we use hashes and encryption to minimize their harm.&lt;/p&gt; &lt;p&gt;Some observers think that quantum computing will soon have the power to break encryption as we know it. That could leave us in a scary world: Everything we send over the wire would be available to governments or large companies possessing quantum computers, which are hulking beasts that need to be refrigerated to within a few degrees of absolute zero. We may soon need a &lt;a href="https://nakedsecurity.sophos.com/2022/08/03/post-quantum-cryptography-new-algorithm-gone-in-60-minutes/"&gt;new army of Luhns, Diffies, and other security experts&lt;/a&gt; to find a way to save the internet as we know it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/20/how-hashing-and-cryptography-made-internet-possible" title="How hashing and cryptography made the internet possible"&gt;How hashing and cryptography made the internet possible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andy Oram</dc:creator><dc:date>2022-09-20T07:00:00Z</dc:date></entry><entry><title>Quarkus Tools for IntelliJ 1.13.0 released!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/&#xA;            " /><author><name>Jeff Maury (https://twitter.com/jeffmaury)</name></author><id>https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/</id><updated>2022-09-20T00:00:00Z</updated><published>2022-09-20T00:00:00Z</published><summary type="html">We are very pleased to announce the 1.13.0 release of Quarkus Tools for IntelliJ. This release improves the Qute developer experience. Improved Qute developer experience InlayHint support InlayHint is a new feature. It allows to add inline information about parameters and variables. This is very useful for Qute templates to...</summary><dc:creator>Jeff Maury (https://twitter.com/jeffmaury)</dc:creator><dc:date>2022-09-20T00:00:00Z</dc:date></entry><entry><title>Best ways to learn about Linux from Red Hat Developer</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/19/best-ways-learn-about-linux-red-hat-developer" /><author><name>Heiker Medina</name></author><id>02dd2697-1069-4f18-a29a-6798c4e87270</id><updated>2022-09-19T07:00:00Z</updated><published>2022-09-19T07:00:00Z</published><summary type="html">&lt;p&gt;Looking for tips and deep dives on &lt;a href="https://developers.redhat.com/topics/linux" target="_blank"&gt;Linux&lt;/a&gt;, including &lt;a href="https://developers.redhat.com/products/rhel/overview" target="_blank"&gt;Red Hat Enterprise Linux&lt;/a&gt;? Red Hat Developer has a wide range of content for you. These are some of our favorite and most popular articles, cheat sheets, and lessons to help you get the most out of Linux, along with underlying tools like GCC and Linux-based platforms like Docker:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/learn/lessons/linux-commands"&gt;Helpful Linux commands&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Don't know where to get started in RHEL? This interactive lesson schools you in a series of must-know commands.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/10/modular-perl-red-hat-enterprise-linux-8"&gt;Modular Perl in Red Hat Enterprise Linux 8&lt;/a&gt; (Author: Petr Pisar)&lt;/p&gt; &lt;p&gt;Red Hat Enterprise Linux comes with &lt;em&gt;modules,&lt;/em&gt; a packaging concept allowing system administrators to select the desired software from multiple packaged versions. Learn how to manage Perl as a module, as well as how to manage CPAN modules provided by Perl.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/cheat-sheets/intermediate-linux-cheat-sheet"&gt;Intermediate Linux Cheat Sheet&lt;/a&gt; (Author: Alex Soto Bueno and Bob Reselman)&lt;/p&gt; &lt;p&gt;Ready to level up your Linux knowledge? This cheat sheet presents a collection of Linux commands and executables for developers and system administrators who want to move beyond the basics. You'll find tips on managing processes, users, and groups on Linux, as well as monitoring disk and network usage.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/state-static-analysis-gcc-12-compiler"&gt;The state of static analysis in the GCC 12 compiler&lt;/a&gt; (Author: David Malcolm)&lt;/p&gt; &lt;p&gt;A new feature in the newest version of GCC will help programmers identify and fix two big potential problems: Variables that might have been set to the wrong values and objects that nobody has defined as ready to use.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/06/introduction-linux-bridging-commands-and-features"&gt;An introduction to Linux bridging commands and features&lt;/a&gt; (Author: Hangbin Liu)&lt;/p&gt; &lt;p&gt;How can you use Linux to make a computer work like a switch—that is, a device that connects different devices? A Linux &lt;em&gt;bridge&lt;/em&gt; allows machines to talk to each other even if they're on different internal networks. Learn how to use &lt;code&gt;bridge&lt;/code&gt; and &lt;code&gt;ip link&lt;/code&gt;, the two commands for handling bridges, to build and fix networks.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;Podman basics: Resources for beginners and experts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Podman is a tool for building containers that plays the same role as Docker and is mostly compatible with it. Developers getting started with Podman and those seeking more advanced information should take a deep dive into these resources.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/17/reduce-size-container-images-dockerslim"&gt;Reduce the size of container images with DockerSlim&lt;/a&gt; (Author: Karan Singh)&lt;/p&gt; &lt;p&gt;Using Docker to package your application code together with its dependencies creates a container image. The smaller the container image is, the faster your application will spin up for the first time. But many container images are just too large! This article explains how you can make them smaller so they can load more quickly.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/04/25/podman-basics-cheat-sheet"&gt;Podman basics cheat sheet&lt;/a&gt; (Author: Doug Tidwell)&lt;/p&gt; &lt;p&gt;With help from Podman, Buildah, and Skopeo, you can quickly create and manage rootless containers. This cheat sheet will help you master these tools.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/containers-without-docker"&gt;Containers without docker | DevNation Tech Talk&lt;/a&gt; (Presenter: Cedric Clyburn)&lt;/p&gt; &lt;p&gt;How do you create and run containers without Docker? Red Hat Developer Advocate Cedric Clyburn talks about life after Docker and why containers are here to stay.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/11/easier-way-generate-pdfs-html-templates"&gt;An easier way to generate PDFs from HTML templates&lt;/a&gt; (Author: Muhammad Edwin)&lt;/p&gt; &lt;p&gt;This article shows how to use an open source program, &lt;code&gt;wkhtmltopdf&lt;/code&gt;, to generate PDFs from HTML forms. The example uses Spring Boot, which simplifies Java application development, and Red Hat's &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Images&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/05/extracting-information-python-source-code"&gt;Extracting information from Python source code&lt;/a&gt; (Author: Fridolin Pokorny)&lt;/p&gt; &lt;p&gt;In Python, everything is a symbol—even words like "python" or "import." Invectio is a tool that lets you find all the symbols used or provided by a program. Using state-of-the-art static analysis and machine learning, Invectio can help you better understand software.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/learn/lessons/RHEL-open-lab"&gt;Red Hat Enterprise Linux open lab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explore any Linux topic you choose in this open lab environment designed without pre-planned content.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here are some of our other exciting and top-performing articles–have a look!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/16/code-specialization-mir-lightweight-jit-compiler"&gt;Code specialization for the MIR lightweight JIT compiler&lt;/a&gt; (Author: Vladimir Makarov)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/05/developers-guide-using-kafka-java-part-1"&gt;A developer's guide to using Kafka with Java, Part 1&lt;/a&gt; (Author: Bob Reselman)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/06/07/how-debug-stack-frames-and-recursion-gdb"&gt;How to debug stack frames and recursion in GDB&lt;/a&gt; (Author: Bruno Larsen)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/31/your-go-application-fips-compliant"&gt;Is your Go application FIPS compliant?&lt;/a&gt; (Authors: Antonio Cardace and Sam Fowler)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/23/how-install-command-line-tools-mac"&gt;How to install command-line tools on a Mac&lt;/a&gt; (Author: Varsha Sharma)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Visit our &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux topic page&lt;/a&gt; to discover the most relevant and recent articles on Linux at Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/19/best-ways-learn-about-linux-red-hat-developer" title="Best ways to learn about Linux from Red Hat Developer"&gt;Best ways to learn about Linux from Red Hat Developer&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Heiker Medina</dc:creator><dc:date>2022-09-19T07:00:00Z</dc:date></entry><entry><title>GCC's new fortification level: The gains and costs</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/17/gccs-new-fortification-level" /><author><name>Siddhesh Poyarekar</name></author><id>aa8105eb-3693-4988-8f01-b822ce7471ee</id><updated>2022-09-17T22:00:00Z</updated><published>2022-09-17T22:00:00Z</published><summary type="html">&lt;p&gt;This article describes a new level of fortification supported in GCC. This new level detects more buffer overflows and bugs which mitigates security issues in applications at run time.&lt;/p&gt; &lt;p&gt;C programs routinely suffer from memory management problems. For several years, a &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; preprocessor macro inserted error detection to address these problems at compile time and run time. To add an extra level of security, &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; has been in the GNU C Library (glibc) since version 2.34. I described its mechanisms in my previous blog post, &lt;a href="https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source"&gt;Broadening compiler checks for buffer overflows in _FORTIFY_SOURCE&lt;/a&gt;. There has been compiler support for this builtin in &lt;a href="https://clang.llvm.org"&gt;Clang&lt;/a&gt; for some time. Compiler support has also been available for &lt;a href="https://gcc.gnu.org"&gt;GCC&lt;/a&gt; since the release of version 12 in May 2022. The new mitigation should be available in GNU/Linux distributions with packaged GCC 12.&lt;/p&gt; &lt;p&gt;The following sections discuss two principal gains from this enhanced level of security mitigation and the resulting impact on applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2 principal gains:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Enhanced buffer size detection&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Better fortification coverage&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;1. A new builtin provides enhanced buffer size detection&lt;/h2&gt; &lt;p&gt;There is a new builtin underneath the new &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; macro n GCC 12 named &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt;. This builtin is more powerful than the previous &lt;code&gt;__builtin_object_size&lt;/code&gt; builtin used in &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;. When passed a pointer, &lt;code&gt;__builtin_object_size&lt;/code&gt;returns as a compile-time constant that is either the maximum or minimum object size estimate of the object that pointer may be pointing to at that point in the program. On the other hand, &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; is capable of returning a size expression that is evaluated at execution time. Consequently, the &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; builtin detects buffer overflows in many more places than &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The implementation of &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; in GCC is compatible with &lt;code&gt;__builtin_object_size&lt;/code&gt; and thereby interchangeable, especially in the case of fortification. Whenever possible, the builtin computes a precise object size expression. When the builtin does not determine the size exactly, it returns either a maximum or minimum size estimate, depending on the size type argument.&lt;/p&gt; &lt;p&gt;This code snippet demonstrates the key advantage of returning precise values:&lt;/p&gt; &lt;pre&gt;&lt;code class="cpp"&gt;#include &lt;string.h&gt; #include &lt;stdbool.h&gt; #include &lt;stdlib.h&gt; char *b; char buf1[21]; char *__attribute__ ((noinline)) do_set (bool cond) { char *buf = buf1; if (cond) buf = malloc (42); memset (buf, 0, 22); return buf; } int main (int argc, char **argv) { b = do_set (false); return 0; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The program runs to completion when built with &lt;code&gt;-D_FORTIFY_SOURCE=2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gcc -O -D_FORTIFY_SOURCE=2 -o sample sample.c &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But the program aborts when built with &lt;code&gt;-D_FORTIFY_SOURCE=3&lt;/code&gt; and outputs the following message:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;*** buffer overflow detected ***: terminated Aborted (core dumped) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The key enhancement stems from the difference in behavior between &lt;code&gt;__builtin_object_size&lt;/code&gt; and &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt;. &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; uses &lt;code&gt;__builtin_object_size&lt;/code&gt; and returns the maximum estimate for object size at pointer &lt;code&gt;buf&lt;/code&gt;, which is 42. Hence, GCC assumes that the &lt;code&gt;memset&lt;/code&gt; operation is safe at compile time and does not add a call to check the buffer size at run time.&lt;/p&gt; &lt;p&gt;However, GCC with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; invokes &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; to emit an expression that returns the precise size of the buffer that &lt;code&gt;buf&lt;/code&gt; points to at that part in the program. As a result, GCC realizes that the call to &lt;code&gt;memset&lt;/code&gt; might not be safe. Thus, the compiler inserts a call to &lt;code&gt;__memset_chk&lt;/code&gt; into the running code with that size expression as the bound for &lt;code&gt;buf&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;2. Better fortification coverage&lt;/h2&gt; &lt;p&gt;Building distribution packages with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; revealed several issues that &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; missed. Surprisingly, not all of these issues were straightforward buffer overflows. The improved fortification also encountered issues in the GNU C library (glibc) and raised interesting questions about object lifetimes.&lt;/p&gt; &lt;p&gt;Thus, the benefit of improved fortification coverage has implications beyond buffer overflow mitigation. I will explain the outcomes of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; increased coverage in the following sections.&lt;/p&gt; &lt;h3&gt;More trapped buffer overflows&lt;/h3&gt; &lt;p&gt;Building applications with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; detected many simple buffer overflows, such as the &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=2115476"&gt;off-by-one access in clisp&lt;/a&gt; issue. We expected these revelations, which strengthened our justification for building applications with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To further support the use of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; to improve fortification, we used the &lt;a href="https://github.com/siddhesh/fortify-metrics"&gt;Fortify metrics&lt;/a&gt; GCC plugin to estimate the number of times _FORTIFY_SOURCE=3 resulted in a call to a checking function (&lt;code&gt;__memcpy_chk&lt;/code&gt;, &lt;code&gt;__memset_chk&lt;/code&gt;, etc.). We used Fedora test distribution and some of the &lt;code&gt;Server&lt;/code&gt; package group as the sample, which consisted of 96 packages. The key metric is fortification coverage, defined by counting the number of calls to &lt;code&gt;__builtin_object_size&lt;/code&gt; that resulted in a successful size determination and the ratio of this number taken to the total number of &lt;code&gt;__builtin_object_size&lt;/code&gt; calls. The plugin also shows the number of successful calls if using &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; instead of &lt;code&gt;__builtin_object_size&lt;/code&gt;, allowing us to infer the fortification coverage if all &lt;code&gt;__builtin_object_size&lt;/code&gt; calls were replaced with &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In this short study, we found that &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; improved fortification by nearly 4 times. For example, the Bash shell went from roughly 3.4% coverage with &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; to nearly 47% with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;. This is an improvement of nearly 14 times. Also, fortification of programs in &lt;code&gt;sudo&lt;/code&gt; went from a measly 1.3% to 49.57% — a jump of almost 38 times!&lt;/p&gt; &lt;h3&gt;The discovery of bugs in glibc&lt;/h3&gt; &lt;p&gt;The increased coverage of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; revealed programming patterns in application programs that tripped over the fortification without necessarily a buffer overflow. While there were some bugs in glibc, we had to either explain why we did not support it or discover ways to discourage those programming patterns.&lt;/p&gt; &lt;p&gt;One example is &lt;code&gt;wcrtomb&lt;/code&gt;, where glibc makes stronger assumptions about the object size passed than POSIX allowed. Specifically, glibc assumes that the buffer passed to &lt;code&gt;wcrtomb&lt;/code&gt; is always at least &lt;code&gt;MB_CUR_MAX&lt;/code&gt; bytes long. In contrast, the POSIX description makes no such assumption. Due to this discrepancy, any application that passed a smaller buffer would potentially make &lt;code&gt;wcrtomb&lt;/code&gt; overflow the buffer during conversion. Then the fortified version &lt;code&gt;__wcrtomb_chk&lt;/code&gt; aborts with a buffer overflow, expecting a buffer that is &lt;code&gt;MB_CUR_MAX&lt;/code&gt; bytes long. We fixed this bug in glibc-2.36 by making glibc conform to POSIX .&lt;/p&gt; &lt;p&gt;&lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; revealed another pattern. Applications such as systemd used &lt;code&gt;malloc_usable_size&lt;/code&gt; to determine available space in objects and then used the residual space. The glibc manual discourages this type of usage, dictating that &lt;code&gt;malloc_usable_size&lt;/code&gt; is for diagnostic purposes only. But applications use the function as a hack to avoid reallocating buffers when there is space in the underlying malloc chunk. The implementation of &lt;code&gt;malloc_usable_size&lt;/code&gt; needs to be fixed to return the allocated object size instead of the chunk size in non-diagnostic use. Alternatively, another solution is to deprecate the function. But that is a topic for discussion by the glibc community.&lt;/p&gt; &lt;h3&gt;Strict C standards compliance&lt;/h3&gt; &lt;p&gt;One interesting use case exposed by &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; raised the question of object lifetimes and what developers can do with freed pointers. The bug in question was in &lt;a href="https://sourceforge.net/p/autogen/bugs/212/"&gt;AutoGen&lt;/a&gt;, using a pointer value after reallocation to determine whether the same chunk extended to get the new block of memory. This practice allowed the developer to skip copying over some pointers to optimize for performance. At the same time, the program continued using the same pointer, not the &lt;code&gt;realloc&lt;/code&gt; call result, since the old pointer did not change.&lt;/p&gt; &lt;p&gt;Seeing that the old pointer continued without an update, the compiler assumed that the object size remained the same. How could it know otherwise? The compiler then failed to account for the reallocation, resulting in an abort due to the perceived buffer overflow.&lt;/p&gt; &lt;p&gt;Strictly speaking, the C standards prohibit using a pointer to an object after its lifetime ends. It should neither be read nor dereferenced. In this context, it is a bug in the application.&lt;/p&gt; &lt;p&gt;However, this idiom is commonly used by developers to prevent making redundant copies. Future updates to &lt;a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105217"&gt;GCC&lt;/a&gt; may account for this idiom wherever possible, but applications should also explicitly indicate object lifetimes to remain compliant. In the AutoGen example, a simple fix is to unconditionally refresh the pointer after reallocation, ensuring the compiler can detect the new object size.&lt;/p&gt; &lt;h2&gt;The gains of improved security coverage outweigh the cost&lt;/h2&gt; &lt;p&gt;Building with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; may impact the size and performance of the code. Since &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; generated only constant sizes, its overhead was negligible. However, &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; may generate additional code to compute object sizes. These additions may also cause secondary effects, such as register pressure during code generation. Code size tends to increase the size of resultant binaries for the same reason.&lt;/p&gt; &lt;p&gt;We need a proper study of performance and code size to understand the magnitude of the impact created by &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; additional runtime code generation. However the performance and code size overhead may well be worth it due to the magnitude of improvement in security coverage.&lt;/p&gt; &lt;h2&gt;The future of buffer overflow detection&lt;/h2&gt; &lt;p&gt;&lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; has led to significant gains in security mitigation. GCC 12 support brings those gains to distribution builds. But the new level of fortification also revealed interesting issues that require additional work to support correctly. For more background information, check out my previous article, &lt;a href="https://www.redhat.com/en/blog/enhance-application-security-fortifysource"&gt;Enhance application security with FORTIFY_SOURCE&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Object size determination and fortification remain relevant areas for improvements in compiler toolchains. The toolchain team at Red Hat continues to be involved in the GNU and LLVM communities to make these improvements.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/17/gccs-new-fortification-level" title="GCC's new fortification level: The gains and costs"&gt;GCC's new fortification level: The gains and costs&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Siddhesh Poyarekar</dc:creator><dc:date>2022-09-17T22:00:00Z</dc:date></entry><entry><title>My advice for updating use of the Docker Hub OpenJDK image</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/16/updating-docker-hubs-openjdk-image" /><author><name>Tim Ellison</name></author><id>3bcb8704-1585-4386-8123-ee3bcc089043</id><updated>2022-09-16T18:00:00Z</updated><published>2022-09-16T18:00:00Z</published><summary type="html">&lt;p&gt;The Java runtime environment in your containers could stop receiving updates in the coming months. It's time to take action. This article explains the decisions that led to this issue and proposes a solution.&lt;/p&gt; &lt;h2&gt;OpenJDK and Java SE updates&lt;/h2&gt; &lt;p&gt;&lt;a href="https://openjdk.org/"&gt;OpenJDK&lt;/a&gt; is an open source implementation of the Java Platform, Standard Edition (Java SE), on which multiple companies and contributors collaborate.&lt;/p&gt; &lt;p&gt;A project at OpenJDK represents each new feature release of the Java SE specification. Subsequent updates to those features, including functional and security fixes, are led by maintainers working in the &lt;a href="https://openjdk.org/projects/jdk-updates/"&gt;JDK updates project&lt;/a&gt;. Long-term supported releases such as Java SE 8 (since March 2014), Java SE 11 (since Sept 2018), and Java SE 17 (since Sept 2021) undergo a quarterly release update under the guidance of a lead maintainer.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://openjdk.org/projects/jdk-updates/maintainers.html"&gt;repository maintainers' role&lt;/a&gt; is to ensure that updates are both necessary and appropriate for deployed releases. They consider the opinions of multiple contributors when making such update decisions. Many vendors and distributors of Java SE subsequently build from the OpenJDK source code to provide new releases of their own branded Java SE offerings.&lt;/p&gt; &lt;p&gt;Andrew Haley (Red Hat) is the lead maintainer for Java 8 updates and Java 11 updates, and Goetz Lindenmaier (SAP) is the lead maintainer for Java 17 updates. Update maintainers affiliated with companies that provide commercially supported distributions of OpenJDK based on Java SE work as independent contributors to the project.&lt;/p&gt; &lt;h2&gt;Docker Hub deprecates OpenJDK images&lt;/h2&gt; &lt;p&gt;For many years, the official &lt;a href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt; image builders took OpenJDK Java SE update binaries from &lt;a href="https://adoptium.net/"&gt;Eclipse Adoptium&lt;/a&gt; and other locations to build their own image. But in July 2022, the Docker Hub image builders &lt;a href="https://hub.docker.com/_/openjdk"&gt;announced the deprecation&lt;/a&gt; of this popular image.&lt;/p&gt; &lt;p&gt;Now, Docker asks users to obtain their builds of OpenJDK, either from a commercial Java vendor or directly from the Adoptium project. There will be no further updates to the existing OpenJDK image, so users risk falling behind with functional and security updates to their Java SE usage unless they move to an alternate provider. I believe the official &lt;a href="https://hub.docker.com/_/eclipse-temurin"&gt;Eclipse Temurin image&lt;/a&gt; maintained by the Adoptium project is the obvious choice for a replacement image.&lt;/p&gt; &lt;h2&gt;Eclipse Adoptium builds JDKs&lt;/h2&gt; &lt;p&gt;OpenJDK does not provide binary updates directly from the update projects. Since July 2022, these long-term supported Java update projects have depended upon &lt;a href="https://adoptium.net/"&gt;Eclipse Adoptium&lt;/a&gt; to build and distribute consumable OpenJDK binaries.&lt;/p&gt; &lt;p&gt;Adoptium is a project dedicated to building, testing, and distributing up-to-date and ready-to-use OpenJDK binaries under an open source license. Adoptium calls their builds of OpenJDK, Temurin. They are available across a broad range of processors and operating systems. These Temurin binaries have over half a billion downloads and earned the trust of enterprise production environments worldwide. A vendor-independent &lt;a href="https://adoptium.net/members"&gt;working group&lt;/a&gt; based at the Eclipse software foundation leads Adoptium.&lt;/p&gt; &lt;p&gt;The Adoptium community provides binaries built directly from OpenJDK source code. These Temurin binaries are available as direct downloads, installers, or container images and are faithful representations of the OpenJDK update source built under controlled conditions.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://hub.docker.com/_/eclipse-temurin"&gt;official Docker Hub Temurin images&lt;/a&gt; contain the latest releases of the OpenJDK updates for several Java SE versions, thoroughly tested with various applications. The images work as direct drop-in replacements for the OpenJDK images. Some OpenJDK images already contain Temurin binaries.&lt;/p&gt; &lt;h2&gt;How to move from OpenJDK images to Eclipse Temurin images&lt;/h2&gt; &lt;p&gt;The Docker Hub's deprecation decision presents a problem. But there is a solution. We recommend moving from the &lt;a href="https://hub.docker.com/_/openjdk"&gt;OpenJDK image&lt;/a&gt; to &lt;a href="https://hub.docker.com/_/eclipse-temurin"&gt;the official Docker Hub Eclipse Temurin image&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The process is simple. All you have to do is identify the &lt;code&gt;FROM&lt;/code&gt; lines in Dockerfiles such as this:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM: openjdk:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Change the lines as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM eclipse-temurin:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The process for changing the use of images other than version 17 is equivalent. You can &lt;a href="https://github.com/adoptium/adoptium-support/issues"&gt;report&lt;/a&gt; issues to the Adoptium community.&lt;/p&gt; &lt;h2&gt;Red Hat support&lt;/h2&gt; &lt;p&gt;We encourage everyone to switch to Eclipse Temurin. Many &lt;a href="https://github.com/jenkinsci/docker/pull/1429"&gt;application images&lt;/a&gt; and &lt;a href="https://github.com/javastacks/spring-boot-best-practice/blob/fc6709cf2ec2fc00b4dfae7210ce503f9c10560c/spring-boot-docker/Dockerfile"&gt;examples of best practices&lt;/a&gt; have successfully made the change.&lt;/p&gt; &lt;p&gt;Red Hat recently &lt;a href="https://developers.redhat.com/articles/2022/08/24/red-hat-expands-support-java-eclipse-temurin"&gt;announced direct support for Temurin&lt;/a&gt; in development and production as part of Red Hat Runtimes, Red Hat OpenShift, and Red Hat Build of OpenJDK. Red Hat support assures customers that the move to Temurin will be smooth, allowing you to continue focusing on building products that integrate and automate modern business applications and processes.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/16/updating-docker-hubs-openjdk-image" title="My advice for updating use of the Docker Hub OpenJDK image"&gt;My advice for updating use of the Docker Hub OpenJDK image&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tim Ellison</dc:creator><dc:date>2022-09-16T18:00:00Z</dc:date></entry><entry><title>Regex how-to: Quantifiers, pattern collections, and word boundaries</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/16/regex-how-quantifiers-pattern-collections-and-word-boundaries" /><author><name>Bob Reselman</name></author><id>2182a29a-626a-444f-a313-1e4a14d6eeb7</id><updated>2022-09-16T07:00:00Z</updated><published>2022-09-16T07:00:00Z</published><summary type="html">&lt;p&gt;Filtering and searching text with regular expressions is an important skill for every developer. Regular expressions can be tricky to master. To work with them effectively, you need a detailed understanding of their symbols and syntax.&lt;/p&gt; &lt;p&gt;Fortunately, learning to work with regular expressions can be incremental. You don't need to learn everything all at once to do useful work. Rather, you can start with the basics and then move into more complex topics while developing your understanding and using what you know as you go along.&lt;/p&gt; &lt;p&gt;This article is the second in a series. The &lt;a href="https://developers.redhat.com/articles/2022/08/03/beginners-guide-regular-expressions-grep"&gt;first article&lt;/a&gt; introduced some basic elements of regular expressions: The basic metacharacters (&lt;code&gt;.*^$\s\d&lt;/code&gt;) as well as the escape metacharacter &lt;code&gt;\&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This article introduces some more advanced syntax: quantifiers, pattern collections, groups, and word boundaries. If you haven't read the first article, you might want to review it now before continuing with this content.&lt;/p&gt; &lt;p&gt;These articles demonstrate regular expressions by piping string output from an &lt;a href="https://www.redhat.com/sysadmin/essential-linux-commands"&gt;&lt;code&gt;echo&lt;/code&gt;&lt;/a&gt; command to the &lt;a href="https://www.redhat.com/sysadmin/how-to-use-grep"&gt;&lt;code&gt;grep&lt;/code&gt;&lt;/a&gt; utility. The &lt;code&gt;grep&lt;/code&gt; utility uses a regular expression to filter content. The benefit of demonstrating regular expressions using &lt;code&gt;grep&lt;/code&gt; is that you don't need to set up any special programming environment. You can execute an example of a regular expression immediately by copying and pasting the code directly into your terminal window running under Linux.&lt;/p&gt; &lt;h2&gt;What's the difference between a regular character and a metacharacter&lt;/h2&gt; &lt;p&gt;A regular character is a letter, digit, or punctuation used in everyday text. When you declare a regular character in a regular expression, the regular expression engine searches content for that declared character. For example, were you to declare the regular character &lt;code&gt;h&lt;/code&gt; in a regular expression, the engine would look for occurrences of the character &lt;code&gt;h&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;A metacharacter is a placeholder symbol. For example, the metacharacter &lt;code&gt;.&lt;/code&gt; (dot) represents "any character," and means &lt;em&gt;any character matches here.&lt;/em&gt; The metacharacter &lt;code&gt;\d&lt;/code&gt; represents a numerical digit, and means &lt;em&gt;any digit matches here.&lt;/em&gt; Thus, when you use a metacharacter, the regex engine searches for characters that comply with the particular metacharacter or set of metacharacters.&lt;/p&gt; &lt;h2&gt;What are quantifiers?&lt;/h2&gt; &lt;p&gt;A quantifier is a syntactic structure in regular expressions that indicates the number of times a character occurs in sequence in the input text. There are two ways to declare a quantifier. One way is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;x{n}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this syntax:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the character to match.&lt;/li&gt; &lt;li&gt;&lt;code&gt;n&lt;/code&gt; indicates the number of times the character needs to occur.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A related syntax declares a quantifier with a minimum and maximum range:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;x{n,m}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this syntax:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the character to match.&lt;/li&gt; &lt;li&gt;&lt;code&gt;n&lt;/code&gt; indicates the minimum number of occurrences and &lt;code&gt;m&lt;/code&gt; indicates the maximum number of occurrences.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The following example uses a quantifier to create a matching pattern that identifies two occurrences of the regular character &lt;code&gt;g&lt;/code&gt; in sequence:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po 'g{2}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff and the pet Lucky. Gre&lt;strong&gt;gg&lt;/strong&gt; and the dog Fido. Chris has 1 bird named Tweety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Thus, the regular expression returns the following result:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;gg&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses a quantifier to create a matching pattern that identifies a minimum and a maximum for occurrences of the character &lt;code&gt;g&lt;/code&gt; in a sequence. The minimum length is 1 and the maximum is 2. The regular expression is processed in a case-insensitive manner, as indicated by the &lt;code&gt;-i&lt;/code&gt; option to &lt;code&gt;grep&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Poi 'g{1,2}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff and the pet Lucky. &lt;strong&gt;G&lt;/strong&gt;re&lt;strong&gt;gg&lt;/strong&gt; and the do&lt;strong&gt;g&lt;/strong&gt; Fido. Chris has 1 bird named Tweety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Because each sequence is identified and returned on a one-by-one basis, the output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;G gg g&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;What are pattern collections?&lt;/h2&gt; &lt;p&gt;A pattern collection is a syntactic structure that describes a &lt;a href="https://www.gnu.org/software/grep/manual/html_node/Character-Classes-and-Bracket-Expressions.html"&gt;character class&lt;/a&gt;. A character class is a set of metacharacters and regular characters that combine to create a matching pattern that, like a metacharacter, can match many different characters in text. A pattern collection is defined between square brackets (&lt;code&gt;[ ]&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;The following example uses the &lt;code&gt;[A-Z]&lt;/code&gt; character class, which denotes any uppercase character from &lt;code&gt;A&lt;/code&gt; to &lt;code&gt;Z&lt;/code&gt; inclusive, to create a pattern collection that matches only uppercase characters in the given text:&lt;/p&gt; &lt;p&gt;&lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '[A-Z]'&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;strong&gt;J&lt;/strong&gt;eff and the pet &lt;strong&gt;L&lt;/strong&gt;ucky. &lt;strong&gt;G&lt;/strong&gt;regg and the dog &lt;strong&gt;F&lt;/strong&gt;ido. &lt;strong&gt;C&lt;/strong&gt;hris has 1 bird named &lt;strong&gt;T&lt;/strong&gt;weety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;J L G F C T&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses the &lt;code&gt;[0-9]&lt;/code&gt; character class, which denotes any digit between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;9,&lt;/code&gt; to create a pattern collection that matches only numeric characters in the given text:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '[0-9]'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff and the pet Lucky. Gregg and the dog Fido. Chris has &lt;strong&gt;1&lt;/strong&gt; bird named Tweety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses a pattern collection that matches certain exact regular characters within a set of regular characters. The regular expression says: &lt;em&gt;Match any &lt;code&gt;f&lt;/code&gt;, &lt;code&gt;G&lt;/code&gt;, or &lt;code&gt;F&lt;/code&gt;&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '[fGF]'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Je&lt;strong&gt;ff&lt;/strong&gt; and the pet Lucky. &lt;strong&gt;G&lt;/strong&gt;regg and the dog &lt;strong&gt;F&lt;/strong&gt;ido. Chris has 1 bird named Tweety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;f f G F&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses a pattern collection with both metacharacters and regular characters. The logic behind the regular expression says: &lt;em&gt;Match any &lt;code&gt;g&lt;/code&gt;, &lt;code&gt;r&lt;/code&gt;, or &lt;code&gt;e&lt;/code&gt; followed by a space character and then the string &lt;code&gt;Fido&lt;/code&gt;&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '[gre]\sFido'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff and the pet Lucky. Gregg and the do&lt;strong&gt;g Fido&lt;/strong&gt;. Chris has 1 bird named Tweety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;g Fido&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses two pattern collections along with metacharacters that are outside them. The regular expression says: &lt;em&gt;Match a numeric character, then continue matching any character zero or many times that is followed by an uppercase character&lt;/em&gt;. The pattern collection &lt;code&gt;[0-9]&lt;/code&gt; indicates any numeral from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;9&lt;/code&gt;. The metacharacters &lt;code&gt;.*&lt;/code&gt; indicate zero or more instances of any character, and the pattern collection &lt;code&gt;[A-Z]&lt;/code&gt; indicates any uppercase character from &lt;code&gt;A&lt;/code&gt; to &lt;code&gt;Z&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '[0-9].*[A-Z]'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff and the pet Lucky. Gregg and the dog Fido. Chris has &lt;strong&gt;1 bird named T&lt;/strong&gt;weety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;1 bird named T&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses the negation metacharacter &lt;code&gt;^&lt;/code&gt; within a pattern collection. The negation metacharacter indicates that the succeeding characters are &lt;em&gt;not&lt;/em&gt; to be matched when the regular expression is being executed.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: As you might remember from the first article in this series, &lt;code&gt;^&lt;/code&gt; is the same metacharacter that indicates a line start—but only when used &lt;em&gt;outside&lt;/em&gt; square brackets. The &lt;code&gt;^&lt;/code&gt; metacharacter indicates negation &lt;em&gt;only&lt;/em&gt; when it appears within the square brackets (&lt;code&gt;[ ]&lt;/code&gt;) that declare a pattern collection.&lt;/p&gt; &lt;p&gt;The following collection pattern says: &lt;em&gt;Match any character that is not &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;e&lt;/code&gt;, &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;o&lt;/code&gt;, or &lt;code&gt;u&lt;/code&gt;&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky." $ echo $teststr | grep -Po '[^aeiou]'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text. The text is underlined to make the space characters apparent:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;u&gt;&lt;strong&gt;J&lt;/strong&gt;e&lt;strong&gt;ff &lt;/strong&gt;a&lt;strong&gt;nd th&lt;/strong&gt;e&lt;strong&gt; p&lt;/strong&gt;e&lt;strong&gt;t L&lt;/strong&gt;u&lt;strong&gt;cky.&lt;/strong&gt;&lt;/u&gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Space characters in the following output are also underlined to make them apparent. Space characters are matched by this regular expression:&lt;/p&gt; &lt;pre class="language-bash"&gt; &lt;code&gt;J f f _ n d _ t h _ p t _ L c k y . &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Groups&lt;/h2&gt; &lt;p&gt;A group in a regular expression is, as the name implies, a group of characters declared according to a specific definition. A group declaration can include metacharacters and regular characters. A group is declared between open and closed parentheses like this: &lt;code&gt;( )&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The following example uses a &lt;code&gt;.&lt;/code&gt; (dot) metacharacter, which indicates "any character." The declared group says: &lt;em&gt;Match any three characters as a group and return each group&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '(...)'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in alternating bold and non-bold text as shown in the following text. Again, the text is underlined to make the space characters apparent:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;u&gt;&lt;strong&gt;Jef&lt;/strong&gt;f a&lt;strong&gt;nd &lt;/strong&gt;the&lt;strong&gt; pe&lt;/strong&gt;t L&lt;strong&gt;uck&lt;/strong&gt;y. &lt;strong&gt;Gre&lt;/strong&gt;gg &lt;strong&gt;and&lt;/strong&gt; th&lt;strong&gt;e d&lt;/strong&gt;og &lt;strong&gt;Fi&lt;/strong&gt;do. &lt;strong&gt;Chr&lt;/strong&gt;is &lt;strong&gt;has&lt;/strong&gt; 1 &lt;strong&gt;bir&lt;/strong&gt;d n&lt;strong&gt;ame&lt;/strong&gt;d T&lt;strong&gt;wee&lt;/strong&gt;ty.&lt;/u&gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Because the group is identified and returned on a one-by-one basis, the output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Jef f_a nd_ the _pe t_L uck y._ Gre gg_ and _th e_d og_ Fid o._ Chr is_ has _1_ bir d_n ame d_T wee ty. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses the &lt;code&gt;.&lt;/code&gt; (dot) metacharacter along with the regular character &lt;code&gt;y&lt;/code&gt; to define a group of three characters, of which the first two characters can be anything and the third character must be &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '(..y)'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff and the pet Lu&lt;strong&gt;cky&lt;/strong&gt;. Gregg and the dog Fido. Chris has 1 bird named Twe&lt;strong&gt;ety&lt;/strong&gt;.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;cky ety&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example demonstrates a regular expression group that uses the &lt;code&gt;.&lt;/code&gt; (dot) metacharacter along with the &lt;code&gt;\d&lt;/code&gt; metacharacter to define a group of five characters, of which the first two characters are any regular character, the third character is a digit, and the last two characters are any regular characters:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '(..\d..)'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text. The text is underlined to make the space characters apparent.&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;u&gt;Jeff and the pet Lucky. Gregg and the dog Fido. Chris ha&lt;strong&gt;s 1 b&lt;/strong&gt;ird&lt;/u&gt;&lt;/code&gt;&lt;code&gt;&lt;u&gt; named Tweety.&lt;/u&gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;strong&gt;&lt;code class="java"&gt;s&lt;u&gt; &lt;/u&gt;1&lt;u&gt; &lt;/u&gt;b&lt;/code&gt;&lt;/strong&gt;&lt;/pre&gt; &lt;h2&gt;Word boundaries&lt;/h2&gt; &lt;p&gt;A word character is declared using the metacharacters &lt;code&gt;\w&lt;/code&gt;. A word character indicates any uppercase character, lowercase character, numeric character, or connector character such as a hyphen.&lt;/p&gt; &lt;p&gt;A word boundary is defined as a transition between a word character and a beginning space, an ending space, or a punctuation mark ( &lt;code&gt;.!?&lt;/code&gt; ). A word boundary is declared using the metacharacters &lt;code&gt;\b&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The following example demonstrates a regular expression that uses the metacharacters &lt;code&gt;\w+&lt;/code&gt; to find occurrences of words within text. The metacharacter &lt;code&gt;+&lt;/code&gt; indicates one or more occurrences of a character. The logic in play is: &lt;em&gt;Match one or more word characters&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. $ echo $teststr | grep -Po '\w+'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;strong&gt;Jeff&lt;/strong&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;strong&gt;the&lt;/strong&gt; &lt;strong&gt;pet&lt;/strong&gt; &lt;strong&gt;Lucky&lt;/strong&gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Because each word is identified and returned on a one-by-one basis, the output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Jeff and the pet Lucky&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses a word boundary to find occurrences of the regular character &lt;code&gt;a&lt;/code&gt; that appears at the beginning of a word:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;"Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '\ba'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text:&lt;/p&gt; &lt;p&gt;&lt;code&gt;and the pet Lucky. Gregg &lt;strong&gt;a&lt;/strong&gt;nd the dog Fido. Chris has 1 bird named Tweety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;a a&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses a word boundary to find occurrences of the regular character &lt;code&gt;y&lt;/code&gt; that appear at the end of a word:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po 'y\b'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text. Note that punctuation marks at the end of a word are not considered word characters and are excluded from the match:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;u&gt;Jeff and the pet Luck&lt;strong&gt;y&lt;/strong&gt;. Gregg and the dog Fido. Chris has 1 bird named Tweet&lt;strong&gt;y&lt;/strong&gt;.&lt;/u&gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;y y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example uses a word boundary to find occurrences of the regular characters &lt;code&gt;Tweety&lt;/code&gt; that appear at the end of a word:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po 'Tweety\b'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text. Again, notice that punctuation marks at the end of a word are excluded:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named &lt;strong&gt;Tweety&lt;/strong&gt;.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Tweety&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example contains a regular expression group that uses word boundaries to find occurrences of words that start with the regular character &lt;code&gt;a&lt;/code&gt; and end with the regular character &lt;code&gt;d&lt;/code&gt;. The regular expression uses the metacharacters &lt;code&gt;\w*&lt;/code&gt; to declare all occurrences of word characters:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ teststr="Jeff and the pet Lucky. Gregg and the dog Fido. Chris has 1 bird named Tweety." $ echo $teststr | grep -Po '\ba\w*d\b'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The regular expression matches the characters highlighted in bold in the following text.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jeff &lt;strong&gt;and&lt;/strong&gt; the pet Lucky. Gregg &lt;strong&gt;and&lt;/strong&gt; the dog Fido. Chris has 1 bird named Tweety.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;and and&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Grouping and specifying multiple characters simultaneously extend regular expressions&lt;/h2&gt; &lt;p&gt;This article gave you an introduction to working with quantifiers, pattern collections, groups, and word boundaries. You learned to use quantifiers to declare a range of character occurrences to match. Also, you learned that pattern collections enable you to declare character classes that match characters in a generic manner. Groups execute matches that declare a particular set of characters. Word boundaries allow you to make matches by working within the boundaries of space characters and punctuation marks.&lt;/p&gt; &lt;p&gt;These intermediate concepts covered in this article will bring additional power and versatility to working regular expressions. But there's a lot more to learn. Fortunately, as mentioned at the beginning of this article, you can use the concepts and techniques discussed in this article immediately.&lt;/p&gt; &lt;p&gt;The key is to start practicing what you've learned now. Mastery is the result of small, incremental accomplishments. As with any skill, the more you practice, the better you'll get.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/16/regex-how-quantifiers-pattern-collections-and-word-boundaries" title="Regex how-to: Quantifiers, pattern collections, and word boundaries"&gt;Regex how-to: Quantifiers, pattern collections, and word boundaries&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-09-16T07:00:00Z</dc:date></entry><entry><title type="html">New Keycloak maintainer: Michal Hajas</title><link rel="alternate" href="https://www.keycloak.org/2022/09/mhajas" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org/2022/09/mhajas</id><updated>2022-09-16T00:00:00Z</updated><content type="html">We are pleased to welcome as an official maintainer of Keycloak. Michal has been with the Keycloak project since September 2015, and since that period has to almost every component of Keycloak - core server, authorization services, adapters, javascript, code auto-generation, legacy operator - either by review or code contribution. Since his first involvement, he has steadily contributed code, currently . Lately, he has designed and co-developed Hot Rod storage and has been instrumental in overall establishing the new map storage. He reviews community contributions and offers help to finalize PRs, as well as participates in community discussions and issue triaging. He understands and respects the code of conduct, and in reviews helps maintaining it.</content><dc:creator>Stian Thorgersen</dc:creator></entry><entry><title type="html">Multiple repositories Pull Request chaos, crawl them all in one single place</title><link rel="alternate" href="https://blog.kie.org/2022/09/multiple-repositories-pull-request-chaos-crawl-them-all-in-one-single-place.html" /><author><name>Enrique Mingorance Cano</name></author><id>https://blog.kie.org/2022/09/multiple-repositories-pull-request-chaos-crawl-them-all-in-one-single-place.html</id><updated>2022-09-15T17:00:00Z</updated><content type="html">Flickr chaos – https://bit.ly/3Q2zfYS It is very frequent to find software engineering projects where multiple repositories are involved for the same or different projects, somehow related between them, a lot of people push their pull requests to any of them and it is very normal to lose tracking of the situation or you have to constantly browse them all to have a clearer picture about what is going on. That’s the situation we had here at the Red Hat Business Automation team and we solved it by creating a helpful tool you can easily use for your set of projects, easy, quick and for free. THE CROSS-REPO PRS PROBLEM This is already covered by entry, so feel free to read it in case you are not familiar with this kind of situation or concepts. THE CHAIN-STATUS SOLUTION So we said to ourselves, what if we would have a centralized place, a web page for instance, to be able to see in a quick look what’s the situation about all the pull requests for all of our repositories? was the solution. Prerequisites: * It has to solve not only our particular problem, so anyone can use it. * It has to be public, no authentication required. * It has to be fast, we can’t wait for the whole pull request set to be crawled everytime anyone gets into the application. * Multiple streams or different project set can be handled in different views, like different products or product versions from the same place. * The content can be filtered out. So the conclusion was to create in one hand a React web page to consume the pull request information from a static report and another tool to generate that report based on Github information. This way: * The information will be produced asynchronously, the frequency will be up to the user/developer and Github API rate limit problems will be avoided. * The information can be taken even from private repositories and be exposed publicly and no authentication will be required.  * No waiting time while information is requested from Github service. * The webpage (HTML+JS files) can be stored on any web service, even on free services like or . * No backend server is required. RUNNING EXAMPLE You can check KIE RHBA status web page at Chain Status web tool screenshot HOW CAN I ADD IT TO MY ORGANIZATION? The best way to integrate this tool in your organization or set of repositories is by using the provided configurable . In particular this tool comes with two main easy-to-use : * Generate App: this action aims to build and copy the React web application inside your repository and publish it using NPM tool. * Generate Data: given a project structure and some project information as input, this action is focused on generating the data report gathering the information using the Github API. This report is then used by the web application as a content source. Thus, in order to use these actions on your organization, you only have to add two (one per action) on your main repository as follows: 1. Prerequisites: having a Github token properly configured in your organization, on how to configure it. 2. Generate app workflow (generate_status_page.yaml): add the Github workflow for the web page generation, this should generally be run only once (or whenever there are changes on the web app look and feel). name: Generate status page on: workflow_dispatch jobs:   generate-status-page:     if: github.repository_owner == '&lt;OWNER&gt;'     concurrency:       group: generate-status-page       cancel-in-progress: true     strategy:       matrix:         os: [ubuntu-latest]       fail-fast: true     runs-on: ubuntu-latest     name: Generate status page     steps:       - name: Generate status page         uses: kiegroup/chain-status/.ci/actions/generate-app@main         with:           info-md-url: "&lt;PATH-TO-INFO&gt;"           github-token: "${{ secrets.GITHUB_TOKEN }}"           gh-pages-branch: "gh-pages" 3. Generate data workflow (generate_status_page_data.yaml): add the periodic workflow that will continuously generate the data fetched by the web application. name: Generate status page data on:   workflow_dispatch:   schedule:     - cron: '0 * * * *' jobs:   generate-status-page-data:     if: github.repository_owner == '&lt;OWNER&gt;'     concurrency:       group: generate-status-page-data       cancel-in-progress: true     strategy:       matrix:         os: [ubuntu-latest]       fail-fast: true     runs-on: ubuntu-latest     name: Generate status page data     steps:       - name: Generate status page data         uses: kiegroup/chain-status/.ci/actions/generate-data@main         with:           definition-file: &lt;PATH-TO-DEFINITION-FILE&gt;           # projects: &lt;PROJECTS-LIST&gt;           title: &lt;TITLE&gt;           subtitle: &lt;SUBTITLE&gt;           base-branch-filter: &lt;BRANCH-LIST&gt;           created-by: Github Action           created-url: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}           logger-level: debug           github-token: "${{ secrets.GITHUB_TOKEN }}"           gh-pages-branch: "gh-pages"  As already introduced, the generate data flow relies on a project structure definition which can be provided either using build-chain definition file or a projects list: * Build-chain definition file (using ‘definition-file’ field), a YAML definition file for cross-related inter-dependent projects which was introduced for . This tool is already covered by , so feel free to read it if you want to get more details on it and on its definition files. * Projects list (using ‘projects’ field), a comma-separated list of projects for which you would like to provide Pull Requests statuses. [Still a Work in Progress ] This was a brief explanation on how you could integrate this tool in your organization, if you need more details on this feel free to reach the homepage, where you can find a step-by-step guide on how to integrate it with some links to running examples. ADDITIONAL FUNCTIONALITIES Additionally to the pull request summary functionality, it is also possible to add multiple Jenkins status reports. The main advantage of this feature is that you can check the status of all your Jenkins jobs in a single place, making it easier to check what runs succeeded/failed and also the time and average time jobs are consuming. As an example you can check the KIE RHBA daily builds page   To configure the Jenkins status reports feature, you can create a Jenkins pipeline that will generate and update the data periodically. You can schedule the Jenkins pipeline to run and keep the status updated based on your required demand. You can add the following steps as part of your Jenkins pipeline to generate and update the status report: 1. Clone the GitHub pages repository stage('Clone gh-pages repository') {   steps {     script {       println "Checking out https://github.com/${ghPagesRepository}:${ghPagesBranch} into ${ghPagesRepoFolder} folder"       sh "git clone -b ${ghPagesBranch} --single-branch https://github.com/${ghPagesRepository} ${ghPagesRepoFolder}"     }   } } 2. Install the chain-status tool stage('Install chain-status tool') {   steps {     script {       try {         sh "npm install -g @kie/chain-status-action"       } catch(e) {         println '[WARNING] Error installing @kie/chain-status-action.'       }     }   } } 3. Generate the updated data stage('Generate data') {   steps {     script {       dir(ghPagesRepoFolder) {         sh "build-chain-status-report --jenkinsUrl ${jenkinsURL} --jobUrl ${jenkinsJobPath} -t ${projectTitle} -st ${projectSubtitle} --certFilePath ${jenkinsCertFile} --outputFolderPath ./data/ --skipZero -cb \"Jenkins Job\" -cu \"${env.BUILD_URL}\" --order 1001"       }     }   } } 4. Push changes to update the status report stage('Push changes to repository') {   steps {     script {       println "Pushing changes to ${ghPagesRepository}:${ghPagesBranch}"         dir(ghPagesRepoFolder) {           withCredentials([usernamePassword(credentialsId: "${githubCredentialsId}", usernameVariable: 'GITHUB_USER', passwordVariable: 'GITHUB_TOKEN')]) { githubscm.setUserConfig("${GITHUB_USER}")           sh("git config --local credential.helper \"!f() { echo username=\\$GITHUB_USER; echo password=\\$GITHUB_TOKEN; }; f\"")           sh 'git add data/*'           sh 'git commit -m "Generate Jenkins Data"'           sh "git push origin ${ghPagesBranch}"                                   }       }     }   } } NEXT STEPS AND LIMITATIONS HISTORIC FUNCTIONALITY Since the generator tool registers every day status, we expect to offer the historic view functionality to be able to compare status between dates.See TO COVER NOT ONLY GITHUB BUT OTHER REPOSITORY SERVICES Right now we only cover Github for the generator tool to take information from, but we expect to cover another kind of services like Gitlab or Bitbucket. CONCLUSION We have been using this tool for , and repositories for a year and we can say it’s a very useful tool which solves the cross-repo pull requests summary problem. After a year of experience with the tool we can say the tool offers: * To be able to constantly see the status of the different contributions from the different people. * Who is working on what, like which are my own open pull requests. * To quickly check obsolete contributions and to be able to keep our repositories very clean. * To publicly offer Jenkins jobs summary no matter whether the Jenkins is accessible or not. * To quickly check how healthy our CI/CD stuff is thanks to the error index information from the tool. * To be able to see related pull requests for every pull request, thanks to the cross-repo pull request functionality. USEFUL LINKS [Chain status]   [Build chain tool] [Build chain npm package]   [Configuration reader] [RHBA definition and project tree files]  [RHBA flows]   Featured photo by The post appeared first on .</content><dc:creator>Enrique Mingorance Cano</dc:creator></entry><entry><title>How to implement a job queue with Redis</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/redis-job-queue/&#xA;            " /><author><name>Clement Escoffier (https://twitter.com/clementplop)</name></author><id>https://quarkus.io/blog/redis-job-queue/</id><updated>2022-09-15T00:00:00Z</updated><published>2022-09-15T00:00:00Z</published><summary type="html">In how to cache with Redis, we implemented a simple cache backed by Redis. That’s just one use case of Redis. Redis is also used as a messaging server to implement the processing of background jobs or other kinds of messaging tasks. This post explores implementing this pattern with Quarkus...</summary><dc:creator>Clement Escoffier (https://twitter.com/clementplop)</dc:creator><dc:date>2022-09-15T00:00:00Z</dc:date></entry></feed>
