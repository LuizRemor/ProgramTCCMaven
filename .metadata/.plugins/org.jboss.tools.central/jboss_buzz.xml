<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Comparing Choices in DMN Modeling</title><link rel="alternate" href="https://blog.kie.org/2022/10/comparing-choices-in-modeling-dmn.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2022/10/comparing-choices-in-modeling-dmn.html</id><updated>2022-10-04T05:00:00Z</updated><content type="html">Recently in a , Keith Swenson, VP of R&amp;amp;D Fujitsu America and former leader of the DMN “Technical Compatibility Kit,” did a comparative analysis between free DMN modeling alternatives. The author compared TrisoTech DMN Modeler, Camunda, Red Hat Drools Workbench, and our beloved . As the results are interesting, I’ve decided to share them in a blog post. LITTLE BACKGROUND: KIE SANDBOX AND DMN ‘RUNNER’ Started with a prototype in late 2020, aiming to explore ways to augment the developer authoring experience for BPMN and DMN assets; gradually got traction and became an integrated part of our Tooling experience. Check out this for a walkthrough of the top features of KIE Sandbox. Focusing on a seamless authoring experience and an instantaneous feedback loop for DMN models, the DMN runner was mid-2021, and quickly became one of my favorite innovations on Tooling. Using a fast and automatic form generation and evaluation based on DMN models, DMN runner allows users to quickly try and experiment with their DMN runner in authoring, emulating an experience similar to authoring a spreadsheet like Excel or Google Docs. COMPARING CHOICES IN MODELING DMN In a recent , Keith Swenson provided the following feedback from our tools: &gt; What is particularly impressive is the DMN test capability. To get it to run, &gt; you need to download and start the KIE Sandbox Extended Services, which ran &gt; without a hitch. The web UI automatically noticed that the engine was &gt; installed. Then, it would automatically generate forms for inputting the data. &gt; Most impressive was that it can handle structured data records and arrays, &gt; even arrays of structured records. The output appears in the next column over. &gt; The complex table and list commands all ran. It is hard for me to really &gt; express how great it was after days of trying to get various combinations to &gt; work, to find an environment where everything ran without a problem. &gt; So, if you want practical experience with DMN models and running the models, &gt; go right to KIE Sandbox. It works. &gt; – Keith Swenson The author also summarizes that among other competitors, KIE Sandbox is the “best all around tool for a DMN practitioner, and built on RedHat so certainly compatible with that.” Keith Swenson, our tooling team, would like to thank you for the feedback, and we are looking forward to ways to improve even more our Tooling! The post appeared first on .</content><dc:creator>Eder Ignatowicz</dc:creator></entry><entry><title type="html">KIE Tools Highlights &amp;#8211; Q3</title><link rel="alternate" href="https://blog.kie.org/2022/10/kie-tools-highlights-q3.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2022/10/kie-tools-highlights-q3.html</id><updated>2022-10-03T05:00:00Z</updated><content type="html">Some days ago, we just launched KIE Tools and wrap-up the deliverables of our team for the third quarter. The main goal of this milestone was to expand the Serverless Workflow tooling to provide the best developer experience for the Serverless Logic ecosystem, with highlights to the new Serverless Workflow visualization! This post will give a quick overview of the most important deliverables of this quarter. I hope you enjoy it! MODERNIZE SERVERLESS WORKFLOW VISUALIZATION In partnership with Red Hat UX Team, we are happy to announce that we just released a new diagram visualizer for the Serverless Workflow based on . This new visualization improves the user experience authoring Serverless Workflows, with a much better look and feel, and a lot of additional features like state navigation, error handling, and automatic workflow reloading. See this for full details. SERVERLESS WORKFLOW PLUG-IN FOR KNATIVE CLI Serverless Workflow provides a plug-in named kn-plugin-workflow for Knative CLI, enabling you to quickly set up a local workflow project using the command line. See our for more information. Our plug-in is now included by default in the client, allowing users of this CLI to create and use workflow commands without the need to install any additional plug-in. CUSTOM DASHBOARDS ON DEV UI Besides our default Dashboard on Quarkus Dev UI, users now can have custom dashboards based on . Check out this video: DASHBUILDER SAMPLES Almost every week, we push new samples from our Dashbuilder samples repository. This month’s highlight is William’s , which allows us to visualize data from Quarkus or even embed dashboards in my Quarkus application. You can check it running live . KOGITO SERVERLESS WORKFLOW GUIDES Our team also wrote a lot of guides to Kogito Serverless Workflow . You can start exploring by checking out the . WHAT IS IN PROGRESS? We have several initiatives in R&amp;amp;D and in progress, including: * Expand Knative developer experience, allowing users to create, run and deploy single file workflows without Java dependencies; * Reach feature parity between JSON and YAML text editing experiences, enabling a rich edit experience on YAML-based workflows; * Native integration of Serverless workflow with Ansible, Kaoto and RHODS; * More improvements for Dev UI; * Standalone embeddable Serverless Workflow editor; * Improve auto completion experience, to make it easier to invoke a service or orchestrate and event, i.e. to create specific states like operation and async; * Serverless Logic Web Tools UX Redesign; * Dashbuilder VS Code extension; THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! The post appeared first on .</content><dc:creator>Eder Ignatowicz</dc:creator></entry><entry><title type="html">Eclipse Vert.x 4.3.4 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-3-4" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-3-4</id><updated>2022-10-03T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.3.4 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features. In addition it provides support for virtual threads incubation project.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title>Perform inference using Intel OpenVINO Model Server on OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/30/perform-inference-using-intel-openvino-model-server-openshift" /><author><name>Audrey Reznik</name></author><id>61b1b545-e963-4cef-a514-7ea1507a17fd</id><updated>2022-09-30T07:00:00Z</updated><published>2022-09-30T07:00:00Z</published><summary type="html">&lt;p&gt;Model servers, as illustrated in Figure 1, are very convenient for AI applications. They act as &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and can abstract the entirety of inference execution, making them agnostic to the training framework and hardware. They also offer easy scalability and efficient resource utilization.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_11.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_11.png?itok=2HLp0Sa1" width="547" height="197" alt="Diagram showing a model server as part of an AI application" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A model server as part of an AI application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; are optimal places for deploying model servers. However, managing them directly can be a complex task in a large-scale environment. In this article, you'll learn how the OpenVINO Model Server Operator can make it straightforward.&lt;/p&gt; &lt;h2&gt;Operator installation&lt;/h2&gt; &lt;p&gt;The operator can be easily installed from the OpenShift console. Just navigate to the &lt;strong&gt;OperatorHub&lt;/strong&gt; menu (Figure 2), search for OpenVINO™ Toolkit Operator, then click the Install button.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_7.png?itok=h5ZyukuW" width="600" height="588" alt="Screenshot showing the installation of the OpenVINO Toolkit Operator" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Install the OpenVINO Toolkit Operator. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Deploying an OpenVINO Model Server in OpenShift&lt;/h2&gt; &lt;p&gt;Creating a new instance of the model server is easy in the OpenShift console interface (Figure 3). Click the &lt;strong&gt;Create ModelServer&lt;/strong&gt; and then fill in the interactive form.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig3_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig3_4.png?itok=eIwPJSG9" width="600" height="166" alt="Screenshot showing the creation of a Model Server" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Create a Model Server &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;The default exemplary parameters deploy a fully functional model server with the well-known image classification model ResNet-50. This model is available in the public cloud for anyone to use. Why are we using this model? Because it saves us time from creating our own image classification model from scratch.&lt;/p&gt; &lt;p&gt;A bit more information on the ResNet-50 model just in case you have never heard of it before: The model is a pre-trained deep learning model for image classification of the &lt;em&gt;convolutional neural network,&lt;/em&gt; which is a class of deep neural networks most commonly applied to analyzing images. The &lt;em&gt;50&lt;/em&gt; in the name represents the model being 50 layers deep. The model is trained on a million images in a thousand categories from the &lt;a href="https://www.image-net.org/"&gt;ImageNet database&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you'd rather use the command-line interface (CLI) instead of the OpenShift console, you would use a command like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;oc apply -f https://raw.githubusercontent.com/openvinotoolkit/operator/main/config/samples/intel_v1alpha1_ovms.yaml&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;More complex deployments with &lt;a href="https://docs.openvino.ai/latest/ovms_docs_multiple_models.html"&gt;multiple models&lt;/a&gt; or &lt;a href="https://docs.openvino.ai/latest/ovms_docs_dag.html"&gt;DAG pipelines&lt;/a&gt; can also be deployed fairly easily by adding a config.json file into a configmap and linking it with the &lt;code&gt;ModelServer&lt;/code&gt; resource.&lt;/p&gt; &lt;p&gt;In this article, let's check the usage with the default Resnet model. While deployed, it will create the resources shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig4_5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig4_5.png?itok=z1chFpmN" width="600" height="362" alt="Screenshot showing resources for model deployment" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Resources for model deployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;How to run inferences from ovmsclient&lt;/h2&gt; &lt;p&gt;In this demonstration, let's create a pod in our OpenShift cluster that will act as a client. This can be done from the OpenShift console or from the CLI. We'll use a &lt;code&gt;python:3.8.13&lt;/code&gt; image with a &lt;code&gt;sleep infinity&lt;/code&gt; command just to have a place for an interactive shell. We will submit a jpeg image of a zebra and see if the image can be identified by our model.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;oc create deployment client-test --image=python:3.8.13 -- sleep infinity oc exec -it $(oc get pod -o jsonpath="{.items[0].metadata.name}" -1 app=client-test) -- bash&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;From the interactive shell inside the client container, let's quickly test connectivity with the model server and check the model parameters.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; curl http://model-server-sample-ovms:8081/v1/config { "resnet" : { "model_version_status": [ { "version": "1", "state": "AVAILABLE", "status": { "error_code": "OK", "error_message": "OK" } } } } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Other REST API calls are described in the &lt;a href="https://docs.openvino.ai/2022.1/ovms_docs_server_api.html"&gt;OpenVINO API reference guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now let's use the Python library &lt;code&gt;ovmsclient&lt;/code&gt; to run the inference request:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; python3 -m venv /tmp/venv source /tmp/venv/bin/activate pip install ovmsclient &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;We'll download a zebra picture to test out the classification:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;curl https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/zebra.jpeg -o /tmp/zebra.jpeg &lt;/code&gt; &lt;/pre&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig5.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig5.jpg?itok=63Rf7-mN" width="336" height="224" alt="Image of a zebra" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Picture of a zebra used for prediction. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Below are the Python commands that will display the model metadata using the &lt;code&gt;ovmsclient&lt;/code&gt; library:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; from ovmsclient import make_grpc_client client = make_grpc_client("model-server-sample-ovms:8080") model_metadata = client.get_model_metadata(model_name="resnet") print(model_metadata) &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Those commands produce the following response:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; {'model_version': 1, 'inputs': {'map/TensorArrayStack/TensorArrayGatherV3:0': {'shape': [-1, -1, -1, -1], 'dtype': 'DT_FLOAT'}}, 'outputs': {'softmax_tensor': {'shape': [-1, 1001], 'dtype': 'DT_FLOAT'}}} &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you can create a Python script with basic client content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; cat &gt;&gt; /tmp/predict.py &lt;&lt;EOL from ovmsclient import make_grpc_client import numpy as np client = make_grpc_client("model-server-sample-ovms:8080") with open("/tmp/zebra.jpeg", "rb") as f: data = f.read() inputs = {"map/TensorArrayStack/TensorArrayGatherV3:0": data} results = client.predict(inputs=inputs, model_name="resnet") print("Detected class:", np.argmax(results)) EOL python /tmp/predict.py Detected class: 341 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Based on the ImageNet database which contains a thousand classes, our zebra image was matched to their zebra image, which happens to have the class ID 341 associated with it. This means that our image was successfully matched and is confirmed as a zebra image!&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;As you've seen, the OpenVINO Model Server can be easily deployed and used in OpenShift and Kubernetes environments. In this article, you learned how to run predictions using the &lt;code&gt;ovmsclient&lt;/code&gt; Python library.&lt;/p&gt; &lt;p&gt;You can &lt;a href="https://github.com/openvinotoolkit/operator"&gt;learn more about the Operator&lt;/a&gt; and check out &lt;a href="https://docs.openvino.ai/2022.1/ovms_docs_demos.html"&gt;other demos with OpenVINO Model Server&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/30/perform-inference-using-intel-openvino-model-server-openshift" title="Perform inference using Intel OpenVINO Model Server on OpenShift"&gt;Perform inference using Intel OpenVINO Model Server on OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Audrey Reznik</dc:creator><dc:date>2022-09-30T07:00:00Z</dc:date></entry><entry><title type="html">Infinispan 14.0.0.Final</title><link rel="alternate" href="https://infinispan.org/blog/2022/09/29/infinispan-14" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2022/09/29/infinispan-14</id><updated>2022-09-29T12:00:00Z</updated><content type="html">Flying saucers are probably the most common type of UFO. They are sleek and shiny and, most importantly, they come in peace bringing lots of goodies from outer space! Just like Infinispan 14! Oh, and the fact that it’s also the is no sheer coincidence. JDK REQUIREMENTS You will need at least JDK 11 in order to use Infinispan 14. Infinispan also supports JDK 17 LTS and the recently released JDK 19. JAKARTA EE We now ship variants of most of our modules: just append -jakarta to the artifact name: &lt;dependency&gt; &lt;groupId&gt;org.infinispan&lt;/groupId&gt; &lt;artifactId&gt;infinispan-core-jakarta&lt;/artifactId&gt; &lt;version&gt;14.0.0.Final&lt;/version&gt; &lt;/dependency&gt; CORE * Cluster Listener using includeCurrentState` will have better memory and performance performance. Every key no longer requires calculating its segment while iterating and memory is freed much earlier and is closed as each segment completes transfer * State Transfer reduces how long memory is required to be held therefore reducing required memory overheads when configuring your server. * State transfer metrics are exposed through JMX. Expose the number of segments during transfer. * Size method when invoked on a cache has been optimized in various cases to be an O(1) operation instead of O(N). Involves if expiration and if stores are configured, please check for more information. * Reduced some cases of blocking threads being over utilized, therefore reducing how large the blocking thread pool would need to grow. * Dynamic RBAC: a dynamic, clustered role mapper that can be modified at runtime to grant/deny access to specific principals. QUERY * Native Infinispan indexing annotations which finally replace the legacy Hibernate Query annotations we’ve used in past versions (see ) * Index startup mode to determine what happens to indexes on cache start (see ) * Dynamic index schema updates allow you to evolve your schema at runtime with near-zero impact to your queries (see ) * Support Protobuf’s oneof * We improved the hybrid query system * Support normalizers with the HotRod client PERSISTENCE * SoftIndexFileStore (default file store) segmentation performance has been improved significantly. This also reduces the number of Index segments required which reduces the number of open files and threads required on the server. * JDBCStringBasedStore no longer requires configuring the database min and max version as this is dynamically configured when checking the JDBC connection. * JPAStore has been removed. It had been deprecated for quite a while, but the move to support Hibernate 6 prompted its removal as JPAStore only worked with Hibernate 5. HOT ROD CLIENT * A new Hot Rod client with a completely redesigned API. * Sync (blocking), Async (non-blocking) and sub-APIs that fit with your programming model of choice. try (SyncContainer infinispan = Infinispan.create("hotrod://localhost")) { // Sync SyncCache&lt;String, String&gt; mycache = infinispan.sync().caches().get("mycache"); mycache.set("key", "value"); String value = mycache.get("key"); // set with options mycache.set("key", "anothervalue", writeOptions().lifespan(Duration.ofHours(1)).timeout(Duration.ofMillis(500)).build()); // Async infinispan.async().caches() .get("mycache").thenApply(c -&gt; c.set("key", "value").thenApply(ignore -&gt; c.get("key").thenApply(value -&gt; c.set("key", "anothervalue", writeOptions().lifespan(Duration.ofHours(1)).timeout(Duration.ofMillis(500)).build())) )); // Mutiny infinispan.mutiny().caches() .get("mycache").map(c -&gt; c.query("age &gt; :age").param("age", 80).skip(5).limit(10).find()) .subscribe().with(System.out::println); } SERVER * RESP endpoint: a Redis-compatible endpoint connector (implementing the RESP 3 protocol) with support for a subset of commands: set, get, del, mget, mset, incr, decr, publish, subscribe, auth, ping. The connector integrates with our security and protocol auto-detections, so that it is easily usable from our single-port endpoint. The implemented commands should be enough for typical caching usage. If you would like to see more, reach out via our community. * If you need to use , it’s now possible to use * Masked and external credentials, to avoid the use of secrets in your configuration files * Custom security providers, such as BouncyCastle, can now be used. Just drop your provider implementation in the server/lib and configure: &lt;ssl&gt; &lt;keystore path="server.bcfks" password="secret" alias="server" provider="BC" type="BCFKS"/&gt; &lt;/ssl&gt; * Improved TLS engine configuration, allowing fine-grained ciphersuites selection for both TLSv1.3 and TLSv1.2: &lt;engine enabled-protocols="TLSv1.3 TLSv1.2" enabled-ciphersuites="TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384" enabled-ciphersuites-tls13="TLS_AES_256_GCM_SHA384"/&gt; * Endpoint worker threads configuration has been removed. With the rewrite in Infinispan 13 to utilize non blocking threads, this configuration was unused and deprecated. * integration with our security realms for authentication and authorization. * Experimental support * REST endpoints expose distribution information for caches and clusters. For more information, see and . CONSOLE * Cache creation wizard. See our recent about it CLI * List cache entries, including metadata, using different formats (table, JSON, CSV) * Configuration converter * Schema command to upload, delete, modify protobuf schema * Index command to manage indexes * Client certificate authentication IMAGE * Now based upon * Images provided for both amd64 and arm64 architectures * SERVER_LIBS environment variable added to allow dependencies to be downloaded prior to server startup * The config-generator has been removed. Its functionality can be replaced by using configuration overlays OPERATOR * Multi-Operand support, which means a single operator can managed different versions of Infinispan * FIPS support * Custom user configuration refactored to allow greater control of Infinispan configuration * Image based upon * Bundle provided for both amd64 and arm64 architectures * Admin service is now headless HIBERNATE ORM SECOND-LEVEL CACHE Hibernate caching implementation supporting Hibernate 6. Note that Hibernate 5 caching support is no longer provided due to Jakarta EE migration. OBSERVABILITY * Integration with OpenTelemetry tracing (see ) * Client / server request tracing correlations on both Hot Rod and REST APIs (see ) * Integration with Micrometer to produce Prometheus and OpenMetrics metrics OTHER Infinispan Quarkus server now supports the same command line arguments as the normal JVM Infinispan server. In addition the Infinispan Quarkus native binary can be used in an existing unzipped Infinispan Server zip file for ease of use. DOCUMENTATION Many improvements, updates and fixes. RELEASE NOTES You can look at the to see what has changed since our latest CR. Get them from our .</content><dc:creator>Tristan Tarrant</dc:creator></entry><entry><title>The benefits and limitations of flexible array members</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/29/benefits-limitations-flexible-array-members" /><author><name>Serge Guelton</name></author><id>31f50823-24d0-4da8-9a0f-8cfc13ab586d</id><updated>2022-09-29T07:00:00Z</updated><published>2022-09-29T07:00:00Z</published><summary type="html">&lt;p&gt;Flexible array members (FAM) is an extension of C89 standardized in C99. This article discusses how flexible array members offer convenience and improve performance and how compiler implementations can generate complications.&lt;/p&gt; &lt;p&gt;FAM makes it possible to declare a struct with a dynamic size while keeping a flat memory layout. This is a textbook example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double data[ ]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;data&lt;/code&gt; array starts empty and will be loaded later, perhaps many times. Presumably, the programmer uses the &lt;code&gt;size&lt;/code&gt; member to hold the current number of elements and updates that variable with each change to the &lt;code&gt;data &lt;/code&gt;size.&lt;/p&gt; &lt;h2&gt;Flexible array members vs. pointer implementation&lt;/h2&gt; &lt;p&gt;Flexible array members allow faster allocation, better locality, and solid code generation. The feature is an alternative to a more traditional declaration of &lt;code&gt;data&lt;/code&gt; as a pointer:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double *data; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the pointer implementation, adding an array element requires an extra load initializing the structure on the heap. Each element added to the array requires two allocations for the object and its data member. The process results in fragmented memory between the object and the area pointed at by &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Standard flexible array member behavior&lt;/h2&gt; &lt;p&gt;The C99 standard, section 6.7.2.1.16, defines flexible array members. A struct with a flexible array member behaves in interesting ways.&lt;/p&gt; &lt;p&gt;It is legal to access any index of &lt;code&gt;fam::data&lt;/code&gt;, providing enough memory has been allocated:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam * f = malloc(sizeof(struct fam) + sizeof(double[n])); f - &gt; size = n;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;sizeof&lt;/code&gt; operator behaves as if the FAM had zero elements but accounts for the padding required to position it correctly. For instance, &lt;code&gt;sizeof(struct {char c; float d[];}&lt;/code&gt; is unlikely to be equal to &lt;code&gt;sizeof(char)&lt;/code&gt; because of the padding required to correctly position &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The assignment operator does not copy the flexible array member, which probably explains why that operator is not part of the C++ standard.&lt;/p&gt; &lt;p&gt;This would be the end of this post if there were no nonconformant compiler extensions.&lt;/p&gt; &lt;h2&gt;Nonconforming compiler extensions&lt;/h2&gt; &lt;p&gt;Flexible array members are supported only by GCC and Clang in C89 and C++ as extensions. The extensions use alternate syntax, sometimes called a struct hack.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam_extension { int size; double data[0]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can specify:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam_extension { int size; double data[1]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As it turns out, this syntax extended to any array size due to prior art, as suggested in the FreeBSD developers handbook, &lt;a href="https://docs.freebsd.org/en/books/developers-handbook/sockets/#sockets-sockaddr"&gt;section 7.5.1.1.2 sockaddr&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct sockaddr { unsigned char sa_len; /* total length */ sa_family_t sa_family; /* address family */ char sa_data[14]; /* actually longer; address value */ };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that using an array size different from 0 for the FAM makes the allocation idiom more complex because one needs to subtract the size of the FAM:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam * f = malloc(sizeof(struct sockaddr) + sizeof(char[n]) - sizeof(char[14]));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GCC and Clang extensions normalize the undefined behavior when performing an out-of-bounds access on an array. The program performs regular memory access as if it allocated the memory.&lt;/p&gt; &lt;h2&gt;Limitations of sized arrays&lt;/h2&gt; &lt;p&gt;The ability to consider sized arrays as FAM impacts the accuracy of some kinds of code analysis. Consider, for instance, the &lt;code&gt;-fsanitize=bounds&lt;/code&gt; option in which the instruments array detects when they are out-of-bounds. Without any context information, it cannot add a check to the following access:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double data[]; }; int foo(struct fam* f) { return f -&gt; data[8]; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But if we declare the array as &lt;code&gt;double data[1]&lt;/code&gt;, there is still no instrumentation. The compiler detects a FAM based on the extension definition and performs no check. Even worse, if we declare the array as &lt;code&gt;double data[4]&lt;/code&gt;, trunk GCC performs no check (honoring legacy code, as illustrated in the previous section), while Clang adds a bounds check.&lt;/p&gt; &lt;p&gt;We observe the same behavior for the &lt;code&gt;__builtin_object_size&lt;/code&gt; builtin. This builtin computes the allocated memory reachable from a pointer. When asked for &lt;code&gt;__builtin_object_size(f - &gt; data, 1)&lt;/code&gt;, both GCC and Clang return &lt;code&gt;-1&lt;/code&gt; (indicating a failure to compute that size) for all the declarations of &lt;code&gt;data&lt;/code&gt; we have explored so far. This policy is conservative and removes some of the security offered by &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt;, which relies heavily on the accuracy of &lt;code&gt;__builtin_object_size&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Motivation for stricter standard conformance&lt;/h2&gt; &lt;p&gt;A codebase that strictly conforms to the C99 standard (at least for FAM) would benefit from a compiler strictly following the standard definition of flexible array members. That goal motivates an effort currently led within the Linux kernel community, as demonstrated by &lt;a href="https://lore.kernel.org/lkml/20220322184802.GA2533969@embeddedor/"&gt;this patch&lt;/a&gt;. The &lt;a href="https://www.kernel.org/doc/html/v5.16/process/deprecated.html#zero-length-and-one-element-arrays"&gt;documentation&lt;/a&gt; update favors C99 FAM in place of zero-length arrays.&lt;/p&gt; &lt;p&gt;To take advantage of this development, they developed a compiler option using GCC and Clang to give the programmer control over flexible array syntax. The option is &lt;code&gt;-fstrict-flex-arrays=&lt;/code&gt; whereas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0 reflects the current situation described earlier.&lt;/li&gt; &lt;li&gt;1 considers only &lt;code&gt;[0]&lt;/code&gt;, &lt;code&gt;[1]&lt;/code&gt; and &lt;code&gt;[ ]&lt;/code&gt; as a FAM.&lt;/li&gt; &lt;li&gt;2 considers only &lt;code&gt;[0] &lt;/code&gt;and &lt;code&gt;[ ]&lt;/code&gt;as a FAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Compiling code with a &lt;code&gt;-fstrict-flex-arrays&lt;/code&gt; value greater than 0 unlocks some extra security while breaking (some) backward compatibility, which is why n=0 remains the default.&lt;/p&gt; &lt;h2&gt;Compiler convergence on C-language flexible array members&lt;/h2&gt; &lt;p&gt;Flexible array members is an interesting C99 feature that found its way, through compiler extensions, into C89 and C++. These extensions and legacy codes led to suboptimal code checks in the compiler, which the &lt;code&gt;-fstrict-flex-arrays=&lt;/code&gt; option can now control.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/29/benefits-limitations-flexible-array-members" title="The benefits and limitations of flexible array members"&gt;The benefits and limitations of flexible array members&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serge Guelton</dc:creator><dc:date>2022-09-29T07:00:00Z</dc:date></entry><entry><title type="html">Drools trouble-shooting : Memory issues</title><link rel="alternate" href="https://blog.kie.org/2022/09/drools-trouble-shooting-memory-issues.html" /><author><name>Toshiya Kobayashi</name></author><id>https://blog.kie.org/2022/09/drools-trouble-shooting-memory-issues.html</id><updated>2022-09-29T06:00:00Z</updated><content type="html">Memory issues are another frequent topic in Drools trouble-shooting. This article will explain how to solve issues categorized in 3 types. LONG TERM MEMORY LEAK WHILE USING DROOLS APPLICATIONS The word "Memory leak" is typically used for the situation where a JVM triggers Full GCs but the footprint is constantly increasing. In this case, let’s capture a heap dump. jmap -dump:format=b,file=heap.bin [JAVA_PID] Then analyze the heap dump with a tool. For example, Eclipse Memory Analyzer (MAT) : Firstly, check which objects retain the large part of the heap. For example, the histogram screenshot below suggests that KnowledgeBaseImpl and StatefulKnowlegeSessionImpl retain the large part of the heap. Histogram : many ksessions Secondly, check the number of the suspicious object. In this case, the number of StatefulKnowlegeSessionImpl objects is 749,572. Of course, it depends on your system. Your system might have a large number of concurrent active KieSessions at the same time so it may not be an issue. But if it’s unusually large, it’s likely caused by forgetting dispose. Please review your application codes and make sure to dispose of a KieSession in a finally block, so you will not miss it. Here is a histogram screenshot from another scenario. This time, KieRepositoryImpl retains the large part of the heap. Histogram : large KieRepositoryImpl This issue can be found when you build many KieContainers that may have different artifactIds or different versions. You may hit this issue in Business Central as well. KieRepositoryImpl is a repository to store KieModules that are built resources. However, what KieRepositoryImpl retains are "cache", so you can control the cache size using the system properties below: * kie.repository.project.cache.size: Maximum number of KieModules that are cached in the KieRepositoryImpl. Default value is 100 * kie.repository.project.versions.cache.size: Maximum number of versions of the same artifact that are cached in the KieRepositoryImpl. Default value is 10 You may even set them to 1. OUTOFMEMORYERROR WHEN BUILDING RULES If you hit an OutOfMemoryError when you build rule resources (e.g. ks.newKieBuilder(kfs).buildAll()), you would just need to increase the heap size (-Xmx). There may be room to review and reduce the number of your rules, but there is not much to do usually. The more rules you have, the more heap you would need. OUTOFMEMORYERROR WHEN EXECUTING A KIESESSION You may hit a memory spike and result in an OutOfMemoryError while executing a KieSession. I recommend you to set the JVM option -XX:+HeapDumpOnOutOfMemoryError, so you can capture a heap dump automatically at that time. Here is an example screenshot of the histogram. Histogram : cross-product You see that FromNodeLeftTuple and RightTupleImpl retain the large part of the heap. These objects are used only during KieSession execution, so "short-lived" objects. Seeing these objects is a sign that too many evaluations are happening during KieSession execution. It can be solved by improving rules. Typically, it is caused by "cross-product". The rule caused the issue is this: rule "Find Non unique SSN"   when     $inputList : List()     $p : Person() from $inputList     $nonUniqueSSNList : List(size &gt; 1) from collect (Person(ssn == $p.ssn) from $inputList)   then ... Client code inserts a List that contains 10000 Person objects. The 3rd line of the when condition causes 10000 x 10000 object evaluations inside "from". It caused an OutOfMemoryError. It would also be very slow even if you had enough memory. The rule would be rewritten like this: rule "Find Non unique SSN"   when     $p1: Person()     $p2: Person(this != $p1, ssn == $p1.ssn)   then ... Instead of inserting List itself, insert all Person objects. Then a rule can be very simple. It’s still a kind of "cross-product" so we may have a chance to improve it further depending on the fact model (e.g. $p2: Person(id &gt; $p1.id, ssn == $p1.ssn)). Anyway, it’s much faster than the previous rule. To investigate a bottle-neck of rules, this article will also help. The post appeared first on .</content><dc:creator>Toshiya Kobayashi</dc:creator></entry><entry><title>JBoss Tools for Eclipse 2022-09RC1</title><link rel="alternate" type="text/html" href="https://tools.jboss.org/blog/4.25.0.am1.html" /><category term="release" /><category term="jbosstools" /><category term="devstudio" /><category term="jbosscentral" /><category term="codereadystudio" /><author><name>jeffmaury</name></author><id>https://tools.jboss.org/blog/4.25.0.am1.html</id><updated>2022-09-30T11:40:19Z</updated><published>2022-09-29T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Happy to announce 4.25.0.AM1 (Developer Milestone 1) build for Eclipse 2022-09RC1.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Downloads available at &lt;a href="https://tools.jboss.org/downloads/jbosstools/2022-09/4.25.0.AM1.html"&gt;JBoss Tools 4.25.0 AM1&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-is-new"&gt;&lt;a class="anchor" href="#what-is-new"&gt;&lt;/a&gt;What is New?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Full info is at &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.25.0.AM1.html"&gt;this page&lt;/a&gt;. Some highlights are below.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="general"&gt;&lt;a class="anchor" href="#general"&gt;&lt;/a&gt;General&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="java-17-requirement"&gt;&lt;a class="anchor" href="#java-17-requirement"&gt;&lt;/a&gt;Java 17 requirement&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Java 17 is now a minimum requirement to &lt;strong&gt;run&lt;/strong&gt; JBoss Tools. JBoss Tools continues to support running servers and applications with older Java versions.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="quarkus-tools"&gt;&lt;a class="anchor" href="#quarkus-tools"&gt;&lt;/a&gt;Quarkus Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="improvement-to-the-new-quarkus-project-wizard"&gt;&lt;a class="anchor" href="#improvement-to-the-new-quarkus-project-wizard"&gt;&lt;/a&gt;Improvement to the new Quarkus project wizard&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Quarkus extension ecosystem is composed of extensions that are part of the platform and the others. The Quarkus project wizard has been extended to allow exclusion of extensions that are not part of the platform.&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/quarkus/images/quarkus46.gif" alt="quarkus46" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="hibernate-tools"&gt;&lt;a class="anchor" href="#hibernate-tools"&gt;&lt;/a&gt;Hibernate Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="new-runtime-provider"&gt;&lt;a class="anchor" href="#new-runtime-provider"&gt;&lt;/a&gt;New Runtime Provider&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A new runtime provider has been added for Hibernate 6.1. It incorporates Hibernate Core version 6.1.1.Final and Hibernate Tools version 6.1.1.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="runtime-provider-updates"&gt;&lt;a class="anchor" href="#runtime-provider-updates"&gt;&lt;/a&gt;Runtime Provider Updates&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Hibernate 5.6 runtime provider now incorporates Hibernate Core version 5.6.10.Final and Hibernate Tools version 5.6.10.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="and-more"&gt;&lt;a class="anchor" href="#and-more"&gt;&lt;/a&gt;And more…​&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find more noteworthy updates in on &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.24.0.AM1.html"&gt;this page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Jeff Maury&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;</content><summary>Happy to announce 4.25.0.AM1 (Developer Milestone 1) build for Eclipse 2022-09RC1. Downloads available at JBoss Tools 4.25.0 AM1. What is New? Full info is at this page. Some highlights are below. General Java 17 requirement Java 17 is now a minimum requirement to run JBoss Tools. JBoss Tools continues to support running servers and applications with older Java versions. Quarkus Tools Improvement to the new Quarkus project wizard The Quarkus extension ecosystem is composed of extensions that are part of the platform and the others. The Quarkus project wizard has been extended to allow exclusion of extensions that are not part of the platform. Hibernate Tools New Runtime Provider A new runtime provider has...</summary><dc:creator>jeffmaury</dc:creator><dc:date>2022-09-29T00:00:00Z</dc:date></entry><entry><title>Build a Kogito Serverless Workflow using Serverless Framework</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" /><author><name>Daniele Martinoli</name></author><id>59a057cf-6cdf-418b-8bb1-b494473d43b7</id><updated>2022-09-28T07:00:00Z</updated><published>2022-09-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="http://serverlessworkflow.io/"&gt;Serverless Workflow&lt;/a&gt; is a standard from the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF). &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-orchestrating-serverless"&gt;Kogito&lt;/a&gt; implements the Serverless Workflow specifications to define workflows for event-driven, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications using a DSL-based model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.serverless.com/"&gt;Serverless Framework&lt;/a&gt; is an open source framework that builds, compiles, and packages code for serverless deployment. The framework provides implementations for different cloud providers, including &lt;a href="https://knative.dev"&gt;Knative&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article walks you through the steps to integrate Kogito with Serverless Framework to build a working example on the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; Platform. The article is based on code you can find in &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework"&gt;my GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To run the demo in this article, you need the following tools on your local system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maven (at least 3.8.6)&lt;/li&gt; &lt;li&gt;Java SDK 11+&lt;/li&gt; &lt;li&gt;Docker&lt;/li&gt; &lt;li&gt;Bash terminal&lt;/li&gt; &lt;li&gt;The &lt;a href="https://www.serverless.com/framework/docs/getting-started"&gt;serverless command-line interface&lt;/a&gt; (CLI) from the Serverless Framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You also need accounts on the following systems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenShift 4.8+ (logged in as an account with the &lt;code&gt;cluster-admin&lt;/code&gt; role)&lt;/li&gt; &lt;li&gt;A &lt;a href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt; credential&lt;/li&gt; &lt;li&gt;A &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt; account&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Introducing the Kogito Newsletter Subscription Showcase example&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription"&gt;Kogito Newsletter Subscription Showcase&lt;/a&gt; is a demo based on the Kogito implementation of the Serverless Workflow specification. This example consists of two applications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;subscription-flow&lt;/code&gt;: The workflow orchestrator, defined using the Serverless Workflow specification&lt;/li&gt; &lt;li&gt;&lt;code&gt;subscription-service&lt;/code&gt;: A &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; application implementing the orchestrated services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 1 shows the system architecture of the example application.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/architecture_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/architecture_2.png?itok=D3tkbG8u" width="695" height="251" alt="The Newsletter Subscription Showcase is made of two applications, the Subscription Flow and the Subscription Service, an event Broker and a Persistent Storage " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt; KIE group &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-credit-line field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Credit Line&lt;/span&gt; &lt;span class="rhd-media-credit field__item"&gt; Thanks to KIE https://www.kie.org/ &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription &lt;/span&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The architecture of the Newsletter Subscription Showcase example. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The documentation in the repository describes how to &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription#running-on-knative"&gt;deploy and run the application on Knative&lt;/a&gt;, using the YAML configurations generated by the Maven build of the project, through the &lt;code&gt;knative&lt;/code&gt; build profile, running on &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;minikube&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article shows how to implement the same deployment in the cloud using the Serverless Framework. Our target is a Knative environment installed on OpenShift, but the principles extend to other cloud settings as well.&lt;/p&gt; &lt;h2 id="buildingkogito"&gt;Images for the Kogito Newsletter Subscription Showcase&lt;/h2&gt; &lt;p&gt;The first step is to build and publish the images of the two applications that make up the Newsletter Subscription Showcase example: &lt;code&gt;subscription-flow&lt;/code&gt; and &lt;code&gt;subscription-service&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You can download prebuilt images or build new ones from source. The prebuilt images for our example are available in the &lt;a data-saferedirecturl="https://www.google.com/url?q=http://quay.io/dmartino&amp;amp;source=gmail&amp;amp;ust=1664330157515000&amp;amp;usg=AOvVaw2UuZVMmvMJ1VTnzF74pQqa" href="http://quay.io/dmartino" target="_blank"&gt;quay.io/dmartino&lt;/a&gt; repository; if you download them, you can skip ahead to the next section, entitled "Installing Knative on OpenShift."&lt;/p&gt; &lt;p&gt;If you prefer to build the images yourself, you'll need to clone the Kogito Examples repository, build the applications using the &lt;code&gt;knative&lt;/code&gt; profile, and finally push the Docker images to your Quay repository, using the following commands. Replace &lt;code&gt;QUAY_USER_ID&lt;/code&gt; with your actual ID.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/kiegroup/kogito-examples.git cd kogito-examples git checkout stable cd serverless-workflow-examples/serverless-workflow-newsletter-subscription docker login quay.io mvn clean install -DskipTests -Pknative \ -Dquarkus.container-image.registry=quay.io \ -Dquarkus.container-image.group=QUAY_USER_ID \ -Dquarkus.container-image.push=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify that two images have been generated with the expected tag (as of today, it is &lt;code&gt;1.25.0.Final&lt;/code&gt;) on your &lt;a href="https://quay.io/repository/"&gt;Quay.io&lt;/a&gt; account, and modify the visibility of the images to make them publicly accessible.&lt;/p&gt; &lt;h2&gt;Installing Knative on OpenShift&lt;/h2&gt; &lt;p&gt;Knative can be easily installed on OpenShift using the OpenShift Serverless Operator, and this is the recommended approach we are going to follow.&lt;/p&gt; &lt;p&gt;Install the Red Hat Serverless Operator from the administrator console (Figure 2). The sequence of menu items to choose is &lt;strong&gt;OperatorHub→Red Hat OpenShift Serverless→Install.&lt;/strong&gt; Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/RHServerless.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/RHServerless.png?itok=XcrgtpH3" width="1440" height="747" alt="Administrator console -&gt; OperatorHub -&gt; Red Hat OpenShift Serverless -&gt; Install using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Select the Red Hat OpenShift Serverless operator from the OperatorHub page and install it using the default settings. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;One instance of the &lt;code&gt;KnativeServing&lt;/code&gt; custom resource is required to manage Knative serverless applications (Figure 3). Create the instance in the &lt;code&gt;knative-serving&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-serving&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Serving→Create KnativeServing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeServing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeServing1.png?itok=ASMZ9s82" width="1440" height="430" alt="Administrator console -&gt; Project: knative-serving -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Serving -&gt; Create KnativeServing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Install the KnativeServing custom resource from the knative-serving project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Additionally, one instance of the &lt;code&gt;KnativeEventing&lt;/code&gt; Custom Resource is required to manage the events around the Knative serverless applications (Figure 4). Create the instance in the &lt;code&gt;knative-eventing&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-eventing&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Eventing→Create KnativeEventing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeEventing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeEventing1.png?itok=01Fpe29z" width="1440" height="403" alt="Administrator console -&gt; Project: knative-eventing -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Eventing -&gt; Create KnativeEventing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Install the KnativeEventing custom resource from the knative-eventing project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Installing the example application&lt;/h2&gt; &lt;p&gt;To run the application, you need to install a PostgreSQL database. The application also requires some configuration changes to bring it up to date.&lt;/p&gt; &lt;h3&gt;Installing the newsletter-postgres service&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;newsletter-postgres&lt;/code&gt; service is a regular OpenShift deployment of PostgreSQL in a namespace called &lt;code&gt;newsletter-subscription-db&lt;/code&gt;. Execute the following instructions to install the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework.git cd kogito-serverless-workflow-with-serverless-framework oc create namespace newsletter-subscription-db oc adm policy add-scc-to-user anyuid -z default -n newsletter-subscription-db oc apply -f newsletter-postgres/newsletter-postgres.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Preparing the Serverless Framework&lt;/h3&gt; &lt;p&gt;To successfully run the example on OpenShift, we had to apply a few changes to the original implementation of the Knative Cloud Provider in the Serverless Framework. Such updates are needed to align the original Knative version to the one installed with the OpenShift Serverless Operator, and to introduce some extensions that support new settings and fix a few issues. Details are available in a &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework#updates-to-the-knative-provider"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because the example would not run using the default implementation of the Knative Cloud Provider, the &lt;code&gt;package.json&lt;/code&gt; descriptor includes the following dependency:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; "devDependencies": { "serverless-knative": "https://github.com/dmartinol/serverless-knative.git" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The necessary changes are available in the following GitHub repositories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/serverless-knative"&gt;Serverless Knative Plugin&lt;/a&gt;: The Knative Cloud Provider implementation for the Serverless Framework&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-serving"&gt;knative-serving&lt;/a&gt;: A Node.js module to manage Knative Serving instances&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-eventing"&gt;knative-eventing&lt;/a&gt;: A Node.js module to manage Knative Eventing instances&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Unwrapping the Serverless Framework descriptor&lt;/h3&gt; &lt;p&gt;The heart of the Serverless Framework deployment is the &lt;code&gt;serverless.yml&lt;/code&gt; file that sits at the local root of the &lt;code&gt;kogito-serverless-workflow-with-serverless-framework&lt;/code&gt; repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;service: newsletter frameworkVersion: '3' provider: name: knative # optional Docker Hub credentials you need if you're using local Dockerfiles as function handlers docker: username: ${env:DOCKER_HUB_USERNAME} password: ${env:DOCKER_HUB_PASSWORD} functions: event-display: handler: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display@sha256:a214514d6ba674d7393ec8448dd272472b2956207acb3f83152d3071f0ab1911 # autoscaler field is managed by knative provider # Just add any autoscaling related annotation and it will be propagated to the deployed Service and Revision # The plugin automatically adds the 'autoscaling.knative.dev/' prefix to the annotation name autoscaler: min-scale: 1 max-scale: 2 events: - custom: name: new.subscription.2.event-display filter: attributes: type: new.subscription - custom: name: confirm.subscription.2.event-display filter: attributes: type: confirm.subscription subscription-service: handler: Dockerfile.jvm context: ./subscription-service subscription-flow: handler: Dockerfile.jvm context: ./subscription-flow events: - custom: filter: attributes: type: confirm.subscription - sinkBinding: {} plugins: - serverless-knative&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To replicate the architecture of the original example, this deployment includes the functions listed in the following sections (the equivalents of the Knative Service resources).&lt;/p&gt; &lt;h4&gt;event-display&lt;/h4&gt; &lt;p&gt;This is an event logger application implemented with a prebuilt image from the Google Cloud Container Registry. The image is configured with a minimum of one instance to simplify logging activity, and has two &lt;code&gt;custom&lt;/code&gt; events that are mapped onto two Knative Trigger instances.&lt;/p&gt; &lt;h4&gt;subscription-service&lt;/h4&gt; &lt;p&gt;This is the service running the original &lt;code&gt;subscription-service&lt;/code&gt; application. The original source code was copied from the Kogito Examples repository under the &lt;code&gt;subscription-service&lt;/code&gt; folder, to show the option to locally build an application and deploy the serverless service using the Serverless Framework CLI.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;Dockerfile.jvm&lt;/code&gt; file defined by the &lt;code&gt;handler&lt;/code&gt; property builds the Quarkus application and injects the binding properties to connect to the &lt;code&gt;newsletter-postgres&lt;/code&gt; database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;ENV POSTGRES_PASSWORD=cGFzcwo= ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;subscription-flow&lt;/h4&gt; &lt;p&gt;This function runs the &lt;code&gt;subscription-flow&lt;/code&gt; image that you previously built, but with overridden properties to locate the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM quay.io/dmartino/serverless-workflow-newsletter-subscription-flow:1.25.0.Final ENV SUBSCRIPTION_API_URL=http://newsletter-subscription-service.sls-newsletter-dev.svc.cluster.local ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This serverless function is configured with one &lt;code&gt;custom&lt;/code&gt; event, mapped to a Knative Trigger instance, and one &lt;code&gt;sinkBinding&lt;/code&gt; event that generates the Knative SinkBinding to connect the Knative Service with the &lt;code&gt;default&lt;/code&gt; Knative Broker. The &lt;code&gt;default&lt;/code&gt; Knative Broker is automatically created by the &lt;code&gt;eventing.knative.dev/injection&lt;/code&gt; annotation attached to the Knative Trigger instances.&lt;/p&gt; &lt;h2&gt;Deploying the application with the Serverless Framework&lt;/h2&gt; &lt;p&gt;The first step is to build the &lt;code&gt;subscription-service&lt;/code&gt; application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd subscription-service $ mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following instructions assume that you have already installed the &lt;code&gt;serverless&lt;/code&gt; CLI, and set the environment variables &lt;code&gt;DOCKER_HUB_USERNAME&lt;/code&gt; and &lt;code&gt;DOCKER_HUB_PASSWORD&lt;/code&gt; to define the access credentials to the Docker Hub repository. Now deployment is just a matter of running the &lt;code&gt;serverless deploy&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless deploy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info&lt;/code&gt; command returns the deployment status of your application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless info ... Service Information service: newsletter namespace: sls-newsletter-dev Deployed functions event-display: - url: https://newsletter-event-display-sls-newsletter-dev.DOMAIN - custom - custom subscription-service: - url: https://newsletter-subscription-service-sls-newsletter-dev.DOMAIN subscription-flow: - url: https://newsletter-subscription-flow-sls-newsletter-dev.DOMAIN - custom - sinkBinding &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Validating the applications&lt;/h3&gt; &lt;p&gt;Run the applications using the default browser by executing the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ open -t $(oc get ksvc newsletter-subscription-flow -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}') $ open -t $(oc get ksvc newsletter-subscription-service -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}')&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can monitor the &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt; events through the logs of the &lt;code&gt;event-display&lt;/code&gt; pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -l serving.knative.dev/service=newsletter-event-display -f -n sls-newsletter-dev -c user-container&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Serverless Framework supports cloud deployments&lt;/h2&gt; &lt;p&gt;By using the Serverless Framework software, we successfully deployed a serverless application on Red Hat OpenShift, using Knative as the serverless framework.&lt;/p&gt; &lt;p&gt;The default implementation of the Knative Cloud Provider is missing some features and is not compatible with the Red Hat OpenShift Serverless Operator, so a patched implementation was used for the purposes of this article.&lt;/p&gt; &lt;p&gt;The application is defined using the Kogito implementation of the CNCF Serverless Workflow specification, a DSL-based model that targets the serverless technology domain.&lt;/p&gt; &lt;p&gt;Serverless Framework claims to be a cloud-agnostic tool, so nothing prevents us from extending the exercise in the future and adapting this deployment model to run on another cloud platform such as AWS or Azure.&lt;/p&gt; &lt;p&gt;For more information, please read the blog posting &lt;a href="https://knative.dev/blog/articles/event-drive-app-knative-eventing-kogito/"&gt;Orchestrating Events with Knative and Kogito&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" title="Build a Kogito Serverless Workflow using Serverless Framework"&gt;Build a Kogito Serverless Workflow using Serverless Framework&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniele Martinoli</dc:creator><dc:date>2022-09-28T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.13.0.Final released - Cross site request forgery prevention filter, Kafka Dev UI</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-13-0-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-13-0-final-released/</id><updated>2022-09-28T00:00:00Z</updated><published>2022-09-28T00:00:00Z</published><summary type="html">We don’t have a ton of big new features in Quarkus 2.13.0.Final but it comes with a ton of small enhancements that should improve your overall experience with Quarkus. It still comes with some exciting stuff: Cross Site Request Forgery (CSRF) prevention filter for RESTEasy Reactive (well, security is not...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-09-28T00:00:00Z</dc:date></entry></feed>
